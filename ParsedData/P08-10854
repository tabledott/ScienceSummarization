
    We argue that the key challenge to learning an effective model is to define good enough initial conditions.
    Given sufficiently good initial conditions, EM trained models can yield highly competitive results.
    Such models have other benefits as well: they are simple, robust, and computationally more attractive.
    In this paper, we concentrate on methods for deriving sufficiently good initial conditions for EMHMM learning.
    Our method for learning initial conditions for the p(tjw) distributions relies on a mixture of language specific models: a paradigmatic model of similar words (where similar words are words with similar inflection patterns), simple syntagmatic constraints (e.g., the sequence V-V is extremely rare in English).
    These are complemented by a linear lexical context model.
    Such models are simple to build and test.
    We present results for unsupervised PoS tagging of Hebrew text and for the common WSJ English test sets.
    We show that our method achieves state-ofthe-art 