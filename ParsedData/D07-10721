le data.
			The question of ?how many clusters (symbols)??
			has been tackled in the Bayesian nonparametricsliterature via Dirichlet process (DP) mixture mod els (Antoniak, 1974).
			DP mixture models have since been extended to hierarchical Dirichlet processes (HDPs) and HDP-HMMs (Teh et al, 2006; Beal et al., 2002) and applied to many different types of clustering/induction problems in NLP (Johnson et al., 2006; Goldwater et al, 2006).In this paper, we present the hierarchical Dirich let process PCFG (HDP-PCFG).
			a nonparametric Bayesian model of syntactic tree structures based on Dirichlet processes.
			Specifically, an HDP-PCFG is defined to have an infinite number of symbols; the Dirichlet process (DP) prior penalizes the use of more symbols than are supported by the training data.
			Note that ?nonparametric?
			does not mean ?noparameters?; rather, it means that the effective num ber of parameters can grow adaptively as the amount of data increases, which is a desirable property of a learning algori