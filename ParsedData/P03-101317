rfect tags condition).
    However, this comes at the price of an unacceptable reduction in coverage.
    When training the C&amp;R model, we included a variant that makes use of Lopar&#8217;s parameter pooling feature.
    We pooled the estimates for conjoined daughter categories, and for NP and PP mother categories.
    This is a way of taking the idiosyncrasies of the Negra annotation into account, and resulted in a small improvement in performance.
    The most surprising finding is that the best performance was achieved by the unlexicalized PCFG baseline model.
    Both lexicalized models (C&amp;R and Collins) performed worse than the baseline.
    This results is at odds with what has been found for English, where lexicalization is standardly reported to increase performance by about 10%.
    The poor performance of the lexicalized models could be due to a lack of sufficient training data: our Negra training set contains approximately 18,000 sentences, and is therefore significantly smaller than the Pen