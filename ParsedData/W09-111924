have trained our system both on the training and on the development set, which gave us our best F1 score of 90.8 on the CoNLL03 data, yet it failed to improve the performance on other datasets.
    Table 5 summarizes the performance of the system.
    Next, we have compared the performance of our system to that of the Stanford NER tagger, across the datasets discussed above.
    We have chosen to compare against the Stanford tagger because to the best of our knowledge, it is the best publicly available system which is trained on the same data.
    We have downloaded the Stanford NER tagger and used the strongest provided model trained on the CoNLL03 data with distributional similarity features.
    The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al., 2005).
    Our goal was to compare the performance of the taggers across several datasets.
    For the most realistic comparison, we have presented each system with a raw text, and relied on the system&#8217;s 