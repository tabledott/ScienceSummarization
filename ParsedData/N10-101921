e model for prepositions, and just about at the same level as the language model for articles.
    Note, though, that the manual tuning was performed to optimize performance against a different data set (the Chinese Learners of English Corpus: CLEC), so the latter point is not really comparable and hence is not included in the charts.
    We now turn to the question of the required amount of annotated training data for the metaclassifier.
    CLC is commercially available, but it is obvious that for many researchers such a corpus will be too expensive and they will have to create or license their own error-annotated corpus.
    Thus the question of whether one could use less annotated data to train a meta-classifier and still achieve reasonable results becomes important.
    Figure 3 and Figure 4 show results obtained by using decreasing amounts of training data.
    The dotted line shows the language model baseline.
    Any result below the language model performance shows that the training data is insuffici