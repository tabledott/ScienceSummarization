k.
    To evaluate system performance NIST assessors who created the `ideal' written summaries did pairwise comparisons of their summaries to the system-generated summaries, other assessors' summaries, and baseline summaries.
    They used the Summary Evaluation Environment (SEE) 2.0 developed by (Lin 2001) to support the process.
    Using SEE, the assessors compared the system's text (the peer text) to the ideal (the model text).
    As shown in Figure 1, each text was decomposed into a list of units and displayed in separate windows.
    SEE 2.0 provides interfaces for assessors to judge both the content and the quality of summaries.
    To measure content, assessors step through each model unit, mark all system units sharing content with the current model unit (green/dark gray highlight in the model summary window), and specify that the marked system units express all, most, some, or hardly any of the content of the current model unit.
    To measure quality, assessors rate grammaticality3, cohesion4, and