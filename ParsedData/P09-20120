
  Bayesian Learning of a Tree Substitution Grammar
  
    Tree substitution grammars (TSGs) offer many advantages over context-free grammars (CFGs), but are hard to learn.
    Past approaches have resorted to heuristics.
    In this paper, we learn a TSG using Gibbs sampling with a nonparametric prior to control subtree size.
    The learned grammars perform significantly better than heuristically extracted ones on parsing accuracy.
  
  
    Tree substition grammars (TSGs) have potential advantages over regular context-free grammars (CFGs), but there is no obvious way to learn these grammars.
    In particular, learning procedures are not able to take direct advantage of manually annotated corpora like the Penn Treebank, which are not marked for derivations and thus assume a standard CFG.
    Since different TSG derivations can produce the same parse tree, learning procedures must guess the derivations, the number of which is exponential in the tree size.
    This compels heuristic methods of subtree extrac