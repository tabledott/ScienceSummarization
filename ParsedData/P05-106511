SVM).
    For example, perplexity (PP) is an information-theoretic measure often used to assess language models: where H(t|c) is the entropy relative to class c of a length m word sequence t = w1, ..., wm, defined as Low perplexity indicates a better match between the test data and the model, corresponding to a higher probability P(t|c).
    Perplexity scores are used as features in the SVM model described in Section 4.3.
    The likelihood ratio described above could also be used as a feature, but we achieved better results using perplexity.
    Feature selection is a common part of classifier design for many classification problems; however, there are mixed results in the literature on feature selection for text classification tasks.
    In CollinsThompson and Callan&#8217;s work (2004) on readability assessment, LM smoothing techniques are more effective than other forms of explicit feature selection.
    However, feature selection proves to be important in other text classification work, e.g.
    Lee and 