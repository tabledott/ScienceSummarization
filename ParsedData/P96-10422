e-based sample selection approach to minimizing training cost (Dagan and Engelson, 1995).
    When using sample selection, a learning program examines many unlabeled (not annotated) examples, selecting for labeling only those that are most informative for the learner at each stage of training (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993; Lewis and Gale, 1994; Cohn, Atlas, and Ladner, 1994).
    This avoids redundantly annotating many examples that contribute roughly the same information to the learner.
    Our work focuses on sample selection for training probabilistic classifiers.
    In statistical NLP, probabilistic classifiers are often used to select a preferred analysis of the linguistic structure of a text (for example, its syntactic structure (Black et al., 1993), word categories (Church, 1988), or word senses (Gale, Church, and Yarowsky, 1993)).
    As a representative task for probabilistic classification in NLP, we experiment in this paper with sample selection for the popular and well