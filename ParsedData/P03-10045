tails of this construction here) and the calculation of the decision function only require the evaluation of dot products &#966;(xi)&#183;&#966;(x).
    This is critical, since, in some cases, the dot products can be evaluated by a simple Kernel Function: K(x1, x2) = &#966;(x1) &#183; &#966;(x2).
    Substituting kernel function into (1), we have the following decision function.
    One of the advantages of kernels is that they are not limited to vectorial object x, but that they are applicable to any kind of object representation, just given the dot products.
  
  
    For many tasks in NLP, the training and test examples are represented in binary vectors; or sets, since examples in NLP are usually represented in socalled Feature Structures.
    Here, we focus on such cases 1.
    Suppose a feature set F = {1, 2, ... , N} and training examples Xj(j = 1, 2, ... , L), all of which are subsets of F (i.e., Xj C_ F).
    In this case, Xj can be regarded as a binary vector xj = (xj1, xj2, ... , xjN) where xji = 1 