 note that human annotators label arguments based on perfect entity mentions, but our system used the output from the IE system.
    So the gap was also partially due to worse entity detection.
    Error analysis on the inference procedure shows that the propagation rules (3), (4), (7) and (9) produced a few extra false alarms.
    For trigger labeling, most of these errors appear for support verbs such as &#8220;take&#8221; and &#8220;get&#8221; which can only represent an event mention together with other verbs or nouns.
    Some other errors happen on nouns and adjectives.
    These are difficult tasks even for human annotators.
    As shown in table 5 the inter-annotator agreement on trigger identification is only about 40%.
    Besides some obvious overlooked cases (it&#8217;s probably difficult for a human to remember 33 different event types during annotation), most difficulties were caused by judging generic verbs, nouns and adjectives.
    In fact, compared to a statistical tagger trained on the corp