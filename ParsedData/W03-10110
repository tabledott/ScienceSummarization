
  A General Framework For Distributional Similarity
  
    We present a general framework for distributional similarity based on the concepts of precision and recall.
    Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored.
    We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.
  
  
    There are many potential applications of sets of distributionally similar words.
    In the syntactic domain, language models, which can be used to evaluate alternative interpretations of text and speech, require probabilistic information about words and their co-occurrences which is often not available due to the sparse data problem.
    In order to overcome this problem, researchers (e.g.
    Pereira et al. (1993)) have proposed estimating probabilities based on sets of words which are known to be distributio