own experience is consistent with that of Krippendorff: Both in our earlier work (Poesio and Vieira 1998; Poesio 2004a) and in the more recent efforts (Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation of reasonable quality (Poesio 2004a).
    We therefore feel that if a threshold needs to be set, 0.8 is a good value.
    That said, we doubt that a single cutoff point is appropriate for all purposes.
    For some CL studies, particularly on discourse, useful corpora have been obtained while attaining reliability only at the 0.7 level.
    We agree therefore with Craggs and McGee Wood (2005) that setting a specific agreement threshold should not be a prerequisite for publication.
    Instead, as recommended by Di Eugenio and Glass (2004) and others, researchers should report in detail on the methodology that was followed in collecting the reliability data (number of coders, whether they coded independently, whether they relied exclusively on an annotation manual), whether agre