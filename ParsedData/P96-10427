on of the example, the greater our certainty in its classification.
    This is because when the training data entails a specific classification with high certainty, most (in a probabilistic sense) classifiers consistent with the data will produce that classification.
    The committee-based approach was first proposed in a theoretical context for learning binary nonprobabilistic classifiers (Seung, Opper, and Sompolinsky, 1992; Freund et al., 1993).
    In this paper, we extend our previous work (Dagan and Engelson, 1995) where we applied the basic idea of the committee-based approach to probabilistic classification.
    Taking a Bayesian perspective, the posterior probability of a model, P(MIS), is determined given statistics S from the training set (and some prior distribution for the models).
    Committee members are then generated by drawing models randomly from POI IS).
    An example is selected for labeling if the committee members largely disagree on its classification.
    This procedure assumes th