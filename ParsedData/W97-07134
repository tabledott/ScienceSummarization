ut In very few cases, the direct output of a summarization program is compared with a human-made summary or evaluated with the help of human subjects, usually, the results are modest Unfortunately, evaluating the results of a particular implementation does not enable one to determine what part of the failure is due to the implementation itself and what part to its underlying assumptions The position that we take in this paper is that, in order to build high-quality summarization programs, one needs to evaluate not only a representative set of automatically generated outputs (a highly difficult problem by itself), but also the adequacy of the assumptions that these programs use That way, one is able to distinguish the problems that pertain to a particular implementation from those that pertain to the underlying theoretical framework and explore new ways to improve each With few exceptions, automatic approaches to summarization have primarily addressed possible ways to determine the most important parts of a te