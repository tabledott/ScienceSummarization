
  KenLM: Faster and Smaller Language Model Queries
  
    We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and costs.
    The structure uses linear probing hash tables and is designed for speed.
    Compared with the widely- SRILM, our is 2.4 times as fast while using 57% of the mem- The structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed lower memory consumption. simultaneously uses less memory than the smallest lossless baseline and less CPU than the baseline.
    Our code is thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.
    This paper describes the several performance techniques used and presents benchmarks against alternative implementations.
  
  
    Language models are widely applied in natural language processing, and applications such as machine translation make very frequent queries.
    This paper presents methods to query N-gram 