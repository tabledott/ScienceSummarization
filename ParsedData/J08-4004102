nclude by summarizing what in our view are the main recommendations emerging from ten years of experience with coefficients of agreement.
    These can be grouped under three main headings: methodology, choice of coefficients, and interpretation of coefficients.
    Our first recommendation is that annotation efforts should perform and report rigorous reliability testing.
    The last decade has already seen considerable improvement, from the absence of any tests for the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993) or the British National Corpus (Leech, Garside, and Bryant 1994) to the central role played by reliability testing in the Penn Discourse Treebank (Miltsakaki et al. 2004) and OntoNotes (Hovy et al.
    2006).
    But even the latter efforts only measure and report percent agreement.
    We believe that part of the reluctance to report chance-corrected measures is the difficulty in interpreting them.
    However, our experience is that chancecorrected coefficients of agreement do provid