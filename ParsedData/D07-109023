ure the size of language models in total number of n-grams, summed over all orders from 1 to 5.
			There is no frequency cutoff on the n-grams.
			5http://www.nist.gov/speech/tests/mt/doc/ LDCLicense-mt06.pdf contains a list of parallel resources provided by LDC.
			6The bigger sets included are LDC2005T12 (Gigaword, 2.5B tokens), LDC93T3A (Tipster, 500M tokens) and LDC2002T31 (Acquaint, 400M tokens), plus many smaller sets.
			7The test data was generated after 1-Feb-2006; all training data was generated before that date.
			target webnews web # tokens 237M 31G 1.8T vocab size 200k 5M 16M # n-grams 257M 21G 300G LM size (SB) 2G 89G 1.8T time (SB) 20 min 8 hours 1 day time (KN) 2.5 hours 2 days ? # machines 100 400 1500 Table 2: Sizes and approximate training times for 3 language models with Stupid Backoff (SB) and Kneser-Ney Smoothing (KN).There is, however, a frequency cutoff on the vocabulary.
			The minimum frequency for a term to be in cluded in the vocabulary is 2 for the target, ldcnews and webnews dat