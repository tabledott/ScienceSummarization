we always get the same degree of merging per word across all sentence groups, the number of paths would tend to increase with the sentence length.
    This is not the case here.
    Apparently we are getting less merging with longer sentences.
    But still, given 11 sentences, we are capable of generating hundreds, thousands, and in some cases even millions of sentences.
    Obviously, we should not get too happy with our ability to boost the number of equivalent meanings if they are incorrect.
    To assess the quality of the FSAs generated by our algorithm, we use a language model-based metric.
    We train a 4-gram model over one year of the Wall Street Journal using the CMU-Cambridge Statistical Language Modeling toolkit (v2).
    For each sentence group SG, we use this language model to estimate the average entropy of the 11 original sentences in that group (ent(SG)).
    We also compute the average entropy of all the sentences in the corresponding FSA built by our syntax-based algorithm (ent(FSA)).
   