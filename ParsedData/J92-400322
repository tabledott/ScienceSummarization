ies in the text as a whole.
    Looking at the classes in Tables 2 and 3, we feel that this is reasonable for pairs like John and George or liberal and conservative but perhaps less so for pairs like little and prima or Minister and mover.
    We used these classes to construct an interpolated 3-gram class model using the same training text and held-out data as we used for the word-based language model we discussed above.
    We measured the perplexity of the Brown corpus with respect to this model and found it to be 271.
    We then interpolated the class-based estimators with the word-based estimators and found the perplexity of the test data to be 236, which is a small improvement over the perplexity of 244 we obtained with the word-based model.
  
  
    In the previous section, we discussed some methods for grouping words together according to the statistical similarity of their surroundings.
    Here, we discuss two additional types of relations between words that can be discovered by examining various 