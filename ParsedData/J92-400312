riday.
    Of Peter F. Brown and Vincent J. Della Pietra Class-Based n-gram Models of Natural Language course, they will not be identical: we rarely hear someone say Thank God it's Thursday! or worry about Thursday the 13th.
    If we can successfully assign words to classes, it may be possible to make more reasonable predictions for histories that we have not previously seen by assuming that they are similar to other histories that we have seen.
    Suppose that we partition a vocabulary of V words into C classes using a function, 7r, which maps a word, wi, into its class, ci.
    We say that a language model is an ngram class model if it is an n-gram language model and if, in addition, for 1 &lt; k &lt; n, independent parameters: V - C of the form Pr (w j c,), plus the C&amp;quot; - 1 independent parameters of an n-gram language model for a vocabulary of size C. Thus, except in the trivial cases in which C --= V or n 1, an n-gram class language model always has fewer independent parameters than a general n-