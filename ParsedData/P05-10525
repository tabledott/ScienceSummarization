unctions, but we can do more than that with kernels.
    First, there are many wellknown kernels, such as polynomial and radial basis kernels, which extend normal features into a high order space with very little computational cost.
    This could make a linearly non-separable problem separable in the high order feature space.
    Second, kernel functions have many nice combination properties: for example, the sum or product of existing kernels is a valid kernel.
    This forms the basis for the approach described in this paper.
    With these combination properties, we can combine individual kernels representing information from different sources in a principled way.
    Many classifiers can be used with kernels.
    The most popular ones are SVM, KNN, and voted perceptrons.
    Support Vector Machines (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) are linear classifiers that produce a separating hyperplane with largest margin.
    This property gives it good generalization ability in high-dimensional sp