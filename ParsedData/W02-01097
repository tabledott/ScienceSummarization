 students can use their NLP systems to perform real tasks.
    Cleverness.
    Clear designs and implementations are far preferable to ingenious yet indecipherable ones.
  
  
    The toolkit is implemented as a collection of independent modules, each of which defines a specific data structure or task.
    A set of core modules defines basic data types and processing systems that are used throughout the toolkit.
    The token module provides basic classes for processing individual elements of text, such as words or sentences.
    The tree module defines data structures for representing tree structures over text, such as syntax trees and morphological trees.
    The probability module implements classes that encode frequency distributions and probability distributions, including a variety of statistical smoothing techniques.
    The remaining modules define data structures and interfaces for performing specific NLP tasks.
    This list of modules will grow over time, as we add new tasks and algorithms to the t