l machine translation system.
    The intent of this task was to get researchers who develop automatic evaluation metrics for MT to work on the problem of using their metric to optimize the parameters of MT systems.
    Previous workshops have demonstrated that a number of metrics perform better than BLEU in terms of having stronger correlation with human judgments about the rankings of multiple machine translation systems.
    However, most MT system developers still optimize the parameters of their systems to BLEU.
    Here we aim to investigate the question of whether better metrics will result in better quality output when a system is optimized to them.
    Because this was the first year that we ran the tunable metrics task, participation was limited to a few groups on an invitation-only basis.
    Table 16 lists the participants in this task.
    Metrics developers were invited to integrate their evaluation metric into a MERT optimization routine, which was then used to tune the parameters of a fixed st