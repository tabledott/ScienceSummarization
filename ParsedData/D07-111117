l resources was allowed.
			See (Nivre et al, 2007).
	
	
			One of the main assumptions in our use of differ ent models based on the same algorithm is that while the output generated by those models may often differ, agreement between the models is an indication of correctness.
			In our domain adapta tion approach, this was clearly true.
			In fact, the approach would not have worked if this assump tion was false.
			Experiments on the development set were encouraging.
			As stated before, when the parsers agreed, labeled attachment score was over 90, even though the score of each model alone was lower than 79.
			The domain-adapted parser had a score of 82.1, a significant improvement.
			Interes tingly, the ensemble used in the multilingual track also produced good results on the development set for the domain adaptation data, without the use of the unlabeled data at all, with a score of 81.9 (al though the ensemble is more expensive to run).
			The different models used in each track were distinct in a fe