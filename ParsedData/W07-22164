 partially adverted by incorporating rich feature representations over the input (McDonald et al., 2005a).
    The goal of this work is to further our current understanding of the computational nature of nonprojective parsing algorithms for both learning and inference within the data-driven setting.
    We start by investigating and extending the edge-factored model of McDonald et al. (2005b).
    In particular, we appeal to the Matrix Tree Theorem for multi-digraphs to design polynomial-time algorithms for calculating both the partition function and edge expectations over all possible dependency graphs for a given sentence.
    To motivate these algorithms, we show that they can be used in many important learning and inference problems including min-risk decoding, training globally normalized log-linear models, syntactic language modeling, and unsupervised learning via the EM algorithm &#8211; none of which have previously been known to have exact non-projective implementations.
    We then switch focus to m