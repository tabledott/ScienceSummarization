e) is a ground tuple in the database and s =&#8220;Steve Jobs founded Apple, Inc.&#8221; is a sentence containing synonyms for both e1 = Jobs and e2 = Apple, then s may be a natural language expression of the fact that r(e1, e2) holds and could be a useful training example.
    While weak supervision works well when the textual corpus is tightly aligned to the database contents (e.g., matching Wikipedia infoboxes to associated articles (Hoffmann et al., 2010)), Riedel et al. (2010) observe that the heuristic leads to noisy data and poor extraction performance when the method is applied more broadly (e.g., matching Freebase records to NY Times articles).
    To fix this problem they cast weak supervision as a form of multi-instance learning, assuming only that at least one of the sentences containing e1 and e2 are expressing r(e1, e2), and their method yields a substantial improvement in extraction performance.
    However, Riedel et al.&#8217;s model (like that of previous systems (Mintz et al., 2009)) assume