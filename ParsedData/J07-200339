t al.
    2004), and Hiero itself run as a conventional phrase-based system with monotone translation (no phrase reordering).
    The ATS baseline was trained on all the parallel data listed in Table 1, for a total of 159 million words (English side).
    The second language model was also trained on the English side of the whole bitext.
    Phrases of up to 10 in length on the French side were extracted from the parallel text, and minimum-error-rate training (Och 2003) was performed on the development set for 17 features, the same as used in the NIST 2004 and 2005 evaluations.9 These features are similar to the features used for our system, but also include features for phrase-reordering (which are not applicable to our system), IBM Model 1 in both directions, a missing word penalty, and a feature that controls a fallback lexicon.
    The other baseline, which we call Hiero Monotone, is the same as Hiero except with the limitation that extracted rules cannot have any nonterminal symbols on their righthand si