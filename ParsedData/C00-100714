ition using the Viterl)i algorithm, ;m(t this to I) ranking word sequence is the outt)ut of the LP Chooser.
  4 Experiments and Results In order l:o show |;ll~tl; Lhe llSO, of ~t tl:ce lIlode] trod a, grmmnar doe.s indeed hell) pe, rformmme, we pe.rforme, d three experiments: 45 Q Figure 6: Word lattice tbr example sentence after Tree Chooser and Unraveler using the supertag-based model ?
  For the baseline experiment, we impose a random tree structure ibr each sentence of the cortms and build a Tree Model whose parameters consist of whether a lexeme l~t precedes or tbllows her mother lexeme lm.
  We call this the Baseline Left-Right (LR) Model.
  This model generates There was est imate for  phase the second no cost .
  for our example input.
  In the second experiment, we derive the parmneters tbr the LR model flom an an- notated corpus, in particular, the XTAG derivation tree cortms.
  This model gener- ates Th, crc no est imate Jor the second phase was cost .
  tbr our example input.
  In the third experi