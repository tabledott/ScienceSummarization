ich is nonnegative, IF0c can also be used as a potential function.
    Thus, the conditional model for our SSL can be written as: where Z0(x) = Py&#8712;YQc&#8712;C V (yc, x; A0, &#920;).
    Hereafter in this paper, we refer to this conditional model as a &#8216;Joint probability model Embedding style SemiSupervised Conditional Model&#8217;, or JESS-CM for short.
    Given labeled data, Dl={(xn, yn)}Nn=1, the MAP estimation of A0 under a fixed &#920; can be written as: where p(A0) is a prior probability distribution of A0.
    Clearly, JESS-CM shown in Equation 2 has exactly the same form as Equation 1.
    With a fixed &#920;, the log-likelihood, log pj, can be seen simply as the feature functions of JESS-CM as with fi.
    Therefore, embedded joint PMs do not violate the global convergence conditions.
    As a result, as with supervised CRFs, it is guaranteed that A0 has a value that achieves the global maximum of L1(A0|&#920;).
    Moreover, we can obtain the same form of gradient as that of supervised CR