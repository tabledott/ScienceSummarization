
  Phrasetable Smoothing For Statistical Machine Translation
  
    We discuss different strategies for smoothing the phrasetable in Statistical MT, and give results over a range of translation settings.
    We show that any type of smoothing is a better idea than the relativefrequency estimates that are often used.
    The best smoothing techniques yield consistent gains of approximately 1% (absolute) according to the BLEU metric.
  
  
    Smoothing is an important technique in statistical NLP, used to deal with perennial data sparseness and empirical distributions that overfit the training corpus.
    Surprisingly, however, it is rarely mentioned in statistical Machine Translation.
    In particular, state-of-the-art phrase-based SMT relies on a phrasetable&#8212;a large set of ngram pairs over the source and target languages, along with their translation probabilities.
    This table, which may contain tens of millions of entries, and phrases of up to ten words or more, is an excellent candidate for smoot