roduced for the entire TREC 13 set, and also for the first 100questions from the training set TREC 8?12.9 On av erage, each question in the development set has 3.1 positive and 17.1 negative answers.
			There are 3.6 positive and 20.0 negative answers per question in the test set.We tokenized sentences using the standard tree bank tokenization script, and then we performedpart-of-speech tagging using MXPOST tagger (Ratnaparkhi, 1996).
			The resulting POS-tagged sentences were then parsed using MSTParser (McDon ald et al, 2005), trained on the entire Penn Treebank to produce labeled dependency parse trees (we used a coarse dependency label set that includes twelve label types).
			We used BBN Identifinder (Bikel et al, 1999) for named-entity tagging.As answers in our task are considered to be sin gle sentences, our evaluation differs slightly from TREC, where an answer string (a word or phrase like 1977 or George Bush) has to be accompaniedby a supporting document ID. As discussed by Punyakanok et al (2004), 