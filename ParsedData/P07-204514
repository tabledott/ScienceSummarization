tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors.
    This may encourage more syntactically correct output.
    In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas.
    Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006).
  
  
    Machine translation input currently takes the form of simple sequences of words.
    However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/speech processing tools (such as named entity recognizers, speech recognizers, morphological analyzers, etc.).
    These upstream processes tend to generate multiple, erroneous hypotheses with varying confidence.
    Current MT systems are designed to process only one input hypothesis, making them vulnerable to errors in the input.
    In experim