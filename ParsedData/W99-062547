 precision when recall is 39.1%, i.e., close to our system's recall; and only 20.8% recall at precision of 62.2%, comparable to our classifier's ilarity metrics.
    For comparison purposes, we list the average recall, precision, and accuracy obtained by TF*IDF and SMART at the two points in the precision-recall curve identified for each method in the text (i.e., the point where the method's precision is most similar to ours, and the point where its recall is most similar to ours). precision.
    SMART (in its default configuration) offered only a small improvement over the base TF*IDF implementation, and significantly underperformed our method, obtaining 34.1% precision at recall of 36.7%, and 21.5% recall at 62.4% precision.
    The default method of always marking a pair as dissimilar obtains of course 0% recall and undefined precision.
    Figure 5 illustrates the difference between our system and straight TF*IDF at different points of the precision-recall spectrum.
    When overall accuracy (total percen