tensive annotations (and their corresponding text) will be displayed.
    The output of the tool is written to an HTML file in tabular form, as shown in Figure 3.
    Current evaluations for the MUSE NE system are producing average figures of 90-95% Precision and Recall on a selection of different text types (spoken transcriptions, emails etc.).
    The default ANNIE system produces figures of between 80-90% Precision and Recall on news texts.
    This figure is lower than for the MUSE system, because the resources have not been tuned to a specific text type or application, but are intended to be adapted as necessary.
    Work on resolution of anaphora is currently averaging 63% Precision and 45% Recall, although this work is still very much in progress, and we expect these figures to improve in the near future.
    7 Related Work GATE draws from a large pool of previous work on infrastructures, architectures and development environments for representing and processing language resources, corpora, and annotat