
  Building Deep Dependency Structures Using A Wide-Coverage CCG Parser
  
    This paper describes a wide-coverage statistical parser that uses Combinatory Categorial Grammar (CCG) to derive dependency structures.
    The parser differs from most existing wide-coverage treebank parsers in capturing the long-range dependencies inherent in constructions such as coordination, extraction, raising and control, as well as the standard local predicate-argument dependencies.
    A set of dependency structures used for training and testing the parser is obtained from a treebank of CCG normal-form derivations, which have been derived (semi-) automatically from the Penn Treebank.
    The parser correctly recovers over 80% of labelled dependencies, and around 90% of unlabelled dependencies.
  
  
    Most recent wide-coverage statistical parsers have used models based on lexical dependencies (e.g.
    Collins (1999), Charniak (2000)).
    However, the dependencies are typically derived from a context-free phrase structu