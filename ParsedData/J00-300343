 six DA classes.
    The tree achieved a classification accuracy of 45.4% on an independent test set with the same uniform six-class distribution.
    The chance accuracy on this set is 16.6%, so the tree clearly extracts useful information from the prosodic features.
    We then used the decision tree posteriors as scaled DA likelihoods in the dialogue model HMM, combining it with various n-gram dialogue grammars for testing on our full standard test set.
    For the purpose of model integration, the likelihoods of the Other class were assigned to all DA types comprised by that class.
    As shown in Table 7, the tree with dialogue grammar performs significantly better than chance on the raw DA distribution, although not as well as the word-based methods (cf.
    Table 6). networks compare to decision trees for the type of data studied here.
    Neural networks are worth investigating since they offer potential advantages over decision trees.
    They can learn decision surfaces that lie at an angle to the a