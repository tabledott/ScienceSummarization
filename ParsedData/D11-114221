ithm described in Section 2.
  
  
    We compare REVERB to the following systems: Each system is given a set of sentences as input, and returns a set of binary extractions as output.
    We created a test set of 500 sentences sampled from the Web, using Yahoo&#8217;s random link service.3 After running each extractor over the input sentences, two human judges independently evaluated each extraction as correct or incorrect.
    The judges reached agreement on 86% of the extractions, with an agreement score of n = 0.68.
    We report results on the subset of the data where the two judges concur.
    The judges labeled uninformative extractions conservatively.
    That is, if critical information was dropped from the relation phrase but included in the second argument, it is labeled correct.
    For example, both the extractions (Ackerman, is a professor of, biology) and (Ackerman, is, a professor of biology) are considered correct.
    Each system returns confidence scores for its extractions.
    For a given 