their partial derivatives.
    Recall that (Z, r) is computed by INSIDE(HG, ER,Rn) when the edge weights are (pe, pere) with re E Rn.
    Lift these weights to ((pe,pere),V(pe,pere)) = swapping the second and third components of the 4-tuple and transposing the matrix in the fourth component.
    Algebraically, this changes nothing because and are isomorphic, thanks to symmetries in Table 2.
    This method computes the expectation of the gradient rather than the gradient of the expectation&#8212;they are equal. relied on the fact that this relationship still holds even when the scalars Z, are replaced by more complex objects that we wish to differentiate.
    Our discussion below sticks to the scalar case for simplicity, but would generalize fairly straightforwardly. seem wonderful and mysterious.
    We now show in two distinct ways why this follows from our setup of Section 3.1.
    At the end, we derive as a special case the well-known relationship between gradients and expectations in log-linear models.
 