M Model 4.
    Therefore, Q0jl jr eb et denotes the probability of the best monotone hypothesis of IBM Model 4.
    Alternatively, we could use any other single-word based lexicon as well as phrasebased models for this initialization.
    Our choice is the IBM Model4 to make the results as comparable as possible to the search with the IBM constraints.
    We introduce a new parameter pm (m=&#710; monotone), which denotes the probability of a monotone combination of two partial hypotheses.
    We formulated this equation for a bigram language model, but of course, the same method can also be applied for a trigram language model.
    The resulting algorithm is similar to the CYK-parsing algorithm.
    It has a worst-case complexity of O(J3 ' E4).
    Here, J is the length of the source sentence and E is the vocabulary size of the target language.
    Although the described search algorithm has a polynomial-time complexity, even with a bigram language model the search space is very large.
    A full search is po