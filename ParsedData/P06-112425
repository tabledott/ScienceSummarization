e plan to look into these possible improvements and extensions.
    The hierarchical Dirichlet language model of (MacKay and Peto, 1994) was an inspiration for our work.
    Though (MacKay and Peto, 1994) had the right intuition to look at smoothing techniques as the outcome of hierarchical Bayesian models, the use of the Dirichlet distribution as a prior was shown to lead to non-competitive cross-entropy results.
    Our model is a nontrivial but direct generalization of the hierarchical Dirichlet language model that gives state-of-the-art performance.
    We have shown that with a suitable choice of priors (namely the Pitman-Yor process), Bayesian methods can be competitive with the best smoothing techniques.
    The hierarchical Pitman-Yor process is a natural generalization of the recently proposed hierarchical Dirichlet process (Teh et al., 2006).
    The hierarchical Dirichlet process was proposed to solve a different problem&#8212;that of clustering, and it is interesting to note that such a direct gen