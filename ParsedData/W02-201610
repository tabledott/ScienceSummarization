achines (Vapnik,1998) for our experiments because of their state-of-the-art performance and generalization ability.
    SVM is a binary linear classifier trained from the samples, each of which belongs either to positive or negative class as follows: (x1, y1), ... , (xl, yl) (xi E Rn, yi E {+1, &#8722;1}), where xi is a feature vector of the i-th sample represented by an n dimensional vector, and yi is the class (positive(+1) or negative(&#8722;1) class) label of the i-th sample.
    SVMs find the optimal separating hyperplane (w &#8226; x + b) based on the maximal margin strategy.
    The margin can be seen as the distance between the critical examples and the separating hyperplane.
    We omit the details here, the maximal margin strategy can be realized by the following optimization problem:
  
  
    Furthermore, SVMs have the potential to carry out non-linear classifications.
    Though we leave the details to (Vapnik, 1998), the optimization problem can be rewritten into a dual form, where all feature v