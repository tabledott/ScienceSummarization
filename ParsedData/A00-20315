on 3, which is exactly equivalent to equation 1 since everything except the last numerator cancels out of the equation.
    The value of j is chosen such that features Ii fi are sufficiently represented in the training data; sometimes all n features are used, but often that would cause sparse data problems.
    Smoothing is performed on this equation exactly as before: each term is interpolated between the empirical value and the prior estimated probability, according to a value of Ai that estimates confidence.
    But aside from perhaps providing a new way to think about the problem, equation 3 is not particularly useful as it is&#8212;it is exactly the same as what we had before.
    Its real usefulness comes, as shown in (Charniak, 1999), when we move from the notion of a feature chain to a feature tree.
    These feature chains don't capture everything we'd like them to.
    If there are two independent features that are each relatively sparse but occasionally carry a lot of information, then putting one 