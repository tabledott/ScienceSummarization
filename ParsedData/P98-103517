ically significant according to a sign test.
    Simple linear interpolation between our model and the trigram model: yielded a further improvement in PPL, as shown in Table 2.
    The interpolation weight was estimated on check data to be A = 0.36.
    An overall relative reduction of 11% over the trigram model has been achieved.
  
  
    The large difference between the perplexity of our model calculated on the &amp;quot;development&amp;quot; set &#8212; used for model parameter estimation &#8212; and &amp;quot;test&amp;quot; set &#8212; unseen data &#8212; shows that the initial point we choose for the parameter values has already captured a lot of information from the training data.
    The same problem is encountered in standard n-gram language modeling; however, our approach has more flexibility in dealing with it due to the possibility of reestimating the model parameters.
    We believe that the above experiments show the potential of our approach for improved language models.
    Our future plans in