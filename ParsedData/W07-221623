 is stated for the model in Paskin (2001); a similar treatment can be developed for the model in Klein and Manning (2004).
    A generative model of dependency structure might be used to determine the probability of a sentence x by marginalizing out all possible dependency trees, This probability can be used directly as a nonprojective syntactic language model (Chelba et al., 1997) or possibly interpolated with a separate ngram model.
    In unsupervised learning we train our model on a sample of unannotated sentences X = {x&#945;&#65533;|X | &#945;=1.
    Let |x&#945; |= n&#945; and p(T |n&#945;) = &#946;&#945;.
    We choose the parameters that maximize the log-likelihood viewed as a function of the parameters and subject to the normalization conditions, i.e., Py,k pkx,y = 1 and pk x,y &gt; 0.
    Let x&#945;i be the ith word of x&#945;.
    By solving the above constrained optimization problem with the usual Lagrange multipliers method one gets where for each x&#945; the expectation ((i, j)k)x&#171; is def