on of Bayes' rule.
    The denominator is eliminated in the usual fashion, resulting in equation (2).
    Selectively applying the chain rule results in equations (3) and (4).
    In equation (4), the term P(h,t,lia, sp, cr) is the same for every antecedent and is thus removed.
    Equation (6) follows when we break the last component of (5) into two probability distributions.
    In equation (7) we make the following independence assumptions: P(sp, data, = P (s p, data) Then we combine so and dc, into one variable dH , Hobbs distance, since the Hobbs algorithm takes both the syntax and distance into account.
    Since_ 1-4.7 is a vector, we need to normalize P(W1h,t,l, a) to obtain the probability of each element in the vector.
    It is reasonable to assume that the antecedents in W are independent of each other; in other words, P(Wa-4-11W0i h, t, I, a) = P (wp+ilh, t, I , a).
    Thus, where Now we arrive at the final equation for computing the probability of each proposed antecedent: We obtain P(dHla) by 