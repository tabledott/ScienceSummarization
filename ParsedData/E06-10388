compression, the core of which is a decoding algorithm that searches the entire space of compressions.
    Let the score of a compression y for a sentence x as In particular, we are going to factor this score using a first-order Markov assumption on the words in the compressed sentence Finally, we define the score function to be the dot product between a high dimensional feature representation and a corresponding weight vector Note that this factorization will allow us to define features over two adjacent words in the compression as well as the words in-between that were dropped from the original sentence to create the compression.
    We will show in Section 3.2 how this factorization also allows us to include features on dropped phrases and subtrees from both a dependency and a phrase-structure parse of the original sentence.
    Note that these features are meant to capture the same information in both the source and channel models of Knight and Marcu (2000).
    However, here they are merely treated as ev