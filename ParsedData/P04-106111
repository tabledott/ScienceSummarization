rites and let the interaction between the data and the model structure break the initial symmetry.
    If one recasts their experiments in this way, they achieve an accuracy of 44.7% on the Penn treebank, which is higher than choosing a random dependency structure, but lower than simply linking all adjacent words into a left-headed (and right-branching) structure (53.2%).
    A huge limitation of both of the above models is that they are incapable of encoding even first-order valence facts.
    For example, the latter model learns that nouns to the left of the verb (usually subjects) attach to the verb.
    But then, given a NOUN NOUN VERB sequence, both nouns will attach to the verb &#8211; there is no way that the model can learn that verbs have exactly one subject.
    We now turn to an improved dependency model that addresses this problem.
  
  
    The dependency models discussed above are distinct from dependency models used inside highperformance supervised probabilistic parsers in several ways.
    Fi