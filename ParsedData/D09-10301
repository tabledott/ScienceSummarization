 conduct.
    Instead, researchers routinely use automatic metrics like Bleu (Papineni et al., 2002) as the sole evidence of improvement to translation quality.
    Automatic metrics have been criticized for a variety of reasons (Babych and Hartley, 2004; Callison-Burch et al., 2006; Chiang et al., 2008), and it is clear that they only loosely approximate human judgments.
    Therefore, having people evaluate translation output would be preferable, if it were more practical.
    In this paper we demonstrate that the manual evaluation of translation quality is not as expensive or as time consuming as generally thought.
    We use Amazon&#8217;s Mechanical Turk, an online labor market that is designed to pay people small sums of money to complete human intelligence tests &#8211; tasks that are difficult for computers but easy for people.
    We show that:
  
  
    Snow et al. (2008) examined the accuracy of labels created using Mechanical Turk for a variety of natural language processing tasks.
    These tasks