algorithm is compared to the standard annotation.
    Since the learned algorithm classifies all cases, the number of responses is equal to the number of cases, as a consequence, recall is the same as precision, and so is the F measure.
    The tests over 6 texts with 195 definite descriptions gave the following results: The best results were achieved by the algorithm trained for two classes only.
    This is not surprising, especially considering how difficult it was for our subjects to distinguish between discourse-new and bridging descriptions.
    The hand-crafted decision tree (Version 2) achieved 62% recall and 85% precision (F = 71.70%) on those same texts: i.e., a higher precision, but a lower F measure, due to a lower recall, since&#8212;unlike the learned algorithm&#8212;it does not classify all instances of definite descriptions.
    If, however, we take the class discourse-new as a default for all cases of definite descriptions not resolved by the system, recall, precision, and F value go to 77%, 