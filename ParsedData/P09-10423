etter conserved in translation than phrase structure (Fox, 2002; Hwa et al., 2005).
    Nevertheless, several challenges to accurate training and evaluation from aligned bitext remain: (1) partial word alignment due to non-literal or distant translation; (2) errors in word alignments and source language parses, (3) grammatical annotation choices that differ across languages and linguistic theories (e.g., how to analyze auxiliary verbs, conjunctions).
    In this paper, we present a flexible learning framework for transferring dependency grammars via bitext using the posterior regularization framework (Gra&#231;a et al., 2008).
    In particular, we address challenges (1) and (2) by avoiding commitment to an entire projected parse tree in the target language during training.
    Instead, we explore formulations of both generative and discriminative probabilistic models where projected syntactic relations are constrained to hold approximately and only in expectation.
    Finally, we address challenge (3) by int