posed a novel and effective learning scheme for transferring dependency parses across bitext.
    By enforcing projected dependency constraints approximately and in expectation, our framework allows robust learning from noisy partially supervised target sentences, instead of committing to entire parses.
    We show that discriminative training generally outperforms generative approaches even in this very weakly supervised setting.
    By adding easily specified languagespecific constraints, our models begin to rival strong supervised baselines for small amounts of data.
    Our framework can handle a wide range of constraints and we are currently exploring richer syntactic constraints that involve conservation of multiple edge constructions as well as constraints on conservation of surface length of dependencies.
  
  
    This work was partially supported by an Integrative Graduate Education and Research Traineeship grant from National Science Foundation (NSFIGERT 0504487), by ARO MURI SUBTLE W911NF-07-1-021