ssifiers are trained to make the best local decision, unlike generative models they cannot trade off decisions at different positions against each other.
    In other words, sequential classifiers are myopic about the impact of their current decision on later decisions (Bottou, 1991; Lafferty et al., 2001).
    This forced the best sequential classifier systems to resort to heuristic combinations of forward-moving and backward-moving sequential classifiers (Kudo and Matsumoto, 2001).
    Conditional random fields (CRFs) bring together the best of generative and classification models.
    Like classification models, they can accommodate many statistically correlated features of the inputs, and they are trained discriminatively.
    But like generative models, they can trade off decisions at different sequence positions to obtain a globally optimal labeling.
    Lafferty et al. (2001) showed that CRFs beat related classification models as well as HMMs on synthetic data and on a part-of-speech tagging task.
    