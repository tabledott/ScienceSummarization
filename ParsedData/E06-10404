t it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable.
    Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts.
    Similar evaluations have been used e.g.
    by Bangalore et al. (2000) and Marciniak and Strube (2004).
    Such corpus-based evaluations have sometimes been criticised in the NLG community, for example by Reiter and Sripada (2002).
    Grounds for criticism include the fact that regenerating a parsed text is not a realistic NLG task; that texts can be very different from a corpus text but still effectively meet the system&#8217;s communicative goal; and that corpus texts are often not of high enough quality to form a realistic test.
    The MT and document summarisation communities have developed evaluation metrics based on comparing output texts to a corpus of human texts, and have sh