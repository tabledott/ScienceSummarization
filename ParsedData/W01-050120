that co-training is an effective method for learning bracketers from small amounts of labeled data.
    Naturally, the resulting classifier does not perform as well as a fully supervised classifier trained on hundreds of times as much labeled data, but if the difference in accuracy is less important than the effort required to produce the labeled training data, co-training is especially attractive.
    Furthermore, our experiments support the hypothesis that labeled data quality is a crucial issue for co-training.
    Our moderately supervised variant, corrected co-training, maintains labeled data quality without unduly increasing the burden on the human annotator.
    Corrected co-training bridges the gap in accuracy between weak initial classifiers and fully supervised classifiers.
    Finally, as an approach to resolving the tension in weakly supervised learning between accumulating accurate training data and covering the desired task, we suggest combining weakly supervised methods such as co-training or s