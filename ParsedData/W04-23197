3 and c6, and speaker c6 has interacted with speakers c2 and c5.
  
  
    We computed interlabeler reliability among the three labelers for both segmentation (into DA units) and DA labeling, using randomly selected excerpts from the 75 labeled meetings.
    Since agreement on DA segmentation does not appear to have standard associated metrics in the literature, we developed our own approach.
    The philosophy is that any difference in words at the beginning and/or end of a DA could result in a different label for that DA, and the more words that are mismatched, the more likely the difference in label.
    As a very strict measure of reliability, we used the following approach: (1) Take one labeler&#8217;s transcript as a reference.
    (2) Look at each other labeler&#8217;s words.
    For each word, look at the utterance it comes from and see if the reference has the exact same utterance.
    (3) If it does, there is a match.
    Match every word in the utterance, and then mark the matched utterance in the 