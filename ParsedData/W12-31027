trols constructed from the machine translation ranking tasks from prior years.
    Control items were selected such that there was high agreement across the system developers who completed that item.
    In all, there were 229 people who participated in the manual evaluation, with 91 workers putting in more than an hour&#8217;s worth of effort, and 21 putting in more than four hours.
    After filtering Turker rankings against the controls to discard Turkers who fell below a threshold level of agreement on the control questions, there was a collective total of 336 hours of usable labor.
    This is similar to the total of 361 hours of labor collected for WMT11.
    We asked annotators to evaluate system outputs by ranking translated sentences relative to each other.
    This was our official determinant of translation quality.
    The total number of judgments collected for each of the language pairs is given in Table 2.
    Ranking translations relative to each other is a reasonably intuitive task.
    We th