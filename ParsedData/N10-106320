asal models of source given target and target given source; lexical weighting models in both directions, language model, word count, phrase count, distortion penalty, and a lexicalized reordering model.
    Given that the extracted Wikipedia data takes the standard form of parallel sentences, it would be easy to exploit this same data in a number of systems.
    For each language pair we explored two training conditions.
    The &#8220;Medium&#8221; data condition used easily downloadable corpora: Europarl for GermanEnglish and Spanish-English, and JRC/Acquis for Bulgarian-English.
    Additionally we included titles of all linked Wikipedia articles as parallel sentences in the medium data condition.
    The &#8220;Large&#8221; data condition includes all the medium data, and also includes using a broad range of available sources such as data scraped from the web (Resnik and Smith, 2003), data from the United Nations, phrase books, software documentation, and more.
    In each condition, we explored the impac