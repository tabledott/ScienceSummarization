fied.
    Last year&#8217;s evaluation exercise (Lee and Przybocki, 2005) was startling in that Bleu&#8217;s rankings of the ArabicEnglish translation systems failed to fully correspond to the manual evaluation.
    In particular, the entry that was ranked 1st in the human evaluation was ranked 6th by Bleu.
    In this section we examine Bleu&#8217;s failure to correctly rank this entry.
    The manual evaluation conducted for the NIST MT Eval is done by English speakers without reference to the original Arabic or Chinese documents.
    Two judges assigned each sentence in Table 4: Two hypothesis translations with similar Bleu scores but different human scores, and one of four reference translations the hypothesis translations a subjective 1&#8211;5 score along two axes: adequacy and fluency (LDC, 2005).
    Table 3 gives the interpretations of the scores.
    When first evaluating fluency, the judges are shown only the hypothesis translation.
    They are then shown a reference translation and are asked to j