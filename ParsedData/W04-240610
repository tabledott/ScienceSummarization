es on the first order context vectors of feature words.
    The second order experiments in this paper use two different types of features, co&#8211;occurrences and bigrams, defined as they are in the first order experiments.
    Each co&#8211;occurrence identified in training data is assigned a unique index and occupies the corresponding row/column in a word co&#8211;occurrence matrix.
    This is constructed from the co&#8211;occurrence pairs, and is a symmetric adjacency matrix whose cell values show the loglikelihood ratio for the pair of words representing the corresponding row and column.
    Each row of the co&#8211; occurrence matrix can be seen as a first order context vector of the word represented by that row.
    The set of words forming the rows/columns of the co&#8211;occurrence matrix are treated as the feature words.
    Bigram features lead to a bigram matrix such that for each selected bigram WORDi&lt;&gt;WORDj, WORDi represents a single row, say the ith row, and WORDj represents a single co