of the systems in a supervised and an unsupervised framework.
    Table 1 shows the total number of target word instances in the training and testing set, as well as the average number of senses in the gold standard.
    The main difference of the SemEval-2010 as compared to the SemEval-2007 sense induction task is that the training and testing data are treated separately, i.e the testing data are only used for sense tagging, while the training data are only used for sense induction.
    Treating the testing data as new unseen instances ensures a realistic evaluation that allows to evaluate the clustering models of each participating system.
    The evaluation framework of SemEval-2010 WSI task considered two types of evaluation.
    In the first one, unsupervised evaluation, systems&#8217; answers were evaluated according to: (1) VMeasure (Rosenberg and Hirschberg, 2007), and (2) paired F-Score (Artiles et al., 2009).
    Neither of these measures were used in the SemEval2007 WSI task.
    Manandhar &amp; Kl