ram-based ROUGE score (ROUGE-1) has been shown to agree with human judgements most (Lin and Hovy, 2003).
    We show three of the ROUGE metrics in our experiment results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), and ROUGE-W (based on longest common subsequence weighted by the length).
    There are 8 different human judges for DUC 2004 Task 2, and 4 for DUC 2004 Task 4.
    However, a subset of exactly 4 different human judges produced model summaries for any given cluster.
    ROUGE requires a limit on the length of the summaries to be able to make a fair evaluation.
    To stick with the DUC 2004 specifications and to be able to compare our system with human summaries and as well as with other DUC participants, we produced 665-byte summaries for each cluster and computed ROUGE scores against human summaries.
    MEAD2 is a publicly available toolkit for extractive multi-document summarization.
    Although it comes as a centroid-based summarization system by default, its feature set can be extended 