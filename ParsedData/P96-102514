 IBM E has N announced S his C resignation B yesterday The baseNP model considers the words directly to the left and right of each gap, and whether there is a comma between the two words (we write ci = 1 if there is a comma, ci = 0 otherwise).
    Probability estimates are based on counts of consecutive pairs of words in unreduced training data sentences, where baseNP boundaries define whether gaps fall into the S, C, E, B or N categories.
    The probability of a baseNP sequence in an unreduced sentence S is then: The estimation method is analogous to that described in the sparse data section of this paper.
    The method is similar to that described in (Ramshaw and Marcus 95; Church 88), where baseNP detection is also framed as a tagging problem.
    The probability of a parse tree T, given a sentence S, is: The denominator in Equation (9) is not actually constant for different baseNP sequences, but we make this approximation for the sake of efficiency and simplicity.
    In practice this is a good approxim