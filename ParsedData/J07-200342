gram model whose structure cuts across the structure of context-free derivations, and large amounts of training data for meaningful comparison with modern systems&#8212;these all threaten to make training a synchronous grammar and translating with it intractable.
    We have shown how, through training with simple methods inspired by phrase-based models, and translating using a modified CKY with cube pruning, this challenge can be met.
    Clearly, however, we have only scratched the surface of the modeling challenge.
    The fact that moving from flat structures to hierarchical structures significantly improves translation quality suggests that more specific ideas from syntax may be valuable as well.
    There are many possibilities for enriching the simple framework that the present model provides.
    But the course taken here is one of organic development of an approach known to work well at large-scale tasks, and we plan to stay this course in future work towards more syntactically informed statistical m