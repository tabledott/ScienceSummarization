lability of training data.
    In Table 1, one can get an idea of how they unfold in a real scenario.
    With adequately sufficient training data, n-gram TM is expected to outperform NCM in the decoding.
    A perplexity study in section 4.1 will look at the model from another perspective.
  
  
    We use a database from the bilingual dictionary &#8220;Chinese Transliteration of Foreign Personal Names&#8221; which was edited by Xinhua News Agency and was considered the de facto standard of personal name transliteration in today&#8217;s Chinese press.
    The database includes a collection of 37,694 unique English entries and their official Chinese transliteration.
    The listing includes personal names of English, French, Spanish, German, Arabic, Russian and many other origins.
    The database is initially randomly distributed into 13 subsets.
    In the open test, one subset is withheld for testing while the remaining 12 subsets are used as the training materials.
    This process is repeated 13 times to