 of edits, demonstrating significant differences between the types of human judgments.
  
  
    Since the introduction of the BLEU metric (Papineni et al., 2002), statistical MT systems have moved away from human evaluation of their performance and towards rapid evaluation using automatic metrics.
    These automatic metrics are themselves evaluated by their ability to generate scores for MT output that correlate well with human judgments of translation quality.
    Numerous methods of judging MT output by humans have been used, including Fluency, Adequacy, and, more recently, Human-mediated Translation Edit Rate (HTER) (Snover et al., 2006).
    Fluency measures whether a translation is fluent, regardless of the correct meaning, while Adequacy measures whether the translation conveys the correct meaning, even if the translation is not fully fluent.
    Fluency and Adequacy are frequently measured together on a discrete 5 or 7 point scale, with their average being used as a single score of translation qualit