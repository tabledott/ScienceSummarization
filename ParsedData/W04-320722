 increase with more training data, the slightly lower performance at the highest training set size may indicate overtraining of this simple model.
    Unlabeled precision and recall show continued improvement with more Korean training data.
    Even with help from the true English trees, the unsupervised SITGs underperform PCFGs trained on as few as 32 sentences, with the exception of unlabeled recall in one experiment.
    It seems that even some small amount of knowledge of the language helps parsing.
    Crossing brackets for the flattened SITG parses are understandably lower.
    The output of our bilingual parser contains three types of constituents: English-only (aligned to 0), Korean-only (aligned to 0), and bilingual.
    The Korean parse induced by the Korean-only and bilingual constituents is filtered so constituents with intermediate labels (generated by the binarization process) are eliminated.
    A second filter we consider is to keep only the (remaining) bilingual constituents corresponding to 