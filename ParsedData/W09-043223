In-domain LMs were trained on the corresponding English side.
    All in-domain models were either used alone or combined with the baseline models according to multiple-model paradigm explained in Section 4.3.
    Tuning of the interpolation weights was performed on the standard devel
  
  
    This paper investigated cross-domain adaptation of a state-of-the-art SMT system (Moses), by exploiting large but cheap monolingual data.
    We proposed to generate synthetic parallel data by translating monolingual adaptation data with a background system and to train statistical models from the synthetic corpus.
    We found that the largest gain (25% relative) is achieved when in-domain data are available for the target language.
    A smaller performance improvement is still observed (5% relative) if source adaptation data are available.
    We also observed that the most important role is played by the LM adaptation, while the adaptation of the TM and RM gives consistent but small improvement.
    We also showed 