as above, and the role for each potential argument slot was labelled by two human annotators, choosing from the simplified role set allowed by each verb according to VerbNet.
    A slot could also be labelled as an adjunct, or as &#8220;bad&#8221; (incorrectly chunked).
    Agreement between the two annotators was high, yielding a kappa statistic of 0.83.
    After performing the labelling task individually, the annotators reconciled their responses (in consultation with a third annotator) to yield a set of human judgements used for evaluation.
    In our development experiments, we tried an evidence count threshold of either the mean or median over all counts of a particular conjunction of conditioning events.
    (For example, for , this is the mean or median count across all combinations of verb, slot, and noun.)
    The more lenient median setting worked slightly better on the validation set, and was retained for our test experiments.
    We also experimented with initial starting values of 2, 3, and 8 fo