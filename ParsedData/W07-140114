 in the pyramid, in order to assign each peer a weight.
    In this way, the SCUs in the automatic summaries linked to the SCUs in the higher tiers of the pyramid are assigned a heavier weight than those at the bottom.
    For the SUM setting, the RTE-3 annotators selected relevant passages from the peers and used them as T&#8217;s, meanwhile the labels of the corresponding SCUs were used as H&#8217;s.
    Small adjustments were allowed, whenever the texts were not grammatically acceptable.
    This process simulated the need of a summarization system to identify information redundancy, which should be avoided in the summary.
    Each pair of the dataset was judged by three annotators.
    As in previous challenges, pairs on which the annotators disagreed were filtered-out.
    On the test set, the average agreement between each pair of annotators who shared at least 100 examples was 87.8%, with an average Kappa level of 0.75, regarded as substantial agreement according to Landis and Koch (1997).
    19.2 % o