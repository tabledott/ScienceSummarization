ults.
    The annotation experiments show that humans distinguish the seven rhetorical categories with a stability of K = .82, .81, .76 (N = 1,220; k = 2, where K stands for the kappa coefficient, N for the number of items (sentences) annotated, and k for the number of annotators).
    This is equivalent to 93%, 92%, and 90% agreement.
    Reproducibility was measured at K = .71 (N = 4,261, k = 3), which is equivalent to 87% agreement.
    On Krippendorff&#8217;s (1980) scale, agreement of K = .8 or above is considered as reliable, agreement of .67&#8211;.8 as marginally reliable, and less than .67 as unreliable.
    On Landis and Koch&#8217;s (1977) more forgiving scale, agreement of .0&#8211;.2 is considered as showing &#8220;slight&#8221; correlation, .21&#8211;.4 as &#8220;fair,&#8221; .41&#8211;.6 as &#8220;moderate,&#8221; .61&#8211; 0.8 as &#8220;substantial,&#8221; and .81 &#8211;1.0 as &#8220;almost perfect.&#8221; According to these guidelines, our results can be considered reliable, substantial ann