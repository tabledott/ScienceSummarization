    Both for mutual information and the Dice coefficient, this involves comparison with an experimentally determined threshold.
    Although the two measures are similar in that they compare the joint probability p(X=1,Y =1) with the marginal probabilities, they have different asymptotic behaviors.
    This was demonstrated in the previous paragraphs for the cases of small and decreasing relative frequencies.
    Here we examine two more cases associated with specific tests.
    We consider the two extreme cases, where In the first case, both average and specific mutual information are equal to 0 since log p51c-x)xptyYy)) = log 1 = 0 for all x and y, and are thus easily testable, whereas the Dice coefficient is equal to 2xp(r)(cx-1)1&#177;))V-.01)) and is thus a function of the individual frequencies of the two word groups.
    In this case, the test is easier to decide using mutual information.
    In the second case, the results are reversed; specific mutual information is equal to log pP((xtil))2 = log(p(X