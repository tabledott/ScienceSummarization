nd adequacy, the most commonly used manual evaluation metrics.
    We settled on contrastive evaluations of 5 system outputs for a single test sentence.
    See Figure 3 for a screenshot of the evaluation tool.
    Presenting the output of several system allows the human judge to make more informed judgements, contrasting the quality of the different systems.
    The judgements tend to be done more in form of a ranking of the different systems.
    We assumed that such a contrastive assessment would be beneficial for an evaluation that essentially pits different systems against each other.
    While we had up to 11 submissions for a translation direction, we did decide against presenting all 11 system outputs to the human judge.
    Our initial experimentation with the evaluation tool showed that this is often too overwhelming.
    Making the ten judgements (2 types for 5 systems) takes on average 2 minutes.
    Typically, judges initially spent about 3 minutes per sentence, but then accelerate with experienc