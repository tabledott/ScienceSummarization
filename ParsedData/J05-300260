ned with the original sentences from which they were drawn.
    Knight and Marcu (2000) treat reduction as a translation process using a noisychannel model (Brown et al. 1993).
    In this model, a short (compressed) string is treated as a source, and additions to this string are considered to be noise.
    The probability of a source string s is computed by combining a standard probabilistic context-free grammar score, which is derived from the grammar rules that yielded tree s, and a word-bigram score, computed over the leaves of the tree.
    The stochastic channel model creates a large tree t from a smaller tree s by choosing an extension template for each node based on the labels of the node and its children.
    In the decoding stage, the system searches for the short string s that maximizes P(s|t), which (for fixed t) is equivalent to maximizing P(s) &#215; P(t|s).
    While this approach exploits only syntactic and lexical information, Jing and McKeown (2000) also rely on cohesion information, derived