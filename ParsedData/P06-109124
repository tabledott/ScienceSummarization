 work in SMT based on the noisy channel approach presented in (Brown et al., 1993).
    While error-driven training techniques are commonly used to improve the performance of phrasebased translation systems (Chiang, 2005; Och, 2003), this paper presents a novel block sequence translation approach to SMT that is similar to sequential natural language annotation problems such as part-of-speech tagging or shallow parsing, both in modeling and parameter training.
    Unlike earlier approaches to SMT training, which either rely heavily on domain knowledge, or can only handle a small number of features, this approach treats the decoding process as a black box, and can optimize tens millions of parameters automatically, which makes it applicable to other problems as well.
    The choice of our formulation is convex, which ensures that we are able to find the global optimum even for large scale problems.
    The loss function in Eq.
    4 may not be optimal, and using different choices may lead to future improvements