s, 2007; Zettlemoyer and Collins, 2009) try to alleviate the annotation effort by only taking sentence and logical form pairs to train the models.
    Learning is then defined over hidden patterns in the training data that associate logical symbols with lexical and syntactic elements.
    In this work we take an additional step towards alleviating the difficulty of training semantic parsers and present a world response based training protocol.
    Several recent works (Chen and Mooney, 2008; Liang et al., 2009; Branavan et al., 2009) explore using an external world context as a supervision signal for semantic interpretation.
    These works operate in settings different to ours as they rely on an external world state that is directly referenced by the input text.
    Although our framework can also be applied in these settings we do not assume that the text can be grounded in a world state.
    In our experiments the input text consists of generalized statements which describe some information need that does 