can be assigned to a letter.
    Our approach is to use an instance-based learning technique as a local predictor to generate a set of phoneme candidates for each letter chunk, given its context in a word.
    The local predictor produces confidence values for Each candidate phoneme.
    We normalize the confidence values into values between 0 and 1, and treat them as the emission probabilities, while the transition probabilities are derived directly from the phoneme sequences in the training data.
    The pronunciation is generated by considering both phoneme prediction values and transition probabilities.
    The optimal phoneme sequence is found with the Viterbi search algorithm.
    We limit the size of the context to n = 3 in order to avoid overfitting and minimize the complexity of the model.
    Since the candidate set is from the classifier, the search space is limited to a small number of candidate phonemes (1 to 5 phonemes in most cases).
    The HMM postprocessing is independent of local prediction