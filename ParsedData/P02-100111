weight in terms of meaningful features associated by hand with that arc, coin, etc.
    Each feature has a strength E R&gt;0, and a weight is computed as the product of the strengths of its features.10 It is now the strengths that are the learnable parameters.
    This allows meaningful parameter tying: if certain arcs such asu:i &#65533;&#8212;*, &#65533;&#8212;*, and a:ae o:e &#65533;&#8212;* share a contextual &#8220;vowel-fronting&#8221; feature, then their weights rise and fall together with the strength of that feature.
    The resulting machine must be normalized, either per-state or globally, to obtain a joint or a conditional distribution as desired.
    Such approaches have been tried recently in restricted cases (McCallum et al., 2000; Eisner, 2001b; Lafferty et al., 2001).
    Normalization may be postponed and applied instead to the result of combining the FST with other FSTs by composition, union, concatenation, etc.
    A simple example is a probabilistic FSA defined by normalizing the intersec