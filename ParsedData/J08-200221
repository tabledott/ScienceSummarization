ystems (using headword match, ignoring these arguments altogether, or using exact match).
    If we vary the choice taken for these three issues, we can come up with many (at least eight) different evaluation measures, and these details are important, because different choices can lead to rather large differences in reported performance.
    Here we describe in detail our evaluation measures for the results on the February 2004 data reported in this article.
    The measures are similar to the CoNLL evaluation measure, but report a richer set of statistics; the exact differences are discussed at the end of this section.
    For both gold-standard and automatic parses we use one evaluation measure, which we call argument-based evaluation.
    To describe the evaluation measure, we will use as an example the correct and guessed semantic role labelings shown in Figures 2(a) and 2(b).
    Both are shown as labelings on parse tree nodes with labels of the form ARGX and C-ARGX.
    The label C-ARGX is used to repre