ut training vectors.
    If we could calculate the dot products from xi and x2 directly without considering the vectors (I)(xi) and (I, (x2) projected onto the higher-dimensional space, we can reduce the computational complexity considerably.
    Namely, we can reduce the computational overhead if we could find the function K that satisfies: - 4)(x2) = K(xi, x2).
    (8) On the other hand, since we do not need itself for actual learning and classification, 'In general, (1,(x) is a mapping into Hilbert space. all we have to do is to prove the existence of cl&#8226; that satisfies (8) provided the function K is selected properly.
    It is known that (8) holds if and only if the function K satisfies the Mercer condition (Vapnik, 1998).
    In this way, instead of projecting the training data onto the high-dimensional space, we can decrease the computational overhead by replacing the dot products, which is calculated in optimization and classification steps, with the function K. Such a function K is called a Ker