own output.
    Finally, Zhang et al (2006) cluster the parallel training corpus using an algorithm that heuristically minimizes the average entropy of source-side and target-side language models over a fixed number of clusters.
    Each source sentence is then decoded using the language model trained on the cluster that assigns highest likelihood to that sentence.
    The work we present here is complementary to both the IR approaches and Ueffing&#8217;s method because it provides a way of exploiting a preestablished corpus division.
    This has the potential to allow sentences having little surface similarity to the current source text to contribute statistics that may be relevant to its translation, for instance by raising the probability of rare but pertinent words.
    Our work can also be seen as extending all previous approaches in that it assigns weights to components depending on their degree of relevance, rather than assuming a binary distinction between relevant and non-relevant components.
  
  
