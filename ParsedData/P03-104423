bel to each data-point: relevant or non-relevant to the learner&#8217;s scenario.
    The classifier that is being trained is embodied in the set of acquired patterns.
    A data-point can be thought of having one view: the patterns that match on the data-point.
    In both frameworks, the unsupervised learners help one another to bootstrap.
    In co-training, they do so by providing reliable positive examples to each other.
    In counter-training they proceed by finding their own weakly reliable positive evidence, and by providing each other with reliable negative evidence.
    Thus, in effect, the unsupervised learners &#8220;supervise&#8221; each other.
  
  
    In this paper we have presented counter-training, a method for strengthening unsupervised strategies for knowledge acquisition.
    It is a simple way to combine unsupervised learners for a kind of &#8220;mutual supervision&#8221;, where they prevent each other from degradation of accuracy.
    Our experiments in acquisition of semantic patterns