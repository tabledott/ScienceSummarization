nerjee and Lavie, 2005) have described the details underlying the metric and have extensively compared its performance with BLEU and several other MT evaluation metrics.
    This paper recaps the technical details underlying METEOR and describes recent improvements in the metric.
    The latest release extends METEOR to support evaluation of MT output in Spanish, French and German, in addition to English.
    Furthermore, several parameters within the metric have been optimized on language-specific training data.
    We present experimental results that demonstrate the improvements in correlations with human judgments that result from these parameter tunings.
  
  
    METEOR evaluates a translation by computing a score based on explicit word-to-word matches between the translation and a given reference translation.
    If more than one reference translation is available, the translation is scored against each reference independently, and the best scoring pair is used.
    Given a pair of strings to be compar