tion of features.
    The algorithm which generates a classifier given a distribution over the examples (for example, the decision tree induction method) is usually referred to as &#8220;the weak learner.&#8221;The weak learner generally uses an approximate (for example, greedy) method to find a function with a low error rate with respect to the distribution.
    Freund and Schapire (1997) show that provided that at each round of boosting the weak learner returns a feature with greater than (50 + e) % accuracy for some fixed e, the number of training errors falls exponentially quickly with the number of rounds of boosting.
    This fast drop in training errors translates to statistical bounds on generalization performance (Freund and Schapire 1997).
    7 Note that it is also possible to apply these methods to the LogLoss function; see, for example, Friedman et al. (2000) and Duffy and Helmbold (1999).
    Under this view of boosting, the feature selection methods in this article are a particularly simple cas