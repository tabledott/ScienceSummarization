ve the accuracy of dependency parsing across all of these languages, outperforming current stateof-the-art unsupervised grammar induction methods (Headden III et al., 2009; Berg-Kirkpatrick and Klein, 2010).
  
  
    Learning with Linguistic Constraints Our work is situated within a broader class of unsupervised approaches that employ declarative knowledge to improve learning of linguistic structure (Haghighi and Klein, 2006; Chang et al., 2007; Grac&#184;a et al., 2007; Cohen and Smith, 2009b; Druck et al., 2009; Liang et al., 2009a).
    The way we apply constraints is closest to the latter two approaches of posterior regularization and generalized expectation criteria.
    In the posterior regularization framework, constraints are expressed in the form of expectations on posteriors (Grac&#184;a et al., 2007; Ganchev et al., 2009; Grac&#184;a et al., 2009; Ganchev et al., 2010).
    This design enables the model to reflect constraints that are difficult to encode via the model structure or as priors on its