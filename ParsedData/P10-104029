d representations to phrase representations is worth further investigation.
  
  
    Word features can be learned in advance in an unsupervised, task-inspecific, and model-agnostic manner.
    These word features, once learned, are easily disseminated with other researchers, and easily integrated into existing supervised NLP systems.
    The disadvantage, however, is that accuracy might not be as high as a semi-supervised method that includes task-specific information and that jointly learns the supervised and unsupervised tasks (Ando &amp; Zhang, 2005; Suzuki &amp; Isozaki, 2008; Suzuki et al., 2009).
    Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks.
    Ours is the first work to systematically compare different word representations in a controlled way.
    We found that Brown clusters and word embeddings both can improve the accuracy of a near-state-of-the-art supervised NLP system.
    We also fou