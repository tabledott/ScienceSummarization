5% of the samples are changed by at least one rule, as opposed to POS tagging, where only 2.5% of the samples are changed by a rule.
    2.
    The commitment assumption might also not hold.
    For this task, 20% of the samples that were modified by a rule are also changed again by another one.
    A question usually asked about a machine learning algorithm is how well it adapts to larger amounts of training data.
    Since the performance of the Fast TBL algorithm is identical to that of regular TBL, the issue of interest is the dependency between the running time of the algorithm and the amount of training data.
    The experiment was performed with the part-ofspeech data set.
    The four algorithms were trained on training sets of different sizes; training times were recorded and averaged over 4 trials.
    The results are presented in Figure 2(a).
    It is obvious that the Fast TBL algorithm is much more scalable than the regular TBL displaying a linear dependency on the amount of training data, while 