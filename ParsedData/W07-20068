tween the two authors was 93.80%.
			This figure, compared to those in the literature for fine-grained human annotations, gives us a clear indication that the agreement of human annotators strictly depends on the granularity of the adopted sense inventory.
	
	
			We calculated two baselines for the test corpus: a random baseline, in which senses are chosen at random, and the most frequent baseline (MFS), in which we assign the first WordNet sense to each word in the dataset.
			Formally, the accuracy of the random baseline was calculated as follows: BLRand = 1|T | |T |?
			i=1 1 |CoarseSenses(wi)| where T is our test corpus, wi is the i-th word instance in T , and CoarseSenses(wi) is the set ofcoarse senses for wi according to the sense cluster ing we produced as described in Section 2.2.
			The accuracy of the MFS baseline was calculated as: BLMFS = 1|T | |T |?
			i=1 ?(wi, 1) where ?(wi, k) equals 1 when the k-th sense ofword wi belongs to the cluster(s) manually associ ated by the lexicographer to word wi 