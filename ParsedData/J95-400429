than P(IN I IN JJ), but the outcome will be highly dependent upon the context in which this collocation appears.
    The second transformation arises from the fact that when a verb appears in a context such as We do n't eat or We did n't usually drink, the verb is in base form.
    A stochastic trigram tagger would have to capture this linguistic information indirectly from frequency counts of all trigrams of the form shown in figure 5 (where a star can match any part-of-speech tag) and from the fact that P(n't I RB) is fairly high.
    In Weischedel et al. (1993), results are given when training and testing a Markovmodel based tagger on the Penn Treebank Tagged Wall Street journal Corpus.
    They cite results making the closed vocabulary assumption that all possible tags for all words in the test set are known.
    When training contextual probabilities on one million words, an accuracy of 96.7% was achieved.
    Accuracy dropped to 96.3% when contextual probabilities were trained on 64,000 words.
    We tr