lence Grammar (EVG) and its lexicalized extension (L-EVG).
    The primary difference between EVG and DMV is that DMV uses valence information to determine the number of arguments a head takes but not their categories.
    In contrast, EVG allows different distributions over arguments for different valence slots.
    L-EVG extends EVG by conditioning on lexical information as well.
    This allows L-EVG to potentially capture subcategorizations.
    The downside of adding additional conditioning events is that we introduce data sparsity problems.
    Incorporating more valence and lexical information increases the number of parameters to estimate.
    A common solution to data sparsity in supervised parsing is to add smoothing.
    We show that smoothing can be employed in an unsupervised fashion as well, and show that mixing DMV, EVG, and L-EVG together produces state-ofthe-art results on this task.
    To our knowledge, this is the first time that grammars with differing levels of detail have been successfu