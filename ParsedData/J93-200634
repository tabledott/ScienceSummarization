 used for this test, two different attachments for a prepositional phrase produced trees with the same set of rules, but differing in shape.
    Thus the simple, context-free model based on the product of rule probabilities could not capture preferences concerning such attachment.
    By adding to the model probabilities for such additional features, we expect that the power of the probabilistic model to automatically select the correct parse can be substantially increased.
    Second, a much more reliable estimate of p(word I category) can be estimated as described in Section 2.
    In fact, one should be able to improve the estimate of a tree's likelihood via p(S I W) -,-- p(S I T) * p(T I W).
    One purpose for probabilistic models is to contribute to handling new words or partially understood sentences.
    We have done preliminary experiments that show that there is promise in learning lexical, syntactic, and semantic features from context when probabilistic tools are used to help control the ambiguity.