 The results are shown in Figure 3.
    Hierarchical splitting leads to better parameter estimates over directly estimating a grammar with 2k subsymbols per symbol.
    While the two procedures are identical for only two subsymbols (Fl: 76.1%), the hierarchical training performs better for four subsymbols (83.7% vs. 83.2%).
    This advantage grows as the number of subsymbols increases (88.4% vs. 87.3% for 16 subsymbols).
    This trend is to be expected, as the possible interactions between the subsymbols grows as their number grows.
    As an example of how staged training proceeds, Figure 2 shows the evolution of the subsymbols of the determiner (DT) tag, which first splits demonstratives from determiners, then splits quantificational elements from demonstratives along one branch and definites from indefinites along the other.
    5If our purpose was only to model language, as measured for instance by perplexity on new text, it could make sense to erase even the labels of the Penn Treebank to let EM find b