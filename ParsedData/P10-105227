talk with CRFs, due to the size of the label set.
    Our results compare favorably with those reported in (Pal et al., 2006), where the accuracy attains 91.7% using 19075 examples for training and 934 for testing, and with those in (Jeong et al., 2009) (88.4% accuracy with 18,000 (2,000) training (test) instances).
    Table 5 gives the results obtained for the larger Nettalk+prosody task.
    Here, we only report the results obtained with SGD and BCD.
    For OWL-QN, the largest model we could handle was the 3-grm model, which contained 69 million features, and took 48min to train.
    Here again, performance steadily increase with the number of features, showing the benefits of large-scale models.
    We lack comparisons for this task, which seems considerably harder than the sole phonetization task, and all systems seem to plateau around 13.5% accuracy.
    Interestingly, simultaneously predicting the phoneme and its prosodic markers allows to improve the accuracy on the prediction of phonemes, which impr