
  METEOR: An Automatic Metric For MT Evaluation With Improved Correlation With Human Judgments
  
  
    evaluation, such metrics are still of great value and utility* In order to be both effective and useful, an automatic metric for MT evaluation has to satisfy several basic criteria* The primary and most intuitive requirement is that the metric have very high correlation with quantified human notions of MT quality* Furthermore, a good metric should be as sensitive as possible to differences in MT quality between different systems, and between different versions of the same system* The metric should be consistent (same MT system on similar texts should produce similar scores), reliable (MT systems that score similarly can be trusted to perform similarly) and general (applicable to different MT tasks in a wide range of domains and scenarios)* Needless to say, satisfying all of the above criteria is extremely difficult, and all of the metrics that have been proposed so far fall short of adequately addressing 