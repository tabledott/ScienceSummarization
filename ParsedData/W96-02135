hen wi ends with &amp;quot;ing&amp;quot; and when ti =VBG1.
    Thus a model parameter aj effectively serves as a &amp;quot;weight&amp;quot; for a certain contextual predictor, in this case the suffix &amp;quot;ing&amp;quot;, towards the probability of observing a certain tag, in this case a VBG.
    The model generates the space of features by scanning each pair (hi ,ti) in the training data with the feature &amp;quot;templates&amp;quot; given in Table 1.
    Given hi as the current history, a feature always asks some yes/no question about hi, and furthermore constrains ti to be a certain tag.
    The instantiations for the variables X, Y, and T in Table 1 are obtained automatically from the training data.
    The generation of features for tagging unknown words relies on the hypothesized distinction that &amp;quot;rare&amp;quot; words' in the training set are similar to unknown words in test data, with respect to how their spellings help predict their tags.
    The rare word features in Table 1, which look 