).
    To show that an automatic method is a reasonable approximation of human judgments, one needs to demonstrate that these can be reliably elicited.
    However, in contrast to translation, where the evaluation criterion can be defined fairly precisely it is difficult to elicit stable human judgments for summarization (Rath et al., 1961) (Lin and Hovy, 2002).
    Our approach tailors the evaluation to observed distributions of content over a pool of human summaries, rather than to human judgments of summaries.
    Our method involves semantic matching of content units to which differential weights are assigned based on their frequency in a corpus of summaries.
    This can lead to more stable, more informative scores, and hence to a meaningful content evaluation.
    We create a weighted inventory of Summary Content Units&#8211;a pyramid&#8211;that is reliable, predictive and diagnostic, and which constitutes a resource for investigating alternate realizations of the same meaning.
    No other evaluation m