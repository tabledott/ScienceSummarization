 example, given the distribution in Table 3, nUtt1Stat = 2 and nUtt1IReq = 1.
    Each category k contributes (nik 2 ) pairs of agreeing judgments for item i; the amount of agreement agri for item i is the sum of (nik For example, given the results in Table 3, we find the agreement value for Utterance 1 as follows.
    (Notice that this definition of observed agreement is equivalent to the mean of the two-coder observed agreement values from Section 2.4 for all coder pairs.)
    If observed agreement is measured on the basis of pairwise agreement (the proportion of agreeing judgment pairs), it makes sense to measure expected agreement in terms of pairwise comparisons as well, that is, as the probability that any pair of judgments for an item would be in agreement&#8212;or, said otherwise, the probability that two arbitrary coders would make the same judgment for a particular item by chance.
    This is the approach taken by Fleiss (1971).
    Like Scott, Fleiss interprets &#8220;chance agreement&#8221; as the