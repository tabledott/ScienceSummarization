
  Improving Unsupervised Dependency Parsing with Richer Contexts and Smoothing
  
    Unsupervised grammar induction models tend to employ relatively simple models of syntax when compared to their supervised counterparts.
    Traditionally, the unsupervised models have been kept simple due to tractability and data sparsity concerns.
    In this paper, we introduce basic valence frames and lexical information into an unsupervised dependency grammar inducer and show how this additional information can be leveraged via smoothing.
    Our model produces state-of-theart results on the task of unsupervised grammar induction, improving over the best previous work by almost 10 percentage points.
  
  
    The last decade has seen great strides in statistical natural language parsing.
    Supervised and semisupervised methods now provide highly accurate parsers for a number of languages, but require training from corpora hand-annotated with parse trees.
    Unfortunately, manually annotating corpora with parse trees 