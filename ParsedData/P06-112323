ggests that when constraining parse trees are used without gaps, at least 34% of training sentence pairs are likely to introduce noise into the model, even if systematic syntactic differences between languages are factored out.
    We should not be surprised when such constraints do more harm than good.
    To increase the chances that a translation model can explain complex word alignments, some authors have proposed various ways of extending a model&#8217;s domain of locality.
    For example, Callison-Burch et al. (2005) have advocated for longer phrases in finite-state phrase-based translation models.
    We computed the phrase length that would be necessary to cover the words involved in each (3,1,4,2) permutation in the MTEval bitext.
    Figure 5 shows the cumulative percentage of these cases that would be covered by phrases up to a certain length.
    Only 9 of the 171 cases (5.2%) could be covered by phrases of length 10 or less.
    Analogous techniques for tree-structured translation models involve