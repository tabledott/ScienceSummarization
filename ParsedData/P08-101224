ating VB is less prone to overfitting.
  
  
    We have presented an improved and more efficient method of estimating phrase pairs directly.
    By both changing the objective function to include a bias toward sparser models and improving the pruning techniques and efficiency, we achieve significant gains on test data with practical speed.
    In addition, these gains were shown without resorting to external models, such as GIZA++.
    We have shown that VB is both practical and effective for use in MT models.
    However, our best system does not apply VB to a single probability model, as we found an appreciable benefit from bootstrapping each model from simpler models, much as the IBM word alignment models are usually trained in succession.
    We find that VB alone is not sufficient to counteract the tendency of EM to prefer analyses with smaller trees using fewer rules and longer phrases.
    Both the tic-tac-toe pruning and the non-compositional constraint address this problem by reducing the space of p