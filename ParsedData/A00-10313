er but also the various &amp;quot;small&amp;quot; decisions on alternatives.
    The aim of this paper is to give a detailed account of the techniques used in TnT.
    Additionally, we present results of the tagger on the NEGRA corpus (Brants et al., 1999) and the Penn Treebank (Marcus et al., 1993).
    The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996).
    For a comparison to other taggers, the reader is referred to (Zavrel and Daelemans, 1999).
  
  
    TnT uses second order Markov models for part-ofspeech tagging.
    The states of the model represent tags, outputs represent the words.
    Transition probabilities depend on the states, thus pairs of tags.
    Output probabilities only depend on the most recent category.
    To be explicit, we calculate for a given sequence of words w1 of length T. t1 tr are elements of the tagset, the additional tags t_1, to, and t7-,&#177;1 are beginning