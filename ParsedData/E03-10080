
  Bootstrapping Statistical Parsers From Small Datasets
  
    We present a practical co-training method for bootstrapping statistical parsers using a small amount of manually parsed training material and a much larger pool of raw sentences.
    Experimental results show that unlabelled sentences can be used to improve the performance of statistical parsers.
    In addition, we consider the problem of bootstrapping parsers when the manually parsed training material is in a different domain to either the raw sentences or the testing material.
    We show that bootstrapping continues to be useful, even though no manually produced parses from the target domain are used.
  
  
    In this paper we describe how co-training (Blum and Mitchell, 1998) can be used to bootstrap a pair of statistical parsers from a small amount of annotated training data.
    Co-training is a wealdy supervised learning algorithm in which two (or more) learners are iteratively retrained on each other's output.
    It has been applied to