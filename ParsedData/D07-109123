lasses.
			Finally, we went beyond linguistically motivatedfactors and carried out experiments with automatically trained word classes.
			By clustering words to gether by their contextual similarity, we are able to find statistically similarities that may lead to more generalized and robust models.
			We trained models on the IWSLT 2006 task (39,953 sentences).
			Compared to a baseline English?Chinese system, adding word classes on theoutput side as additional factors (in a model as pre English?Chinese Model BLEU baseline (surface) 19.54% surface + word class 21.10% Table 3: Experimental result with automatic word classes obtained by word clustering Chinese?English Recase Method BLEU Standard two-pass: SMT + recase 20.65% Integrated factored model (optimized) 21.08% OutputInput mixed-cased lower-cased lower-casedTable 4: Experimental result with integrated recas ing (IWSLT 2006 task) viously illustrated in Figure 4) to be exploited by a 7-gram sequence model, we observe a gain 1.5% BLEU absolute.
			For mor