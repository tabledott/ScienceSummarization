t, for each Chinese unit, we have 15.1 = 5,640/374 English back-transliteration units!
    Confusion is increased tenfold going backward.
    The difficulty of back-transliteration is also reflected by the perplexity of the languages as in Table 5.
    Based on the same alignment tokenization, we estimate the monolingual language perplexity for Chinese and English independently using the n-gram language models
  
  
    surprise, Chinese names have much lower perplexity than English names thanks to fewer Chinese units.
    This contributes to the success of E2C but presents a great challenge to C2E backtransliteration.
    A back-transliteration is considered correct if it falls within the multiple valid orthographically correct options.
    Experiment results are reported in Table 6.
    As expected, C2E error rate is much higher than that of E2C.
    In this paper, the n-gram TM model serves as the sole knowledge source for transliteration.
    However, if secondary knowledge, such as a lookup table of vali