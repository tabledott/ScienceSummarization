a used to build the model.
    Additional parallel data can be readily acquired, but at the cost of specificity: either the data is entirely unrelated to the task at hand, or the data is from a broad enough pool of topics and styles, such as the web, that any use this corpus may provide is due to its size, and not its relevance.
    The task of domain adaptation is to translate a text in a particular (target) domain for which only a small amount of training data is available, using an MT system trained on a larger set of data that is not restricted to the target domain.
    We call this larger set of data a general-domain corpus, in lieu of the standard yet slightly misleading out-of-domain corpus, to allow a large uncurated corpus to include some text that may be relevant to the target domain.
    Many existing domain adaptation methods fall into two broad categories.
    Adaptation can be done at the corpus level, by selecting, joining, or weighting the datasets upon which the models (and by extension, syst