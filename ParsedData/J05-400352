cuments and 500k Chinese documents (Section 4.2) required only about 7 days of processing time on 10 processors.
    The data that we extract is useful.
    Its impact on MT performance is comparable to that of human-translated data of similar size and domain.
    Thus, although we have focused our experiments on the particular scenario where there is little in-domain training data available, we believe that our method can be useful for increasing the amount of training data, regardless of the domain of interest.
    As we have shown, this could be particularly effective for language pairs for which only very small amounts of parallel data are available.
    By acquiring a large comparable corpus and performing a few bootstrapping iterations, we can obtain a training corpus that yields a competitive MT system.
    We suspect our approach can be used on comparable corpora coming from any domain.
    The only domain-dependent element of the system is the date window parameter of the article selection stage (Fig