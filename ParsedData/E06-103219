 the outlier in Figure 2 is included, but a strong R2 = 0.87 when it is excluded.
    Similarly Figure 3 goes from R2 = 0.002 to a much stronger R2 = 0.742.
    Systems which explore different areas of translation space may produce output which has differing characteristics, and might end up in different regions of the human scores / Bleu score graph.
    We investigated this by performing a manual evaluation comparing the output of two statistical machine translation systems with a rule-based machine translation, and seeing whether Bleu correctly ranked the systems.
    We used Systran for the rule-based system, and used the French-English portion of the Europarl corpus (Koehn, 2005) to train the SMT systems and to evaluate all three systems.
    We built the first phrase-based SMT system with the complete set of Europarl data (1415 million words per language), and optimized its feature functions using minimum error rate training in the standard way (Koehn, 2004).
    We evaluated it and the Systran system w