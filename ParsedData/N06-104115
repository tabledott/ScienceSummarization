ample, we might add PROTO = said to each token of reported (as in figure 3).4 Each prototype word is also its own prototype (since a word has maximum similarity to itself), so when we lock the prototype to a label, we are also pushing all the words distributionally similar to that prototype towards that label.5 3To be clear: this method of constructing a prototype list required statistics from the labeled data.
    However, we believe it to be a fair and necessary approach for several reasons.
    First, we wanted our results to be repeatable.
    Second, we did not want to overly tune this list, though experiments below suggest that tuning could greatly reduce the error rate.
    Finally, it allowed us to run on Chinese, where the authors have no expertise.
    4Details of distributional similarity features: To extract context vectors, we used a window of size 2 in either direction and use the first 250 singular vectors.
    We collected counts from all the WSJ portion of the Penn Treebank as well as the ent