he hidden parameters can be conditioned on information extrinsic to the model, providing an easy way to integrate pre-existing knowledge.
    So far we have only implemented a two-class model, to exploit the differences in translation consistency between content words and function words.
    This relatively simple two-class model linked word tokens in parallel texts as accurately as other translation models in the literature, despite being trained on only one fifth as much data.
    Unlike other translation models, the word-to-word model can automatically produce dictionary-sized translation lexicons, and it can do so with over 99% accuracy.
    Even better accuracy can be achieved with a more fine-grained link class structure.
    Promising features for classification include part of speech, frequency of co-occurrence, relative word position, and translational entropy (Melamed, 1997).
    Another interesting extension is to broaden the definition of a &amp;quot;word&amp;quot; to include multi-word lexical un