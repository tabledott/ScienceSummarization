 500 or 50 seed sentences, and agreement-based co-training was applied, using a cache size of 500 sentences.
    The results are shown in Table 4.
    Co-training continues to be effective, even when the two taggers are imbalanced.
    Also, the final performance of the taggers is around the same value, irrespective of the direction of the imbalance.
    Although bootstrapping from unlabelled data is particularly valuable when only small amounts of training material are available, it is also interesting to see if selftraining or co-training can improve state of the art POS taggers.
    For these experiments, both C&amp;C and TNT were initially trained on sections 00&#8211;18 of the WSJ Penn Treebank, and sections 19&#8211;21 and 22&#8211;24 were used as the development and test sets.
    The 1994&#8211;1996 WSJ text from the NANC was used as unlabelled material to fill the cache.
    The cache size started out at 8000 sentences and increased by 10% in each round to match the increasing labelled training data.