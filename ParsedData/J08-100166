ts.
    The inter-subject agreement was r = .768 (p &lt; .01.)
    Second, we examined the effect of different types of summaries (human- vs. machine-generated.)
    An ANOVA revealed a reliable effect of summary type: F(1;15) = 20.38, p &lt; .01 indicating that human summaries are perceived as significantly more coherent than system-generated ones.
    Finally, we also compared the elicited ratings against the DUC evaluations using correlation analysis.
    The human judgments were discretized to two classes (i.e., 0 or 1) using entropy-based discretization (Witten and Frank 2000).
    We found a moderate correlation between the human ratings and DUC coherence violations (r = .41, p &lt; .01).
    This is expected given that DUC evaluators were using a different scale and and were not explicitly assessing summary coherence.
    The summaries used in our rating elicitation study form the basis of a corpus used for the development of our entity-based coherence models.
    To increase the size of our training a