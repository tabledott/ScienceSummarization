es of C. We pick the setting that yields the best cross-validation error and use that C for determining test error for that fold.
    As usual, the reported accuracies is the average over the five folds.
    This is a binary classification task with two classes of sentiment polarity: positive and negative.
    We use a balanced data-set of 1709 instances for each class and therefore the chance baseline is 50%.
    We use a unigram model as our baseline.
    Researchers report state-of-the-art performance for sentiment analysis on Twitter data using a unigram model (Go et al., 2009; Pak and Paroubek, 2010).
    Table 5 compares the performance of three models: unigram model, feature based model using only 100 Senti-features, and the tree kernel model.
    We report mean and standard deviation of 5-fold test accuracy.
    We observe that the tree kernels outperform the unigram and the Senti-features by 2.58% and 2.66% respectively.
    The 100 Senti-features described in Table 4 performs as well as the unigram 