not agree.
    Finally, a filtering process is carried out to manually remove the low-frequency noisy alignment pairs.
    Table 4 shows some of the extracted normalization pairs.
    As can be seen from the table, our algorithm discovers ambiguous mappings automatically that are otherwise missing from most of the lingo dictionary.
    Given the phrase-aligned SMS corpus, the lexical mapping model, characterized by P(s&#65533;k  |ek) , is easily to be trained using equation (6).
    Our n-gram LM P(en  |en&#8722;1) is trained on English Gigaword provided by LDC using SRILM language modeling toolkit (Stolcke, 2002).
    Backoff smoothing (Jelinek, 1991) is used to adjust and assign a non-zero probability to the unseen words to address data sparseness.
    Given an input , the search, characterized in model.
    In this paper, the maximization problem in equation (7) is solved using a monotone search, implemented as a Viterbi search through dynamic programming.
  
  
    The aim of our experiment is to verify t