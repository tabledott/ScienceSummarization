tinguished STOP symbol as a possible outcome in distribution &#952;. No-Split Model Variant In the absence of subsymbol refinement (i.e., when subsymbol z is set to be identical to coarse symbol s), our model simplifies in some respects.
    In particular, the HDP generation of z is obviated and word x is drawn from a word distribution 0s indexed solely by coarse symbol s. The resulting simplified model closely resembles DMV (Klein and Manning, 2004), except that it 1) explicitly generate words x rather than only partof-speech tags s, 2) encodes richer context and valence information, and 3) imposes a Dirichlet prior on the symbol distribution B.
  
  
    We now describe how to augment our generative model of dependency structure with constraints derived from linguistic knowledge.
    Incorporating arbitrary linguistic rules directly in the generative story is challenging as it requires careful tuning of either the model structure or priors for each constraint.
    Instead, following the approach of Grac&#18