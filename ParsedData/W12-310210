ranslations were included among the systems to be ranked as controls, and the pairwise comparisons were used in determining the best system.
    However, workers have a very clear preference for reference translations, so including them unduly penalized systems that, through (un)luck of the draw, were pitted against the references more often.
    These changes are part of a broader discussion of the best way to produce the system ranking, which we discuss at length in Section 4.
    The system scores are reported in Section 3.3.
    Appendix A provides detailed tables that contain pairwise head-to-head comparisons between pairs of systems.
    Each year we calculate the inter- and intra-annotator agreement for the human evaluation, since a reasonable degree of agreement must exist to support our process as a valid evaluation setup.
    To ensure we had enough data to measure agreement, we occasionally showed annotators items that were repeated from previously completed items.
    These repeated items were dra