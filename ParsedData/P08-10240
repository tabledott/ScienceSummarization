
  A Discriminative Latent Variable Model for Statistical Machine Translation
  
    Large-scale discriminative machine translation promises to further the state-of-the-art, but has failed to deliver convincing gains over current heuristic frequency count systems.
    We argue that a principle reason for this failure is not dealing with multiple, equivalent translations.
    We present a translation model which models derivations as a latent variable, in both training and decoding, and is fully discriminative and globally optimised.
    Results show that accounting for multiple derivations does indeed improve performance.
    Additionally, we show that regularisation is essential for maximum conditional likelihood models in order to avoid degenerate solutions.
  
  
    Statistical machine translation (SMT) has seen a resurgence in popularity in recent years, with progress being driven by a move to phrase-based and syntax-inspired approaches.
    Progress within these approaches however has been less dramatic