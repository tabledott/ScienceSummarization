n the hypothesis as the observation sequence.
    We use a first-order HMM, assuming that the emission probability depends only on the backbone word, and the transition probability p(aj I aj_,,I) depends only on the position of the last state and the length of the backbone.
    Treating the alignment as hidden variable, the conditional probability that the hypothesis is generated by the backbone is given by As in HMM-based bilingual word alignment (Och and Ney, 2003), we also associate a null with each backbone word to allow generating hypothesis words that do not align to any backbone word.
    In HMM-based hypothesis alignment, emission probabilities model the similarity between a backbone word and a hypothesis word, and will be referred to as the similarity model.
    The transition probabilities model word reordering, and will be called the distortion model.
    The similarity model, which specifies the emission probabilities of the HMM, models the similarity between a backbone word and a hypothesis word.