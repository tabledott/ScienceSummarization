rtue of competition in word alignment is that, to a first approximation, only one source word should generate each target word.
    If a good alignment for a word token is found, other plausible alignments are explained away and should be discounted as incorrect for that token.
    As we show in this paper, this effect does not prevail for phrase-level alignments.
    The central difference is that phrase-based models, such as the ones presented in section 2 or Marcu and Wong (2002), contain an element of segmentation.
    That is, they do not merely learn correspondences between phrases, but also segmentations of the source and target sentences.
    However, while it is reasonable to suppose that if one alignment is right, others must be wrong, the situation is more complex for segmentations.
    For example, if one segmentation subsumes another, they are not necessarily incompatible: both may be equally valid.
    While in some cases, such as idiomatic vs. literal translations, two segmentations may be in t