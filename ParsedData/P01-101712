tion 5) sum to one for a large, random, selection of conditioning events2 As with [3], a subset of parses is computed with a non-lexicalized PCFG, and the most probable edges (using an empirically established threshold) have their probabilities recomputed according to the complete probability model of Equation 5.
    Both searches are conducted using dynamic programming.
  
  
    The parser as described in the previous section was trained and tested on the data used in the previously described grammar-based language modeling research [4,15].
    This data is from the Penn Wall Street Journal tree-bank [13], but modified to make the text more &#8220;speech-like&#8221;.
    In particular: 4. all words except for the 10,000 most common are replaced by the symbol UNK.
    As in previous work, files F0 to F20 are used for training, F21-F22 for development, and F23-F24 for testing.
    The results are given in Table 2.
    We refer to the current model as the bihead model.
    &#8220;Bihead&#8221; here emphasizes 