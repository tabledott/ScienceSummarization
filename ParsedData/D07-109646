ed in the co-learning approach of Sagae and Tsujii (2007).
			Unlabeled target data was processed with both parsers.
			Sentences that both parsers agreed on were then added to the original training data.
			This combined data set served as training data for one of the original parsers to produce the final system.
			In a similar fashion, Watson and Briscoe (2007) used a variant of self-training to make use of the unlabeled target data.
			5.4.3 Other Approaches Attardi et al (2007) learnt tree revision rules for the target domain by first parsing unlabeled target data using a strong parser; this data was then combined with labeled source data; a weak parser was applied to this new dataset; finally tree correction rules are collected based on the mistakes of the weak parser with respect to the gold data and the output of the strong parser.
			Another technique used was to filter sentences of the out-of-domain corpus based on their similarity to the target domain, as predicted by a classifier (Dredze et al, 20