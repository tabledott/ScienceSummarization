ns by providing prior knowledge of the expected sense distribution.
    This will be explored in future work.
  
  
    Bruce, 1997a)).
    However, all of these methods require that manually sense tagged text be available to train the algorithm.
    For most domains such text is not available and is expensive to create.
    It seems more reasonable to assume that such text will not Bootstrapping approaches require a small amount of disambiguated text in order to initialize the unsupervised learning algorithm.
    An early example of such an approach is described in (Hearst, 1991).
    A supervised learning algorithm is trained with a small amount of manually sense tagged text and applied to a held out test set.
    Those examples in the test set that are most confidently disambiguated are added to the training sample.
    A more recent bootstrapping approach is described in (Yarowsky, 1995).
    This algorithm requires a small number of training examples to serve as a seed.
    There are a variety of options