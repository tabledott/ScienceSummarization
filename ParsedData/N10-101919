alysis of a random subset of test data to estimate an &amp;quot;upper bound&amp;quot; for our precision/recall curve.
    Our annotator manually categorized each suggested correction into one of seven categories.
    Details of the distribution of suggested corrections into the seven categories are shown in Table 1.
    This analysis involves costly manual evaluation, so we only performed it at one point of the precision/recall curve (our current runtime system setting).
    The sample size was 6,000 sentences for prepositions and 5981 sentences for articles (half of the sentences were flagged as containing at least one article/preposition error while the other half were not).
    On this manual evaluation, we achieve 32.87% precision if we count all flags that do not perfectly match a CLC annotation as a false positive.
    Only counting the last category (introduction of an error) as a false positive, precision is at 85.34%.
    Similarly, for articles, the manual estimation arrives at 76.74% precision, whe