 that participated in the evaluation.
    Similarly, MUC-7 has a test corpus of 20 documents.
    We compared our system's MUC-6 and MUC-7 performance with that of the systems that took part in MUC-6 and MUC-7, respectively.
    When the coreference engine is given new test documents, its output is in the form of SGML files with the coreference chains properly annotated according to the guidelines.3 We then used the scoring programs The decision tree classifier learned for MUC-6. for the respective years to generate the recall and precision scores for our coreference engine.
    Our coreference engine achieves a recall of 58.6% and a precision of 67.3%, yielding a balanced F-measure of 62.6% for MUC-6.
    For MUC-7, the recall is 56.1%, the precision is 65.5%, and the balanced F-measure is 60.4%.4 We plotted the scores of our coreference engine (square-shaped) against the official test scores of the other systems (crossshaped) in Figure 3 and Figure 4.
    We also plotted the learning curves of our coreferen