ds are limited to reranking the output of another parser, while our trained parser can be used alone.
    Neither of these methods use the parameters of a generative probability model, which might explain our better performance (see table 2).
  
  
    This article has investigated the application of discriminative methods to broad coverage natural language parsing.
    We distinguish between two different ways to apply discriminative methods, one where the probability model is changed to a discriminative one, and one where the probability model remains generative but the training method optimizes a discriminative criteria.
    We find that the discriminative probability model is much worse than the generative one, but that training to optimize the discriminative criteria results in improved performance.
    Performance of the latter model on the standard test set achieves 90.1% F-measure on constituents, which is the second best current accuracy level, and only 0.6% below the current best (Bod, 2003).
    Th