s vary across lectures.
  
  
    In this section we present the different corpora used to evaluate our model and provide a brief overview of the evaluation metrics.
    Next, we describe our human segmentation study on the corpus of spoken lecture data.
    A heldout development set of three lectures isused for estimating the optimal word block length for representing nodes, the threshold distances for discarding node edges, the number of uniform chunks for estimating tf-idf lexical weights, the alpha parameter for smoothing, and the length of the smoothing window.
    We use a simple greedy search procedure for optimizing the parameters.
    We evaluate our segmentation algorithm on three sets of data.
    Two of the datasets we use are new segmentation collections that we have compiled for this study,1 and the remaining set includes a standard collection previously used for evaluation of segmentation algorithms.
    Various corpus statistics for the new datasets are presented in Table 1.
    Below we brief