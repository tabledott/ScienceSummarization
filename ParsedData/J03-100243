st iteration of Model 1 achieves significantly worse results than the Dice coefficient, but by only the second iteration, Model 1 gives better results than the Dice coefficient.
    An important result of these experiments is that the hidden Markov alignment model achieves significantly better results than Model 2.
    We attribute this to the fact that the HMM is a homogeneous first-order alignment model, and such models are able to better represent the locality and monotonicity properties of natural languages.
    Both models have the important property of allowing an efficient implementation of the EM algorithm (Section 3).
    On the largest Verbmobil task, the HMM achieves an improvement of 3.8% over Model 2.
    On the largest Hansards task, the improvement is 8.7%.
    Interestingly, this advantage continues to hold after bootstrapping more refined models.
    On Model 4, the improvement is 1.4% and 4.8%, respectively.
    We conclude that it is important to bootstrap the refined alignment models with 