.
    To investigate if the rankings from the 3 judges agreed we employed the Kendall Coefficient of Concordance (W) (Siegel and Castellan, 1988).
    This statistic is useful for determining inter-rater agreement where there are 3 or more judges and the judgements are ordinal, and one is interested in the ranks rather than the actual numerical values.
    W ranges between 0 (little agreement) and 1 (full agreement) and bears a linear relationship to the average Spearman Rank-order Correlation Coefficient taken over all possible pairs of the rankings.
    W is calculated as shown in equation 1 below, where n is the number of items (111 in this case), Ri is the average rank for the ith item and k is the number of raters.
    The second term in the denominator includes a correction for ties where: where ti is the number of tied ranks in the ith grouping of ranks.
    The value k(n &#8212;1)W is approximately distributed as X2 with n &#8212; 1 degrees of freedom.
    We obtained a W score of 0.594 which gives a 