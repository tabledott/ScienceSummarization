s is very small, there is much more room for these biases to surface and therefore for voting to be effective.
    But does voting still offer performance gains when classifiers are trained on much larger corpora?
    The complementarity between two learners was defined by Brill and Wu (1998) in order to quantify the percentage of time when one system is wrong, that another system is correct, and therefore providing an upper bound on combination accuracy.
    As training size increases significantly, we would expect complementarity between classifiers to decrease.
    This is due in part to the fact that a larger training corpus will reduce the data set variance and any bias arising from this.
    Also, some of the differences between classifiers might be due to how they handle a sparse training set.
    As a result of comparing a sample of two learners as a function of increasingly large training sets, we see in Table 1 that complementarity does indeed decrease as training size increases.
    Next we tested 