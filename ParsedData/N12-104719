ly stopping provides a form of regularization) for our six English/French tests, and MegaM with 100 iterations and a reduced initial uniform sample (50 pairs instead of 5000) for our three English/Chinese tests. gradient descent, with qo tuned to optimize the training loss achieved after one epoch (Bottou, 2010).
    Upon reaching a local optimum, we reshuffle our data, re-tune our learning rate, and re-start from the optimum, repeating this process 5 times.
    We do not sharpen our distribution with a temperature or otherwise control for entropy; instead, we trust A = 50 to maintain a reasonable distribution.
    Systems for English/French were trained on Canadian Hansard data (years 2001&#8211;2009) summarized in table 1.7 The dev and test sets were chosen randomly from among the most recent 5 days of Hansard transcripts.
    The system for Zh-En was trained on data from the NIST 2009 Chinese MT evaluation, summarized in table 2.
    The dev set was taken from the NIST 05 evaluation set, augmented with som