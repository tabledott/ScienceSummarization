  
  
    Repeat Step 3 iteratively.
    The training sets (e.g.
    SENSE-A seeds plus newly added examples) will tend to grow, while the residual will tend to shrink.
    Additional details aimed at correcting and avoiding misclassifications will be discussed in Section 6.
  
  
    Stop.
    When the training parameters are held constant, the algorithm will converge on a stable residual set.
    Note that most training examples will exhibit multiple collocations indicative of the same sense (as illustrated in Figure 3).
    The decision list algorithm resolves any conflicts by using only the single most reliable piece of evidence, not a combination of all matching collocations.
    This circumvents many of the problems associated with non-independent evidence sources.
    STEP 5: The classification procedure learned from the final supervised training step may now be applied to new data, and used to annotate the original untagged corpus with sense tags and probabilities.
    An abbreviated sample of the fin