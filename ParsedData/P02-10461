on.
    They also give an intuitive explanation of why co-training works, in terms of maximizing agreement on unlabeled data between classifiers based on different &#8220;views&#8221; of the data.
    Finally, they suggest that the Yarowsky algorithm is a special case of the co-training algorithm.
    The Blum and Mitchell paper has been very influential, but it has some shortcomings.
    The proof they give does not actually apply directly to the co-training algorithm, nor does it directly justify the intuitive account in terms of classifier agreement on unlabeled data, nor, for that matter, does the co-training algorithm directly seek to find classifiers that agree on unlabeled data.
    Moreover, the suggestion that the Yarowsky algorithm is a special case of co-training is based on an incidental detail of the particular application that Yarowsky considers, not on the properties of the core algorithm.
    In recent work, (Dasgupta et al., 2001) prove that a classifier has low generalization error if it agr