 improved.
			Adjective (Train: 231 Test : 231) Verb (Train: 251 Test : 251) Lenient agreement Lenient agreement H1:M H2:M recall H1:M H3:M recall 75.66% 77.88% 97.84% 81.20% 79.06% 93.23% Table 6: Results including manual data.
			3.2 Sentence Sentiment Classifier.
			3.2.1 Data 100 sentences were selected from the DUC 2001 corpus with the topics ?illegal alien?, ?term limits?, ?gun control?, and ?NAFTA?.
			Two humans annotated the 100 sentences with three categories (positive, negative, and N/A).
			To measure the agreement between humans, we used the Kappa statistic (Siegel and Castellan Jr. 1988).
			The Kappa value for the annotation task of 100 sentences was 0.91, which is considered to be reliable.
			3.2.2 Test on Human Annotated Data We experimented on Section 2.2.3?s 3 models of sentiment classifiers, using the 4 different window definitions and 4 variations of word-level classifiers (the two word sentiment equations introduced in Section 2.1.1, first with and then without normalization, to compare