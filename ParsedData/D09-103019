s that the results have a natural interpretation.
    They indicate how understandable the output of a machine translation system is better than Bleu does, and better than other manual evaluation like the relative ranking.
    Despite this advantage, evaluating MT through reading comprehension hasn&#8217;t caught on, due to the difficulty of administering it and due to the fact that the DLPT or similar tests are not publicly available.
    We conducted a reading comprehension evaluation using Mechanical Turk.
    Instead of simply administering the test on Mechanical Turk, we used it for all aspects from test creation to answer grading.
    Our procedure was as follows: Test creation We posted human translations of foreign news articles, and ask Tukers to write three questions and provide sample answers.
    We gave simple instructions on what qualifies as a good reading comprehension question.
    System google.fr-en google.de-en rbmt5.de-en geneva.de-en tromble.de-en Question selection We posted the questio