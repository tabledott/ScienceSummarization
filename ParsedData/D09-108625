child c to come from links in an IBM Model 4 Viterbi alignment, we achieve O(n3k2), where k is the maximum number of possible words aligned to a given target language word.
    In practice, k &#171; m, and parsing is not appreciably slower than in the monolingual setting.
    If all configurations were equiprobable, the source sentence would provide no information to the target.
    In our QG experiments, therefore, we started with a bias towards direct parent&#8211;child links and a very small probability for breakages of locality.
    The values of other configuration parameters seem, experimentally, less important for insuring accurate learning.
    Our experiments compare learning on target language text to learning on parallel text.
    In the latter case, we compare learning from high-precision one-to-one alignments alone, to learning from all alignments using a QG.
    Our development and test data were drawn from the German TIGER and Spanish Cast3LB treebanks as converted to projective dependencies fo