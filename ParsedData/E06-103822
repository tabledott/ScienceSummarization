r is the average of all weight vectors throughout training.
    Averaging has been shown to reduce overfitting (Collins, 2002) as well as reliance on the order of the examples during training.
    We found it to be particularly important for this data set.
  
  
    We use the same experimental methodology as Knight and Marcu (2000).
    We provide every compression to four judges and ask them to evaluate each one for grammaticality and importance on a scale from 1 to 5.
    For each of the 32 sentences in our test set we ask the judges to evaluate three systems: human annotated, the decision tree model of Knight and Marcu (2000) and our system.
    The judges were told all three compressions were automatically generated and the order in which they were presented was randomly chosen for each sentence.
    We compared our system to the decision tree model of Knight and Marcu instead of the noisy-channel model since both performed nearly as well in their evaluation, and the compression rate of the decision tree