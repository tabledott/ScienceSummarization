of the Wall Street Journal corpus, using the corpus tags.
    We chose sections 0 &#8212; 19, a total of about 500,000 words.
    Table 1 shows that the residual conditional entropy with the word baseline is only 0.12.
    This reflects lexical ambiguity.
    If all of the words were unambiguous, then the conditional entropy of the tag given the word would be zero.
    We are therefore justified in ignoring ambiguity for the moment, since it vastly improves the efficiency of the algorithms.
    Clearly as the number of clusters increases, the conditional entropy will decrease, as is demonstrated below.
  
  
    The basic methods here have been studied in detail by (Ney et al., 1994), (Martin et al., 1998) and (Brown et al., 1992).
    We assume a vocabulary of words V = {W1, ... }.
    Our task is to learn a deterministic clustering, that is to say a class membership function g from V into the set of class labels , n}.
    This clustering can be used to define a number of simple statistical models.
    The o