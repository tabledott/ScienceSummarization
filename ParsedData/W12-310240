 metrics that have been traditionally used for measuring performance for regression tasks: Mean Absolute Error (MAE) as a primary metric, and Root of Mean Squared Error (RMSE) as a secondary metric.
    For a given test set 5 with entries si71 &lt; i &lt; |5|, we denote by H(si) the proposed score for entry si (hypothesis), and by V (si) the reference value for entry si (gold-standard value).
    We formally define our metrics as follows: where N = |5|.
    Both these metrics are nonparametric, automatic and deterministic (and therefore consistent), and extrinsically interpretable.
    For instance, a MAE value of 0.5 means that, on average, the absolute difference between the hypothesized score and the reference score value is 0.5.
    The interpretation of RMSE is similar, with the difference that RMSE penalizes larger errors more (via the square function).
    Eleven teams (listed in Table 11) submitted one or more systems to the shared task, with most teams submitting for both ranking and scoring subtasks