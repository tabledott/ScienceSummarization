enerated word, as well as some hidden material.
    The only difference is that the word being conditioned upon is often not the immediately preceding word in the sentence.
    However, one could plausibly construct a consistent bigram language model that generates words with the same dependencies as those in a statistical parser that uses bilexical dependencies derived from head-lexicalization.
    Collins (personal communication, January 2003) notes that his parser&#8217;s unknownword-mapping scheme could be made consistent if one were to add a parameter class that estimated &#710;p(w  |+UNKNOWN+), where w E VL U {+UNKNOWN+}.
    The values of these estimates for a given sentence would be constant across all parses, meaning that the &#8220;superficiency&#8221; of the model would be irrelevant when determining arg max P(T  |S).
    It is assumed that all trees that can be generated by the model have an implicit nonterminal +TOP+ that is the parent of the observed root.
    The observed lexicalized root nonte