 reaching an asymptote as the size of the training set increases.
    Arguably, the largest data set that is available for NLP is the Web,1 which currently consists of at least 3,033 million pages.2 Data retrieved from the Web therefore provide enormous potential for training NLP algorithms, if Banko and Brill&#8217;s (2001a, 2001b) findings for spelling corrections generalize; potential applications include tasks that involve word n-grams and simple surface syntax.
    There is a small body of existing research that tries to harness the potential of the Web for NLP.
    Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the Web to generate corpora for languages for which electronic resources are scarce, and Resnik (1999) describes a method for mining the Web in order to obtain bilingual texts.
    Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) use the Web for word sense disambiguation, Volk (2001) proposes a method for resolving PP attachment ambiguities based on Web data, Markert, Ni