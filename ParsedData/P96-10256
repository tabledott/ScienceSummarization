 ,VP&gt;).
    D is now defined as the m-tuple of dependencies: D = {(AF (1), AF(2)...AF(m)} .
    The model assumes that the dependencies are independent, so that: This section describes the way P(AF (AS, B) is estimated.
    The same sentence is very unlikely to appear both in training and test data, so we need to back-off from the entire sentence context.
    We believe that lexical information is crucial to attachment decisions, so it is natural to condition on the words and tags.
    Let V be the vocabulary of all words seen in training data, T be the set of all part-of-speech tags, and TRAIN be the training set, a set of reduced sentences.
    We define the following functions: where h(z) is an indicator function which is 1 if x is true, 0 if x is false. where P is the set of all triples of non-terminals.
    The denominator is a normalising factor which ensures that The denominator of (9) is constant, so maximising P(DIS, B) over D for fixed S, B is equivalent to maximising the product of the numerator