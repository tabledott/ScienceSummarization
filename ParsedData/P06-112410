s draws from G) visiting a Chinese restaurant with an unbounded number of tables (corresponding to the draws from G0), each of which can accommodate an unbounded number of customers.
    The first customer sits at the first table, and each subsequent customer either joins an already occupied table (assign the word to the corresponding draw from G0), or sits at a new table (assign the word to a new draw from G0).
  
  
    We describe an n-gram language model based on a hierarchical extension of the Pitman-Yor process.
    An n-gram language model defines probabilities over the current word given various contexts consisting of up to n &#8212; 1 words.
    Given a context u, let Gu(w) be the probability of the current word taking on value w. We use a Pitman-Yor process as the prior for Gu[Gu(w)]wEW, in particular, where &#960;(u) is the suffix of u consisting of all but the earliest word.
    The strength and discount parameters are functions of the length |u |of the context, while the mean vector is G&#960;(u)