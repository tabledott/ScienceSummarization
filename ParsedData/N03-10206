d from human judgments as the references to evaluate various automatic scoring methods in the following sections.
  
  
    To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al. 2001).
    The NIST (NIST 2002) scoring metric is based on BLEU.
    The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric.
    To achieve this goal, they used a weighted average of variable length n-gram matches between system translations and a set of human reference translations and showed that a weighted average metric, i.e.
    BLEU, correlating highly with human assessments.
    Similarly, following the BLEU idea, we assume that the closer an automatic summary to a professional human summary, the better it is.
    The question is: &amp;quot;Can we apply BLEU directly without any modifications to evaluate summaries as well?&a