were tuned specifically for each of these conditions.
    There appears to be little difference among these approaches, although genre-based adaptation perhaps has a slight advantage.
  
  
    Mixture modeling is a standard technique in machine learning (Hastie et al., 2001).
    It has been widely used to adapt language models for speech recognition and other applications, for instance using cross-domain topic mixtures, (Iyer and Ostendorf, 1999), dynamic topic mixtures (Kneser and Steinbiss, 1993), hierachical mixtures (Florian and Yarowsky, 1999), and cache mixtures (Kuhn and De Mori, 1990).
    Most previous work on adaptive SMT focuses on the use of IR techniques to identify a relevant subset of the training corpus from which an adapted model can be learned.
    Byrne et al (2003) use cosine distance from the current source document to find relevant parallel texts for training an adapted translation model, with background information for smoothing alignments.
    Hildebrand et al (1995) describe a simil