pproach), the same models were used: MLE channel model, Model 1 channel model, target language model, phrase count, and word count.
    Lambdas were trained in the same manner (Och, 03).
  
  
    MSR-MT used its own word alignment approach as described in (Menezes &amp; Richardson, 01) on the same training data.
    MSR-MT does not use lambdas or a target language model.
  
  
    We present BLEU scores on an unseen 10,000 sentence test set using a single reference translation for each sentence.
    Speed numbers are the end-to-end translation speed in sentences per minute.
    All results are based on a training set size of 100,000 sentences and a phrase size of 4, except Table 5.2 which varies the phrase size and Table 5.3 which varies the training set size.
    Results for our system and the comparison systems are presented in Table 5.1.
    Pharaoh monotone refers to Pharaoh with phrase reordering disabled.
    The difference between Pharaoh and the Treelet system is significant at the 99% confidence lev