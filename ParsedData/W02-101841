t al., 1999) and reused in decoding.
    The decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4.
    However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability.
    The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role.
    Acknowledgments.
    This work was supported by DARPA-ITO grant N66001-00-1-9814 and by NSFSTTR grant 0128379.
  

