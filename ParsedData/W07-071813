ntra-annotator agreement 10% of the items were repeated and evaluated twice by each judge.
    In order to measure inter-annotator agreement 40% of the items were randomly drawn from a common pool that was shared across all Judges were allowed to select whichever data set they wanted, and to evaluate translations into whatever languages they were proficient in.
    Shared task participants were excluded from judging their own systems.
    Table 2 gives a summary of the number of judgments that we collected for translations of individual sentences.
    Since we had 14 translation tasks and four different types of scores, there were 55 different conditions.2 In total we collected over 81,000 judgments.
    Despite the large number of conditions we managed to collect more than 1,000 judgments for most of them.
    This provides a rich source of data for analyzing the quality of translations produced by different systems, the different types of human evaluation, and the correlation of automatic metrics with human