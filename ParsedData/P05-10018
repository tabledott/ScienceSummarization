updated using the new structure matrix O in the next iteration, and the process repeats.
    Figure 1 summarizes the algorithm sketched above, which we call the alternating structure optimization (ASO) algorithm.
    The formal derivation can be found in (Ando and Zhang, 2004).
    It is important to note that this SVD-based ASO (SVD-ASO) procedure is fundamentally different from the usual principle component analysis (PCA), which can be regarded as dimension reduction in the data spaceX.
    By contrast, the dimension reduction performed in the SVD-ASO algorithm is on the predictor space (a set of predictors).
    This is possible because we observe multiple predictors from multiple learning tasks.
    If we regard the observed predictors as sample points of the predictor distribution in the predictor space (corrupted with estimation error, or noise), then SVD-ASO can be interpreted as finding the &#8220;principle components&#8221; (or commonality) of these predictors (i.e., &#8220;what good predictors are l