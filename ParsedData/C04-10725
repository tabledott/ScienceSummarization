of comparing several existing automatic metrics and the three new automatic metrics using ORANGE will be presented in Section 4.
			We conclude this paper and discuss future directions in Section 5.
	
	
			Intuitively a good evaluation metric should give higher score to a good translation than a bad one.
			Therefore, a good translation should be ranked higher than a bad translation based their scores.
			One basic assumption of all automatic evaluation metrics for machine translation is that reference translations are good translations and the more a machine translation is similar to its reference translations the better.
			We adopt this assumption and add one more assumption that automatic translations are usually worst than their reference translations.
			Therefore, reference translations should be ranked higher than machine translations on average if a good automatic evaluation metric is used.
			Based on these assumptions, we propose a new automatic evaluation method for evaluation of automatic machine