ences together provide an exhaustive representation of the unsmoothed error surface for the sentence fs along the line &#955;M + ry &#8226; dM.
    The error surface for the whole training corpus is obtained by merging the interval boundaries (and their corresponding error counts) over all sentences in the training corpus.
    The optimal &#947; can then be found by traversing the merged error surface and choosing a point from the interval where the total error reaches its minimum.
    After the parameter update, &#955;M = &#955;M +&#947;opt~ ' dM1 the decoder may find new translation hypotheses which are merged into the candidate repositories if they are ranked among the top N candidates.
    The relation K = N holds therefore only in the first iteration.
    From the second iteration on, K is usually larger than N. The sequence of line optimizations and decodings is repeated until (1) the candidate repositories remain unchanged and (2) &#947;opt~0.
  
  
    In this section, the algorithm for computing the 