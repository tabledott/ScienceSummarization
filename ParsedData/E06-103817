s case the PP governing on Tuesday.
    We then add features indicating the context from which this node was dropped.
    For example we add a feature specifying that a PP was dropped which was the child of a VP.
    We also add a feature indicating that a PP was dropped which was the left sibling of another PP, etc.
    Ideally, for each production in the tree we would like to add a feature indicating every node that was dropped, e.g.
    &#8220;VP&#8594;VBD NP PP PP &#8658; VP&#8594;VBD NP PP&#8221;.
    However, we cannot necessarily calculate this feature since the extent of the production might be well beyond the local context of first-order feature factorization.
    Furthermore, since the training set is so small, these features are likely to be observed very few times.
    In this section we have described a rich feature set over adjacent words in the compressed sentence, dropped words and phrases from the original sentence, and properties of deep syntactic trees of the original sentence.
    Note tha