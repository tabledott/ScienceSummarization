
  Online Large-Margin Training Of Dependency Parsers
  
    We present an effective training algorithm for linearly-scored dependency parsers that implements online largemargin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996).
    The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements.
  
  
    Research on training parsers from annotated data has for the most part focused on models and training algorithms for phrase structure parsing.
    The best phrase-structure parsing models represent generatively the joint probability P(x, y) of sentence x having the structure y (Collins, 1999; Charniak, 2000).
    Generative parsing models are very convenient because training consists of computing probability estimates from counts of parsing events in the training set.
    However, generative models make complicated and poorly justified independence as