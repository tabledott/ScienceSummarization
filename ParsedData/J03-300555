e bigram with the higher re-created frequency (or probability) is taken to be the seen bigram.
    If this bigram is really the seen one, then the disambiguation is correct.
    The overall percentage of correct disambiguations is a measure of the quality of the re-created frequencies (or probabilities).
    In the following, we will first describe in some detail the experiments that Rooth et al. (1999) and Clark and Weir (2001) conducted.
    We will then discuss how we replicated their experiments using the Web as an alternative smoothing method.
    Rooth et al. (1999) used pseudodisambiguation to evaluate a class-based model that is derived from unlabeled data using the expectation maximization (EM) algorithm.
    From a data set of 1,280,712 (v, n) pairs (obtained from the BNC using Carroll and Rooth&#8217;s [1998] parser), they randomly selected 3,000 pairs, with each pair containing a fairly frequent verb and noun (only verbs and nouns that occurred between 30 and 3,000 times in the data were considere