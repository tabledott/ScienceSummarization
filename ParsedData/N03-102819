ort voted perceptron (Collins, 2002) and MEMMs (McCallum et al., 2000) with the same efficient feature encoding.
    GIS, CG, and L-BFGS were used to train CRFs and MEMMs.
    Table 2 gives representative NP chunking F scores for previous work and for our best model, with the complete set of 3.8 million features.
    The last row of the table gives the score for an MEMM trained with the mixed CG method using an approximate preconditioner.
    The published F score for voted perceptron is 93.53% with a different feature set (Collins, 2002).
    The improved result given here is for the supported feature set; the complete feature set gives a slightly lower score of 94.07%.
    Zhang et al. (2002) reported a higher F score (94.38%) with generalized winnow using additional linguistic features that were not available to us.
    All the results in the rest of this section are for the smaller supported set of 820,000 features.
    Figures 2a and 2b show how preconditioning helps training convergence.
    Since each 