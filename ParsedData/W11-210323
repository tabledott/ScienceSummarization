able degree of agreement must exist to support our process as a valid evaluation setup.
    To ensure we had enough data to measure agreement, we purposely designed the sampling of source segments and translations shown to annotators in a way that ensured some items would be repeated, both within the screens completed by an individual annotator, and across screens completed by different annotators.
    We did so by ensuring that 10% of the generated screens are exact repetitions of previously generated screen within the same batch of screens.
    Furthermore, even within the other 90%, we ensured that a source segment appearing in one screen appears again in two more screens (though with different system outputs).
    Those two details, intentional repetition of source sentences and intentional repetition of system outputs, ensured we had enough data to compute meaningful inter- and intra-annotator agreement rates.
    We measured pairwise agreement among annotators using Cohen&#8217;s kappa coefficient (n) (