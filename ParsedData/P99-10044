uld incorporate &#8212; is.
    The overall goal of the work described here was to discover these key characteristics.
    To this end, we first compared a number of common similarity measures, evaluating them in a parameter-free way on a decision task.
    When grouped by average performance, they fell into several coherent classes, which corresponded to the extent to which the functions focused on the intersection of the supports (regions of positive probability) of the distributions.
    Using this insight, we developed an information-theoretic metric, the skew divergence, which incorporates the support-intersection data in an asymmetric fashion.
    This function yielded the best performance overall: an average error rate reduction of 4% (significant at the .01 level) with respect to the Jensen-Shannon divergence, the best predictor of unseen events in our earlier experiments (Dagan et al., 1999).
    Our contributions are thus three-fold: an empirical comparison of a broad range of similarity metrics usi