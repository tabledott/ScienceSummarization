s, e.g., nouns like lot as in figure 10.
    Annotators will have the opportunity to judge whether particular automatic annotation is &#8220;good enough&#8221; to serve as a preprocessor.
    We hypothesize that a comparison of automatic annotation that fails this level of accuracy against the hand annotation will still be useful for detecting errors.
    Comparisons between the hand annotated data and the automatically annotated data will yield a set of instances that warrant further checking along the same lines as our previously described error checking mechanisms.
  
  
    This paper outlines our current efforts to produce NomBank, annotation of the argument structure for most common nouns in the Penn Treebank II corpus.
    This is part of a larger effort to produce more detailed annotation of the Penn Treebank.
    Annotation for NomBank is progressing quickly.
    We began with a single annotator while we worked on setting the task and have ramped up to four annotators.
    We continue to work on vari