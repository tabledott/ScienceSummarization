set of 2,000 preposition contexts, with the confidence threshold set at 0.9.
    Each preposition in these essays was judged for correctness of usage by one or two human raters.
    The judged rate of occurrence of preposition errors was 0.109 for Rater 1 and 0.098 for Rater 2, i.e., about 1 out of every 10 prepositions was judged to be incorrect.
    The overall proportion of agreement between Rater1 and Rater 2 was 0.926, and kappa was 0.599.
    Table 3 (second column) shows the results for the Classifier vs. Rater 1, using Rater 1 as the gold standard.
    Note that this is not a blind test of the classifier inasmuch as the classifier&#8217;s confidence threshold was adjusted to maximize performance on this set.
    The overall proportion of agreement was 0.942, but kappa was only 0.365 due to the high level of agreement expected by chance, as the Classifier used the response category of &#8220;correct&#8221; more than 97% of the time.
    We found similar results when comparing the judgements of the Clas