tic evaluation metric with a higher value for p is making predictions that are more similar to the human judgments than an automatic evaluation metric with a lower p. Table 17 reports p for the metrics which were used to evaluate translations into English.7.
    Table 7 summarizes the results by averaging the correlation numbers by equally weighting each of the data conditions.
    The table ranks the automatic evaluation metrics based on how well they correlated with human judgments.
    While these are based on a relatively few number of items, and while we have not performed any tests to determine whether the differences in p are statistically significant, the results are nevertheless interesting, since three metrics have higher correlation than Bleu: Tables 18 and 8 report p for the six metrics which were used to evaluate translations into the other languages.
    Here we find that Bleu and TER are the closest to human judgments, but that overall the correlations are much lower than for translations into 