Ez 1(z E Cx) and the expected proportion of conserved edges under distribution p(z  |x) is The posterior regularization framework (Gra&#231;a et al., 2008) was originally defined for generative unsupervised learning.
    The standard objective is to minimize the negative marginal log-likelihood of the data : E[&#8722; log p&#952;(x)] = &#65533;E[&#8722; log Ez p&#952;(z, x)] over the parameters &#952; (we &#65533; use E to denote expectation over the sample sentences x).
    We typically also add standard regularization term on &#952;, resulting from a parameter prior &#8722; log p(&#952;) = R(&#952;), where p(&#952;) is Gaussian for the MST-Parser models and Dirichlet for the valence model.
    To introduce supervision into the model, we define a set 2x of distributions over the hidden variables z satisfying the desired posterior constraints in terms of linear equalities or inequalities on feature expectations (we use inequalities in this paper): In this paper, for example, we use the conservededge-proportio