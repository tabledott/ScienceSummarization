hat is, for each aspect i &#57747; 1...m, there exists some ideal model v[i]&#8727; = (w[i]&#8727;, b[i]&#8727;) such that the signed distance from the prediction to the rth boundary: w[i]&#8727; &#183; xt &#8722; b[i]&#8727;r has the same sign as the auxiliary variable y[i]tr.
    In other words, the minimum margin over all training instances and ranks, &#947; = minr&#65533;t{(w[i]&#8727; &#183; xt &#8722; b[i]&#8727;r)y[i]tr}, is no less than zero.
    Now for the tth training instance, define an agreement auxiliary variable at, where at = 1 when all aspects agree in rank and at = &#8722;1 when at least two aspects disagree in rank.
    First consider the case where the agreement model a perfectly classifies all training instances: (a &#183; xt)at &gt; 0, &#57738;t.
    It is clear that Good Grief decoding with the ideal joint model (&#57749;w[1]&#8727;, b[1]&#8727;&#57750;, ..., &#57749;w[m]&#8727;, b[m]&#8727;&#57750;, a) will produce the same output as the component ranking models run separately (since t