-based (Brown et al., 1993), templatebased (Och et al., 1999), and syntax-based (Yamada and Knight, 2001), to name just a few.
    Although these models use different generative processes to explain how translated words are re-ordered in a target language, at the lexical level they are quite similar; all these models assume that source words are into target individual words may contain a non-existent element, called NULL.
    We suspect that MT researchers have so far chosen to automatically learn translation lexicons defined only over words for primarily pragmatic reasons.
    Large scale bilingual corpora with vocabularies in the range of hundreds of thousands yield very large translation lexicons.
    Tuning the probabilities associated with these large lexicons is a difficult enough task to deter one from trying to scale up to learning phrase-based lexicons.
    Unfortunately, trading space requirements and efficiency for explanatory power often yields non-intuitive results.
    Consider, for example, the