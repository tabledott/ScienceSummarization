re studied under phoneme-based paradigm for machine transliteration, NCM can also be realized under direct orthographic mapping (DOM).
    Next, let&#8217;s look into a bigram case to see what n-gram TM and NCM present to us.
    For E2C conversion, re-writing eqn (1) and eqn (6) , we have The formulation of eqn.
    (8) could be interpreted as a hidden Markov model with Chinese characters as its hidden states and English transliteration units as the observations (Rabiner, 1989).
    The number of parameters in the bigram TM is potentially 2 while in the noisy channel model (NCM) it&#8217;s T + C2, where T is the number of transliteration pairs and C is the number of Chinese transliteration units.
    In eqn.
    (9), the current transliteration depends on both Chinese and English transliteration history while in eqn.
    (8), it depends only on the previous Chinese unit.
    As T2 &gt;&gt;T+ C2, an n-gram TM gives a finer description than that of NCM.
    The actual size of models largely depends on the avai