ved in computing the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predisposed to produce translations that are shorter than the Source sentences given as input.
  
  
    Training the models described in Section 2 is computationally challenging.
    Since there is an exponential number of alignments that can generate a sentence pair (E, F), it is clear that we cannot apply the If one assumes from the outset that any phrases and can be generated from a concept , one would need a supercomputer in order to store in the memory a table that models the distribution.
    Since we don&#8217;t have access to computers with unlimited memory, we initially learn t distribution entries only for the phrases that occur often in the corpus and for unigrams.
    Then, through smoothing, we learn t distribution entries for the phrases that occur rarely as well.
    In order to be considered in step 2 of the algorithm, a phrase has to o