 each view is sufficient for learning the task, co-training can improve an initial weak learner using unlabelled data.
    Dasgupta et al. (2002) extend the theory of cotraining by showing that, by maximising their agreement over the unlabelled data, the two learners make few generalisation errors (under the same independence assumption adopted by Blum and Mitchell).
    Abney (2002) argues that this assumption is extremely restrictive and typically violated in the data, and he proposes a weaker independence assumption.
    Abney also presents a greedy algorithm that maximises agreement on unlabelled data.
    Goldman and Zhou (2000) show that, through careful selection of newly labelled examples, cotraining can work even when the classifiers' views do not fully satisfy the independence assumption.
  
  
    To apply the theory of co-training to parsing, we need to ensure that each parser is capable of learning the parsing task alone and that the two parsers have different views.
    We could also attempt to 