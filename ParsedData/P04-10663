M training method for Model 1 leads to suboptimal performance in terms of wordalignment accuracy.
    In this paper we show that by addressing these issues, substantial improvements in word-alignment accuracy can be achieved.
  
  
    Model 1 is a probabilistic generative model within a framework that assumes a source sentence 5 of length l translates as a target sentence T, according to the following stochastic process: &#8211; A generating word si in 5 (including a null word so) is selected, and &#8211; The target word tj at position j is generated depending on si.
    Model 1 is defined as a particularly simple instance of this framework, by assuming all possible lengths for T (less than some arbitrary upper bound) have a uniform probability E, all possible choices of source sentence generating words are equally likely, and the translation probability tr(tj|si) of the generated target language word depends only on the generating source language word&#8212;which Brown et al. (1993a) show yields the followi