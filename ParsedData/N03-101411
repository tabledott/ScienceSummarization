thod less dependent on the choice of input representation .
    In addition, because the inputs to include previous history representations, the mapping is defined recursively.
    This recursion allows the input to to be unbounded, because an unbounded derivation history can be successively compressed into a fixed-length vector of history features.
    Training a Simple Synchrony Network (SSN) is similar to training a log-linear model.
    First an appropriate error function is defined for the network&#8217;s outputs, and then some form of gradient descent learning is applied to search for a minimum of this error function.2 This learning simultaneously tries to optimize the parameters of the output computation and the parameters of the mapping from the derivation history to the history representation.
    With multi-layered networks such as SSNs, this training is not guaranteed to converge to a global optimum, but in practice a set of parameters whose error is close to the optimum can be found.
    The reaso