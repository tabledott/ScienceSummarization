ion.
    To classify trees, we here extend the decision stump definition as follows.
    Definition 3 Decision Stumps for Trees Let t and x be labeled ordered trees, and y be a class label (y &#8712; {&#177;1}), a decision stump classifier for trees is given by The parameter for classification is the tuple ht, yi, hereafter referred to as the rule of the decision stumps.
    The decision stumps are trained to find rule h&#710;t, &#710;yi that minimizes the error rate for the given training data T = {hxi, yii}Li=1: In this paper, we will use gain instead of error rate for clarity.
    The decision stumps classifiers for trees are too inaccurate to be applied to real applications, since the final decision relies on the existence of a single tree.
    However, accuracies can be boosted by the Boosting algorithm (Freund and Schapire, 1996; Schapire and Singer, 2000).
    Boosting repeatedly calls a given weak learner to finally produce hypothesis f, which is a linear combination of K hypotheses produced by the pr