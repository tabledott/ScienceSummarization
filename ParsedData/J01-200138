that which minimizes that number (which is, again, the optimal compressed length of that substring), plus the compressed length of each of the lexical items that have been hypothesized to form the lexicon of the corpus.
    It would certainly be natural to try using this figure of merit on words in English, along with the constraint that all words should be divided into exactly two pieces.
    Applied straightforwardly, however, this gives uninteresting results: words will always be divided into two pieces, where one of the pieces is the first or the last letter of the word, since individual letters are so much more common than morphemes.'
    (I will refer to this effect as peripheral cutting below.)
    In addition&#8212;and this is less obvious&#8212;the hierarchical character of de Marcken's model of chunking leaves no place for a qualitative difference between high-frequency &amp;quot;chunks,&amp;quot; on the one hand, and true morphemes, on the other: str is a high-frequency chunk in English (as schl is