
  Syntactic Features For Evaluation Of Machine Translation
  
    Automatic evaluation of machine translabased on computing similarity between system output and human reference translations, has revolutionized the development of MT systems.
    We explore the use of syntactic information, including constituent labels and head-modifier dependencies, in computing similarity between output and reference.
    Our results show that adding syntactic information to the evaluation metric improves both sentence-level and corpus-level correlation with human judgments.
  
  
    Evaluation has long been a stumbling block in the development of machine translation systems, due to the simple fact that there are many correct translations for a given sentence.
    Human evaluation of system output is costly in both time and money, leading to the rise of automatic evaluation metrics in recent years.
    The most commonly used automatic evaluation metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), are based on