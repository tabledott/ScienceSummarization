not be improved upon, the selected sample can be rejected.
    For naive co-training, new samples will always be added, and so there is a possibility that the noise accumulated at later stages will start to degrade performance (see Pierce and Cardie (2001)).
    Second, for naive co-training, the optimal amount of data to be added at each round (i.e. the cache size) is a parameter that needs to be determined on held out data, whereas the agreement-based method determines this automatically.
    We also performed a number of experiments using much more unlabelled training material than before.
    Instead of using 50, 000 sentences from the 1994 WSJ section of the North American News Corpus, we used 417, 000 sentences (from the same section) and ran the experiments until the unlabelled data had been exhausted.
    One experiment used naive co-training, with 50 seed sentences and a cache of size 500.
    This led to an agreement rate of 99%, with performance levels of 85.4% and 85.4% for TNT and C&amp;C respect