byadding insertion features which can take into ac count an agreement with the source side that is notdirectly captured by word pair features.
			Hierarchi cal features are somewhat effective in the 2005 test set by considering the dependency structure of the source side.Finally, we compared our online training algo rithm with sparse features with a baseline system in Table 3.
			The baseline hierarchical phrase-based system is trained using standard max-BLEU training (MERT) without sparse features (Och, 2003).
			Table 3 shows the results obtained by varying the m-oracle and k-best size (k, m = 1, 10) using all structural features and all token types.
			We also experimentedsentence-wise BLEU as an objective function constrained by 10-oracle and 10-best list.
			Even the 1oracle 1-best configuration achieved significant im provements over the baseline system.
			The use ofa larger k-best list further optimizes to the devel opment set, but at the cost of degraded translation quality in the 2004 test set.
			T