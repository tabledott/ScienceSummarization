(X), and p(Y) are the joint and marginal probability mass functions of the variables X and Y, respectively.
    Using maximum likelihood estimates for the probabilities in the above equation, we have where fx, fy, and fxy are the absolute frequencies of appearance of &amp;quot;1&amp;quot;s for the variables X, Y, and both X and Y together, respectively.
    On the other hand, in computational linguistics, information-theoretic measures such as mutual information are widely used (e.g., Bahl et al. 1986; Church and Hanks 1990; Church et al.
    1991; Dagan, Marcus, and Markovitch 1993; Su, Wu, and Chang 1994).
    In information theory, the mutual information I(X, Y) between two binary random variables X and Y is defined as However, in computational linguistics, the term mutual information has been used most of the time to describe only a part of the above sum, namely the term from the X =1, Y =1 case (unweighted by the joint probability p(X = 1, Y = 1)) .
    In other words, this alternative measure of mutual 