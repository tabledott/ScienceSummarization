ds, and about 10,000 when we trained on 1,000,000 words.
    Thus, even Size of trkag training sets. though an additional 4,000 sequences are observed in the full training set, they are so rare (0.4%) that they do not significantly affect the overall accuracy.
    In our initial experiments, which were limited to known words, the error rate for a supervised tri-tag model increased only from 3.30% to 3.87% when the size of the training set was reduced from 1 million words to 64,000 words (see Figure 1).
    All that is really necessary, recalling the rule of thumb, is enough training to allow for ten of each of the tag sequences that do occur.
    This result is applicable to new tag sets, subdomains, or languages.
    We simply continue to increase the amount of training data until the number of training tokens is at least ten times the number of different sequences observed so far.
    Alternatively, we can stop when the singleton events account for a small enough percentage (say 5%) of the total data.
    T