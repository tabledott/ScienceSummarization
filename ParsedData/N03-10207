mp;quot;.
    We first ran IBM's BLEU evaluation script unmodified over the DUC 2001 model and peer summary set.
    The resulting Spearman rank order correlation coefficient (&#961;) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman &#961; for the multidocument task is 0.67 using one reference and 0.70 using three.
    These numbers indicate that they positively correlate at &#945; = 0.018.
    Therefore, BLEU seems a promising automatic scoring metric for summary evaluation.
    According to Papineni et al. (2001), BLEU is essentially a precision metric.
    It measures how well a machine translation overlaps with multiple human translations using n-gram co-occurrence statistics.
    N-gram precision in BLEU is computed as follows: Where Countclip(n-gram) is the maximum number of ngrams co-occurring in a candidate translation and a reference translation, and Count(n-gram) is the number of n-grams in