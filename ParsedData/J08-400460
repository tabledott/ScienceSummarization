tely a qualitative one, but a baseline requirement is an acceptable level of agreement among the annotators, who serve as the instruments of measurement.
    Reliability studies test the soundness of an annotation scheme and guidelines, which is not to be equated with the machine-learnability of data produced by such guidelines.
    The simplest and most common coding in CL involves labeling segments of text with a limited number of linguistic categories: Examples include part-of-speech tagging, dialogue act tagging, and named entity tagging.
    The practices used to test reliability for this type of annotation tend to be based on the assumption that the categories used in the annotation are mutually exclusive and equally distinct from one another; this assumption seems to have worked out well in practice, but questions about it have been raised even for the annotation of parts of speech (Babarczy, Carroll, and Sampson 2006), let alone for discourse coding tasks such as dialogue act coding.
    We concentrat