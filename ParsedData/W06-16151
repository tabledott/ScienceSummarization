).
    Furthermore, end-to-end systems like speech recognizers (Roark et al., 2004) and automatic translators (Och, 2003) use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data.
    However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data.
    In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (Roark and Bacchiani, 2003; Florian et al., 2004; Chelba and Acero, 2004; Ando, 2004; Lease and Charniak, 2005; Daum&#180;e III and Marcu, 2006).
    This work focuses on using unlabeled data from both the source and target domains to learn a common feature representation that is meaningful across both domains.
    We hypothesize that a discriminative model trained in the source domain using this common featur