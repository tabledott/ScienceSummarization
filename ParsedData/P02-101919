assumption, we get the expression in Figure 1.
    The probabilities P(pron wjw) are given by the letter-to-phone model.
    In the following subsections, we first describe how we train and apply the individual error models, and then we show performance results for the combined model compared to the letterbased error model.
    The error model LTR was trained exactly as described originally by Brill and Moore (2000).
    Given a training set of pairs fw2, r2g the algorithm estimates a set of rewrite probabilities p(a !
    ,Q) which are the basis for computing probabilities PLTR(wjr).
    The parameters of the PH model PPH(pron wjpron r) are obtained by training a phone-sequence-to-phone-sequence error model starting from the same training set of pairs fw2, r2g of misspelling and correct word as for the LTR model.
    We convert this set to a set of pronunciations of misspellings and pronunciations of correct words in the following way: For each training sample fw2, r2g we generate m training samples of corre