nomena such as variation for variation&#8217;s sake.
    Metrics that do not exclusively reward similarity with reference texts (such as NIST) are more likely to correlate well with human judges, but all of the existing metrics that we looked at still penalised generators that do not always choose the most frequent variant.
    The results we have reported here are for a relatively simple sublanguage and domain, and more empirical research needs to be done on how well different evaluation metrics and methodologies (including different types of human evaluations) correlate with each other.
    In order to establish reliable and trusted automatic cross-system evaluation methodologies, it seems likely that the NLG community will need to establish how to collect large amounts of high-quality reference texts and develop new evaluation metrics specifically for NLG that correlate more reliably with human judgments of text quality and appropriateness.
    Ultimately, research should also look at developing new evalua