ions in the test set.
    From Table 4, we see that as with the syntactic regularity study, the RNN-based representations perform best.
    In this case, however, Turian&#8217;s CW vectors are comparable in performance to the HLBL vectors.
    With the RNN vectors, the performance improves as the number of dimensions increases.
    Surprisingly, we found that even though the RNN vectors are not trained or tuned specifically for this task, the model achieves better results (RNN-320, RNN640 &amp; RNN-1600) than the previously best performing system, UTD-NB (Rink and Harabagiu, 2012).
  
  
    We have presented a generally applicable vector offset method for identifying linguistic regularities in continuous space word representations.
    We have shown that the word representations learned by a RNNLM do an especially good job in capturing these regularities.
    We present a new dataset for measuring syntactic performance, and achieve almost 40% correct.
    We also evaluate semantic generalization on the SemEv