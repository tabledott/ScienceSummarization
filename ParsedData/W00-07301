all, let us define the training data which belongs to either positive or negative class as follows: xi is a feature vector of the i-th sample represented by an n dimensional vector. yi is the class (positive(+1) or negative(-1) class) label of the i-th data.
    In basic SVMs framework, we try to separate the positive and negative examples by hyperplane written as: (w-x)+b= 0 wElln,bE R. SVMs find the &amp;quot;optimal&amp;quot; hyperplane (optimal parameter w, b) which separates the training data into two classes precisely.
    What &amp;quot;optimal&amp;quot; means?
    In order to define it, we need to consider the margin between two classes.
    Figures 1 illustrates this idea.
    The solid lines show two possible hyperplanes, each of which correctly separates the training data into two classes.
    The two dashed lines parallel to the separating hyperplane show the boundaries in which one can move the separating hyperplane without misclassification.
    We call the distance between each parallel dashed 