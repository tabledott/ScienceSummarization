erated from the lattice for each tuning iteration.
    The system and feature weights were tuned on the union of NIST MT03 and MT04 tasks.
    All four reference translations available for the tuning and test sets were used.
    A first set of weights with the bigram LM was optimized with three iterations.
    A second set of weights was tuned for 5-gram -best list re-scoring.
    The bi-gram and 5-gram English language models were trained on about 7 billion words.
    The final combination outputs were detokenized and cased before scoring.
    The tuning set results on the Arabic to English NIST MT03+MT04 task are shown in Table 1.
    The best score on each metric is shown in bold face fonts.
    The row labeled as no weights corresponds to Equation 5 with uniform system weights and zero NULL weight.
    The baseline corresponds to Equation 5 with TER tuned weights.
    The following three rows correspond to the improved confusion network decoding with different optimization metrics.
    As expected, the sc