tions under which the PCFG does in fact define a distribution over the possible derivations (trees) generated by the underlying grammar.
    The first condition is that the rule probabilities define conditional distributions over how each nonterminal in the grammar can expand.
    The second is a technical condition that guarantees that the stochastic process generating trees terminates in a finite number of steps with probability one.
    A central problem in PCFGs is to define the conditional probability P(&#946;  |X) for each rule X &#8594; &#946; in the grammar.
    A simple way to do this is to take counts from a treebank and then to use the maximum-likelihood estimates: If the treebank has actually been generated from a probabilistic context-free grammar with the same rules and nonterminals as the model, then in the limit, as the training sample size approaches infinity, the probability distribution implied by these estimates will converge to the distribution of the underlying grammar.2 Once the model h