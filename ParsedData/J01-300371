nd our classifier's, degree of agreement over chance, with the gold standard and with each other.
    Expected chance agreement varies with the number and the relative proportions of categories used by the experts.
    This means that two given pairs of experts might reach the same percent agreement on a given task, but not have the same expected chance agreement, if they assigned verbs to classes in different proportions.
    The Kappa statistic ranges from 0, for no agreement above chance, to 1, for perfect agreement.
    The interpretation of the scale of agreement depends on the domain, like all correlations.
    Carletta (1996) cites the convention from the domain of content analysis indicating that .67 &lt;K &lt; .8 indicates marginal agreement, while K&gt; .8 is an indication of good agreement.
    We can observe that only one of our agreement figures comes close to reaching what would be considered &amp;quot;good&amp;quot; under this interpretation.
    Given the very high level of expertise of our hu