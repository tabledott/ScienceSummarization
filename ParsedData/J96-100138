 = 1)), and it can be shown that the average mutual information becomes equal to the entropy H(X) of X (or Y).
    Both of these measures depend on the individual probabilities (or relative frequencies) of the word groups, whereas the Dice coefficient is equal to px p) 1.
    In this case, the test is easier to decide using the Dice coefficient.
    Since we are looking for a way to identify positively correlated events we must be able to easily test the second case, while testing the first case is not relevant.
    Specific mutual information is a good measure of independence (which it was designed to measure), but good measures of independence are not necessarily good measures of similarity.
    The above arguments all support the use of the Dice coefficient over either average or specific mutual information.
    We have confirmed the theoretically expected behavior of the similarity measures through testing.
    In our early work on Champollion (Smadja 1992), we used specific mutual information (SI) as a c