ted anew.
    Summing over all the tables labeled with the same word yields the probability distribution for the ith word given previously observed words w&#8722;i: where n(w&#8722;i) w is the number of instances of w observed in w&#8722;i.
    The first term is the probability of generating w from the cache (i.e., sitting at an occupied table), and the second term is the probability of generating it anew (sitting at an unoccupied table).
    The actual table assignments z&#8722;i only become important later, in the bigram model.
    There are several important points to note about this model.
    First, the probability of generating a particular word from the cache increases as more instances of that word are observed.
    This richget-richer process creates a power-law distribution on word frequencies (Goldwater et al., 2006), the same sort of distribution found empirically in natural language.
    Second, the parameter &#945;0 can be used to control how sparse the solutions found by the model are.
    This