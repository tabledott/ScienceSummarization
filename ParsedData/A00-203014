ion results are summarized in Table 1.
    In both Template Entity (TE) and Template Relation (TR), our system finished in second place among all entrants.
    Nearly all of the work was done by the sentential model; disabling the cross-sentence model entirely reduced our overall F-Score by only 2 points.
    Given multiple constituents that cover identical spans in the chart, only those constituents with probabilities within a While our focus throughout the project was on TE and TR, we became curious about how well the model did at part-of-speech tagging, syntactic parsing, and at name finding.
    We evaluated part-of-speech tagging and parsing accuracy on the Wall Street Journal using a now standard procedure (see Collins 97), and evaluated name finding accuracy on the MUC7 named entity test.
    The results are summarized in Table 2.
    While performance did not quite match the best previously reported results for any of these three tasks, we were pleased to observe that the scores were at or near state-