xperimental online systems (Radev et al. 2001, McKeown et al.
    2002).
    Despite these efforts, however, there are no common, convenient, and repeatable evaluation methods that can be easily applied to support system development and just-in-time comparison among different summarization methods.
    The Document Understanding Conference (DUC 2002) run by the National Institute of Standards and Technology (NIST) sets out to address this problem by providing annual large scale common evaluations in text summarization.
    However, these evaluations involve human judges and hence are subject to variability (Rath et al. 1961).
    For example, Lin and Hovy (2002) pointed out that 18% of the data contained multiple judgments in the DUC 2001 single document evaluation1.
    To further progress in automatic summarization, in this paper we conduct an in-depth study of automatic evaluation methods based on n-gram co-occurrence in the context of DUC.
    Due to the setup in DUC, the evaluations we discussed here are