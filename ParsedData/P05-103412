nate tree-like representation.
    We currently use a small set of features reflecting very local information in the dependency tree to model P(pos(m,t)  |S, T): As an example, consider the children of propri&#233;t&#233; in Figure 3.
    The head-relative positions One can also include features of siblings to produce a Markov ordering model.
    However, we found that this had litt of its modifiers la and Cancel are -1 and +1, respectively.
    Thus we try to predict as follows: Channel Models: We incorporate two distinct channel models, a maximum likelihood estimate (MLE) model and a model computed using Model-1 word-to-word alignment probabilities as in (Vogel et al., 03).
    The MLE model effectively captures non-literal phrasal translations such as idioms, but suffers fr om data sparsity.
    The wordto-word model does not typically suffer from data sparsity, but prefers more literal translations.
    Given a set of treelet translation pairs that cover a given input dependency tree and produce a target 