extracted belongs to the same semantic class.
    The second level of bootstrapping smoothes over some of the problems caused by this assumption.
    In comparative experiments (Thelen and Riloff, 2002), Basilisk outperformed Meta-Bootstrapping.
    But since our goal of learning subjective nouns is different from the original intent of the algorithms, we tried them both.
    We also suspected they might learn different words, in which case using both algorithms could be worthwhile.
    The Meta-Bootstrapping and Basilisk algorithms need seed words and an unannotated text corpus as input.
    Since we did not need annotated texts, we created a much larger training corpus, the bootstrapping corpus, by gathering 950 new texts from the FBIS source mentioned in Section 2.2.
    To find candidate seed words, we automatically identified 850 nouns that were positively correlated with subjective sentences in another data set.
    However, it is crucial that the seed words occur frequently in our FBIS texts or the boo