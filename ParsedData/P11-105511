itive probability only tent, and uses facts from a database (e.g., Freebase) to assignments that produce the labeled facts yi but as supervision for the aggregate-level variables Y'. that the second considers all valid sets of extractions.
    As input we have (1) E, a set of sentences, (2) Of course, these expectations themselves, espeE, a set of entities mentioned in the sentences, (3) cially the second one, would be difficult to comR, a set of relation names, and (4) A, a database pute exactly.
    Our second modification is to do of atomic facts of the form r(e1, e2) for r E R and a Viterbi approximation, by replacing the expectaei E E. Since we are using weak learning, the Y' tions with maximizations.
    Specifically, we compute variables in Y are not directly observed, but can be the most likely sentence extractions for the label approximated from the database A.
    We use a proce- facts arg maxz p(z|xi, yi; 0) and the most likely exdure, relVector(e1, e2) to return a bit vector whose traction for the