tes in Table 7, which correspond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search errors) using no heuristic function and an mWER of 32.6% (57 search errors) using the combined heuristic function.
    The reason is that without a heuristic function, often the &#8220;easy&#8221; part of the input sentence is translated first.
    This yields severe reordering errors. n-gram-based language models.
    Ideally, we would like to take into account long-range dependencies.
    Yet long n-grams are seen rarely and are therefore rarely used on unseen data.
    Therefore, we expect that extending the history length will at some point not improve further translation quality.
    Table 8 shows the effect of the length of the language model history on translation quality.
    We see that the language model perplexity improves from 4,781 for a unigram model to 29.9 for a trigram model.
    The corresponding translation quality improves from an mWER of 45.9% to an mWER of 31.8%.
    The largest e