2008).
    This two-sortedness of defining properties of meaning: &#8216;logical form&#8217; versus &#8216;contextual use&#8217;, has left the quest for &#8216;what is the foundational structure of meaning?&#8217; even more of a challenge.
    Recently, Coecke et al. (2010) used high level cross-disciplinary techniques from logic, category theory, and physics to bring the above two approaches together.
    They developed a unified mathematical framework whereby a sentence vector is by definition a function of the Kronecker product of its word vectors.
    A concrete instantiation of this theory was exemplified on a toy hand crafted corpus by Grefenstette et al. (2011).
    In this paper we implement it by training the model over the entire BNC.
    The highlight of our implementation is that words with relational types, such as verbs, adjectives, and adverbs are matrices that act on their arguments.
    We provide a general algorithm for building (or indeed learning) these matrices from the corpus.
    The im