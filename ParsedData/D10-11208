e typological features of the language.
    We also argue that cross-language universals are beneficial for automatic language processing; however, our focus is on learning language-specific adaptations of these rules from data.
  
  
    The central hypothesis of this work is that unsupervised dependency grammar induction can be improved using universal linguistic knowledge.
    Toward this end our approach is comprised of two components: a probabilistic model that explains how sentences are generated from latent dependency structures and a technique for incorporating declarative rules into the inference process.
    We first describe the generative story in this section before turning to how constraints are applied during inference in Section 4.
    Our model takes as input (i.e., as observed) a set of sentences where each word is annotated with a coarse part-of-speech tag.
    Table 2 provides a detailed technical description of our model&#8217;s generative process, and Figure 1 presents a model diagram.
 