rence resolution scores range from 85-90% on the ACE 2004 and 2005 data sets to a much lower 6070% on the MUC 6 and 7 data sets (e.g.
    Soon et al. (2001) and Yang et al.
    (2003)).
    What accounts for these differences?
    Are they due to properties of the documents or domains?
    Or do differences in the coreference task definitions account for the differences in performance?
    Given a new text collection and domain, what level of performance should we expect?
    We have little understanding of which aspects of the coreference resolution problem are handled well or poorly by state-of-the-art systems.
    Except for some fairly general statements, for example that proper names are easier to resolve than pronouns, which are easier than common nouns, there has been little analysis of which aspects of the problem have achieved success and which remain elusive.
    The goal of this paper is to take initial steps toward making sense of the disparate performance results reported for NP coreference resol