n no global optimum can be found is that it is intractable to find the optimal mapping from the derivation history to the history representation.
    Given this difficulty, it is important to impose appropriate biases on the search for a good history representation.
  
  
    When researchers choose a hand-crafted set of features to represent the derivation history, they are imposing a domain-dependent bias on the learning process through the independence assumptions which are implied by this choice.
    In this work we do not make any independence assumptions, but instead impose soft biases to emphasize some features of the derivation history over others.
    This is achieved through the choice of what features are input explicitly to the computation of and what other history representations 1As is standard, is the sigmoid activation function applied to a weighted sum of its inputs.
    Multi-layered neural networks of this form can approximate arbitrary mappings from inputs to outputs (Hornik et al., 1989),