clude with Section 4, followed by an Appendix describing the training algorithm in more detail.
  
  
    We first introduce our translation model with an example.
    Section 2.2 will describe the model more formally.
    We assume that an English parse tree is fed into a noisy channel and that it is translated to a Japanese sentence.1 PRP-VB1-VB2.
    The probability of reordering it into PRP-VB2-VB1 is 0.723 (the second row in the r-table in Table 1).
    We also reorder VB-TO into TO-VB, and TO-NN into NN-TO, so therefore the probability of the second tree in Figure 1 is Next, an extra word is stochastically inserted at each node.
    A word can be inserted either to the left of the node, to the right of the node, or nowhere.
    Brown et al. (1993) assumes that there is an invisible NULL word in the input sentence and it generates output words that are distributed into random positions.
    Here, we instead decide the position on the basis of the nodes of the input parse tree.
    The insertion probabili