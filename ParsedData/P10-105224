ous setting (using OWL-QN to perform the optimization), suggesting that for the tasks at hand, these features are actually helping.
    The training speed depends of two main factors: the number of iterations needed to achieve convergence and the computational cost of one iteration.
    In this section, we analyze and compare the runtime efficiency of the three optimizers.
    As far as convergence is concerned, the two forms of regularization (E2 and E1) yield the same performance (see Table 3), and the three algorithms exhibit more or less the same behavior.
    They quickly reach an acceptable set of active parameters, which is often several orders of magnitude smaller than the whole parameter set (see results below in Table 4 and 5).
    Full convergence, reflected by a stabilization of the objective function, is however not so easily achieved.
    We have often observed a slow, yet steady, decrease of the log-loss, accompanied with a diminution of the number of active features as the number of iterations