.
    Finding an optimal assignment of words to classes is computationally hard, but we describe two algorithms for finding a suboptimal assignment.
    In Section 4, we apply mutual information to two other forms of word clustering.
    First, we use it to find pairs of words that function together as a single lexical entity.
    Then, by examining the probability that two words will appear within a reasonable distance of one another, we use it to find classes that have some loose semantic coherence.
    In describing our work, we draw freely on terminology and notation from the mathematical theory of communication.
    The reader who is unfamiliar with this field or who has allowed his or her facility with some of its concepts to fall into disrepair may profit from a brief perusal of Feller (1950) and Gallagher (1968).
    In the first of these, the reader should focus on conditional probabilities and on Markov chains; in the second, on entropy and mutual information.
    Source-channel setup.
  
  
    Fig