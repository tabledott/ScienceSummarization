tion using individual features varies greatly, from little above the baseline to almost 22% above the baseline, or a reduction of a third of the error rate, a very good result for a single feature.
    (All reported accuracies in Table 8 are statistically distinct, at the p &lt; .01 level, using an ANOVA [df = 249, F = 334.721, with a Tukey-Kramer post test.)
    The first line of Table 9 shows that the combination of all features achieves an accuracy of 69.8%, which is 35.9% over the baseline, for a reduction in the error rate of 54%.
    This is a rather considerable result, given the very low baseline (33.9%).
    Moreover, recall that our training and testing sets are always disjoint (cf., Lapata and Brew [1999]; Siegel [1999]); in other words, we are predicting the classification of verbs that were never seen in the training corpus, the hardest situation for a classification algorithm.
    The second through sixth lines of Table 9 show the accuracy achieved on each subset of features that results from re