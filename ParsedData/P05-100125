 likely to be names in English).
    Our method captures the spirit of predictive word-clustering but is more general and effective on our tasks.
    It is possible to develop a general theory to show that the auxiliary problems we use are helpful under reasonable conditions.
    The intuition is as follows.
    Suppose we split the features into two parts -P1 and 'P2 and predict -P1 based on 'P2.
    Suppose features in -P1 are correlated to the class labels (but not necessarily correlated among themselves).
    Then, the auxiliary prediction problems are related to the target task, and thus can reveal useful structures of -P2.
    Under some conditions, it can be shown that features in 'P2 with similar predictive performance tend to map to similar low-dimensional vectors through O.
    This effect can be empirically observed in Figure 10 and will be formally shown elsewhere.
    Recall that throughout the experiments, we fix the row-dimension ofO(for each feature group) to 50.
    Figure 11 plots F-measure 