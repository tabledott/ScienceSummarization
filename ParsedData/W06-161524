 domain specific classification models.
    Our work focuses on finding a common representation for features from different domains, not instances.
    We believe this is an important distinction, since the same instance can contain some features which are common across domains and some which are domain specific.
    The key difference between the previous four pieces of work and our own is the use of unlabeled data.
    We do not require labeled training data in the new domain to demonstrate an improvement over our baseline models.
    We believe this is essential, since many domains of application in natural language processing have no labeled training data.
    Lease and Charniak (2005) adapt a WSJ parser to biomedical text without any biomedical treebanked data.
    However, they assume other labeled resources in the target domain.
    In Section 7.3 we give similar parsing results, but we adapt a source domain tagger to obtain the PoS resources.
    To the best of our knowledge, SCL is the first method t