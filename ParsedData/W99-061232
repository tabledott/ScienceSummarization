idence in favor of one of the named entity classes must be very compelling to overcome this bias.
    With only 50 training words per context this is difficult, and in the face of such strong odds against any of the named entity classes the conservative nature of the learning algorithm only braves an entity label (correctly) for 38% more words than the baseline model.
    In contrast, its performance on entity classification rather than identification, measured by forced choice accuracy in labelling the given entities, is comparable to all the other languages, with 79% accuracy relative to the 62% baseline.2 Figure 3 demonstrates that the performance of the algorithm is highly sensitive to the size of the training data.
    Based on Romanian, the first graph shows that as the size of the raw text for bootstrapping increases, F-measure performance increases roughly logrithmically, due almost exclusively to increases in precision.
    (Approximately the same number of unique entities are being identified, but d