anslation engine-related resources (Section 6.1) and could use any additional external resources.
    We have also provided a software package to extract baseline quality estimation features (Section 6.3).
    Participants could submit up to two systems for two variations of the task: ranking, where participants submit a ranking of translations (no ties allowed), without necessarily giving any explicit scores for translations, and scoring, where participants submit a score for each sentence (in the [1,5] range).
    Each of these subtasks is evaluated using specific metrics (Section 6.2).
    The training data used was selected from data available from previous WMT shared-tasks for machine-translation: a subset of the WMT10 English-Spanish test set, and a subset of the WMT09 English-Spanish test set, for a total of 1832 sentences.
    The training data consists of the following resources: The guidelines used by the PE-effort judges to assign scores 1-5 for each of the (source, MT-output, PE-output) triplets a