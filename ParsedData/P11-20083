tional categories specific to Twitter, including URLs and hashtags.
    Next, we obtained a random sample of mostly American English1 tweets from October 27, 2010, automatically tokenized them using a Twitter tokenizer (O&#8217;Connor et al., 2010b),2 and pre-tagged them using the WSJ-trained Stanford POS Tagger (Toutanova et al., 2003) in order to speed up manual annotation.
    Heuristics were used to mark tokens belonging to special Twitter categories, which took precedence over the Stanford tags.
    Stage 1 was a round of manual annotation: 17 researchers corrected the automatic predictions from Stage 0 via a custom Web interface.
    A total of 2,217 tweets were distributed to the annotators in this stage; 390 were identified as non-English and removed, leaving 1,827 annotated tweets (26,436 tokens).
    The annotation process uncovered several situations for which our tagset, annotation guidelines, and tokenization rules were deficient or ambiguous.
    Based on these considerations we revised the toke