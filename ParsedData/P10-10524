ation of these techniques in the same software package.
    Second, the experimental demonstration that using large output label sets is doable and that very large feature sets actually help improve prediction accuracy.
    In addition, we show how sparsity in structured feature sets can be used in incremental training regimes, where long-range features are progressively incorporated in the model insofar as the shorter range features have proven useful.
    The rest of the paper is organized as follows: we first recall the basics of CRFs in Section 2, and discuss three ways to train CRFs with a `1 penalty in Section 3.
    We then detail several implementation issues that need to be addressed when dealing with massive feature sets in Section 4.
    Our experiments are reported in Section 5.
    The main conclusions of this study are drawn in Section 6.
  
  
    In this section, we recall the basics of Conditional Random Fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006) and introduce the notati