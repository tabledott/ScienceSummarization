ines the set of derivations that are possible, so the induction of a probabilistic model first requires a lexicon.
    Therefore, the learning task can be separated into two sub-tasks: (1) the induction of a lexicon, followed by (2) the induction of a probabilistic model.
    Both sub-tasks require a training set, {(ei, fi)}, where each training example (ei, fi) is an NL sentence, ei, paired with its correct MR, fi.
    Lexical induction also requires an unambiguous CFG of the MRL.
    Since there is no lexicon to begin with, it is not possible to include correct derivations in the training data.
    This is unlike most recent work on syntactic parsing based on gold-standard treebanks.
    Therefore, the induction of a probabilistic model for derivations is done in an unsupervised manner.
  
  
    In this section, we focus on lexical learning, which is done by finding optimal word alignments between so that f is a translation of e. Since there may NL sentences and their MRs in the training set.
    By defini