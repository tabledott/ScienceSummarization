s the wrong analysis (and this is counted as a failure for the evaluation in this section), the chosen analysis may agree with some of the correct morphological features.
    We discuss our performance on the POS feature in Section 8.
  
  
    The term &#8220;tokenization&#8221; refers to the segmenting of a naturally occurring input sequence of orthographic symbols into elementary symbols (&#8220;tokens&#8221;) used in subsequent processing steps (such as parsing) as basic units.
    In our approach, we determine all morphological properties of a word at once, so we can use this information to determine tokenization.
    There is not a single possible or obvious tokenization scheme: a tokenization scheme is an analytical tool devised by the researcher.
    We evaluate in this section how well our morphological disambiguation curacy measures for each input word whether it gets tokenized correctly, independently of the number of resulting tokens; the token-based measures refer to the four token fields into wh