ihood.
    The ITG Posterior alignments result from applying competitive thresholding to alignment posteriors under the ITG model.
    Our supervised ITG model gave a 1.1 BLEU increase over GIZA++.
  
  
    This work presented the first large-scale application of ITG to discriminative word alignment.
    We empirically investigated the performance of conditional likelihood training of ITG word aligners under simple and normal form grammars.
    We showed that through the combination of relaxed learning objectives, many-to-one block alignment potential, and efficient pruning, ITG models can yield state-of-the art word alignments, even when the underlying gold alignments are highly nonITG.
    Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance.
  

