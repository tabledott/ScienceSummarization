0.92 BLEUS5 0.73 0.69 0.78 0.88 0.83 0.91 BLEUS6 0.70 0.65 0.75 0.87 0.82 0.91 BLEUS7 0.65 0.60 0.70 0.85 0.80 0.89 BLEUS8 0.58 0.52 0.64 0.82 0.76 0.86 BLEUS9 0.50 0.44 0.57 0.76 0.70 0.82 Adequacy Fluency Table 3.
			Pearson's correlation analysis BLEUS1 to 9 vs. adequacy and fluency of 8 machine translation systems in 2003 NIST Chinese-English machine translation evaluation.
			Method ORANGE Avg Rank 95%-CI-L 95%-CI-U ROUGE-L 20.56% 211 190 234 ROUGE-W-1.1 20.45% 210 189 232 ROUGE-W-1.2 20.47% 210 186 230 ROUGE-W-1.3 20.69% 212 188 234 ROUGE-W-1.4 20.91% 214 191 238 ROUGE-W-1.5 21.17% 217 196 241 ROUGE-W-1.6 21.47% 220 199 242 ROUGE-W-1.7 21.72% 223 200 245 ROUGE-W-1.8 21.88% 224 204 246 ROUGE-W-1.9 22.04% 226 203 249 ROUGE-W-2.0 22.25% 228 206 250 Table 4.
			ORANGE scores for ROUGE-L and ROUGE-W-1.1 to 2.0.
			Method ORANGE Avg Rank 95%-CI-L 95%-CI-U ROUGE-S0 25.15% 258 234 280 ROUGE-S1 22.44% 230 209 253 ROUGE-S2 20.38% 209 186 231 ROUGE-S3 19.81% 203 183 226 ROUGE-S4 19.66% 202 177 224 ROUGE-S5 19.95% 