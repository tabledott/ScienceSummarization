few attempts at creating automatic metrics that can be more easily replicated and used to compare different systems.
    ParaMetric (Callison-Burch et al., 2008) compares the paraphrases discovered by an automatic system with ones annotated by humans, measuring precision and recall.
    This approach requires additional human annotations to identify the paraphrases within parallel texts (Cohn et al., 2008) and does not evaluate the systems at the sentence level.
    The more recently proposed metric PEM (Paraphrase Evaluation Metric) (Liu et al., 2010) produces a single score that captures the semantic adequacy, fluency, and lexical dissimilarity of candidate paraphrases, relying on bilingual data to learn semantic equivalences without using n-gram similarity between candidate and reference sentences.
    In addition, the metric was shown to correlate well with human judgments.
    However, a significant drawback of this approach is that PEM requires substantial in-domain bilingual data to train the semantic 