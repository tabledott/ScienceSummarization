tly by increasing the number of features which could usefully be incorporated.
    The number of features used in our complex models &#8211; in the several hundreds of thousands, is extremely high in comparison with the data set size and the number of features used in other machine learning domains.
    We describe two sets of experiments aimed at comparing models with and without regularization.
    One is for a simple model with a relatively small number of features, and the other is for a model with a large number of features.
    The usefulness of priors in maximum entropy models is not new to this work: Gaussian prior smoothing is advocated in Chen and Rosenfeld (2000), and used in all the stochastic LFG work (Johnson et al., 1999).
    However, until recently, its role and importance have not been widely understood.
    For example, Zhang and Oles (2001) attribute the perceived limited success of logistic regression for text categorization to a lack of use of regularization.
    At any rate, regularized