and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC).
    We view the 3-dimensional tensor as a mapping which assigns each target word w a non-zero value c, given the context (l,v).
    All wordcontext combinations not listed in T are implicitly assigned a zero value.
    Now we consider two possible approaches for obtaining vectors, depending on their application.
    First, we let the D most frequent contexts constitute the D dimensions that each word vector will have.
    Table 2 shows the 11 contexts (l,v) that appear most frequently in T. Thus, each target word&#8217;s vector is defined component-wise as: for j = 1,...,D. This approach is used when a fixed vector dimensionality is necessary.
    A more dynamic approach is possible when very few words w1,...,wn are involved in a test.
    Their representations can then have a denser format, that is, with no zero-valued components.
    For this we identify the set of contexts common to the words involved, c