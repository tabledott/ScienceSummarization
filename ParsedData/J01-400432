 our NLP modules, only about 35% of these 3,600 phrases are &amp;quot;person&amp;quot; and &amp;quot;organization&amp;quot; entities and references.
    Concentrating on just these types has thus affected the overall recall of the RESOLVE system.
    RESOLVE's way of generating training examples also differs from our system's: instances are created for all possible pairings of &amp;quot;relevant entities&amp;quot; and &amp;quot;relevant references,&amp;quot; instead of our system's method of stopping at the first coreferential noun phrase when traversing back from the anaphor under consideration.
    We implemented RESOLVE's way of generating training examples, and the results (DSO-TRG) are reported in Table 3 and Table 4.
    For MUC-7, there is no drop in F-measure; for MUC-6, the F-measure dropped slightly.
    RESOLVE makes use of 39 features, considerably more than our system's 12 features.
    RESOLVE's feature set includes the two highly informative features, ALIAS and STR_MATCH.
    RESOLVE does not u