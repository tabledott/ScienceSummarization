lts for a trigram model of the data in column one, results for the grammar-model in column two, and results for a model in which the two are interpoBoth the were trained and tested on the same training and testing corpora, to be described in Section 4.1.
    As indicated in the table, the trigram model achieved a perplexity of 167 for the testing corpus.
    The grammar models did slightly better (e.g., 158.28 for the Chelba and Jelinek (C&amp;J) parser), but it is the interpolation of the two that is clearly the winner (e.g., 137.26 for the Roark parser/trigram combination).
    In both papers the interpolation constants were 0.36 for the trigram estimate and 0.64 for the grammar estimate.
    While both of these reasons for strict-left-toright parsing (search and trigram interpolation) are valid, they are not necessarily compelling.
    The ability to combine easily with trigram models is important only as long as trigram models can improve grammar models.
    A sufficiently good grammar model would obviate