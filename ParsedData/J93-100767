 the probability of appearance of a given word, w. Entropy measures the predictability of a corpus, in other words, the bigger the entropy of a corpus the less predictable it is.
    In an ideal language model, the entropy of a corpus should not depend on its size.
    However, word probabilities are difficult to approximate (see, for example, Bell CD inches of rain.
    .
    .
    . acid rain.
    .
    .
    .
    CD inches of rain fell heavy rain .
    .
    .
    . the Atlantic hurricane season hurricane force winds rain forests to reduce acid rain .
    .
    .
    . a major hurricane light rain .
    .
    .
    . the most powerful hurricane to hit the .
    .
    .
    . an inch of rain .
    .
    .
    . to save the world s rain forests wind and rain .
    .
    .
    . a cold rain [19871 for a thorough discussion on probability estimation), and in most cases entropy grows with the size of the corpus.
    In this section, we use a simple unigram language model trained on the corpus and we approximat