t never worked perfectly, even on so small a stretch as a single word.
    Rapid phones, such as flaps, were often missed; long phones, such as liquids and stressed vowels, were often broken into several separate segments; and very often phones were simply mislabeled.
    The back end was designed to overcome these problems by navigating through the finite-state network, applying a complicated set of hand-tuned penalties and bonuses to the various paths in order to favor those paths where the low-level acoustics matched the high-level grammatical constraints.
    This system of hand-tuned penalties and bonuses correctly recognized 35% of the sentences (and 77% of the words) in the test set.
    At the time, this level of performance was actually quite impressive, but these days, one would expect much more, now that most systems use parameters trained on real data, rather than a complicated set of hand-tuned penalties and bonuses.
    Although the penalties and bonuses were sometimes thought of as probabilitie