ems currently in use.
    It rewards matches of n-gram sequences, but measures only at most indirectly overall grammatical coherence.
    The BLEU score has been shown to correlate well with human judgement, when statistical machine translation systems are compared (Doddington, 2002; Przybocki, 2004; Li, 2005).
    However, a recent study (Callison-Burch et al., 2006), pointed out that this correlation may not always be strong.
    They demonstrated this with the comparison of statistical systems against (a) manually post-edited MT output, and (b) a rule-based commercial system.
    The development of automatic scoring methods is an open field of research.
    It was our hope that this competition, which included the manual and automatic evaluation of statistical systems and one rulebased commercial system, will give further insight into the relation between automatic and manual evaluation.
    At the very least, we are creating a data resource (the manual annotations) that may the basis of future research in