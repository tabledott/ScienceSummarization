according to the objective function being used; that transformation is then added to the ordered transformation list and the training corpus is updated by applying the learned transformation.
    Learning continues until no transformation can be found whose application results in an improvement to the annotated corpus.
    Other more sophisticated search techniques could be used, such as simulated annealing or learning with a look-ahead window, but we have not yet explored these alternatives.
    Figure 2 shows an example of learning transformations.
    In this example, we assume there are only four possible transformations, Ti through T4, and that the objective function is the total number of errors.
    The unannotated training corpus is processed by the initial-state annotator, and this results in an annotated corpus with 5,100 errors, determined by comparing the output of the initial-state annotator with the manually derived annotations for this corpus.
    Next, we apply each of the possible transformat