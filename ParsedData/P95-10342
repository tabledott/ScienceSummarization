0,000-node knowledge base skeleton derived from resources like WordNet (Miller, 1990), Longman's Dictionary (Procter, 1978), and the PENMAN Upper Model (Bateman, 1990).
    These representations are turned into English during generation.
    Because we are processing unrestricted newspaper text, all modules in JAPANGLOSS must be robust.
    In addition, we show how the model is useful in simplifying the design of a generator and its knowledge bases even when perfect knowledge is available.
    This is accomplished by relegating some aspects of lexical choice (such as preposition selection and noncompositional interlexical constraints) to a statistical component.
    The generator can then use simpler rules and combine them more freely; the price of this simplicity is that some of the output may be invalid.
    At this point, the statistical component intervenes and filters from the output all but the fluent expressions.
    The advantage of this two-level approach is that the knowledge bases in the generator 