
  Further Meta-Evaluation of Machine Translation
  
    j schroeder ed ac uk Abstract This paper analyzes the translation quality of machine translation systems for 10 language pairs translating between Czech, English, French, German, Hungarian, and Spanish.
    We report the translation quality of over 30 diverse translation systems based on a large-scale manual evaluation involving hundreds of hours of effort.
    We use the human judgments of the systems to analyze automatic evaluation metrics for translation quality, and we report the strength of the correlation with human judgments at both the system-level and at the sentence-level.
    We validate our manual evaluation methodology by measuring intraand inter-annotator agreement, and collecting timing information.
  
  
    This paper presents the results the shared tasks of the 2008 ACL Workshop on Statistical Machine Translation, which builds on two past workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007).
    There were two shared tasks thi