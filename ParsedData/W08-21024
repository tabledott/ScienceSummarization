icient training with the perceptron.
    We describe a method that leverages a simple, first-order dependency parser to restrict the search space of the TAG parser in training and testing.
    The lower-order parser runs in O(n3H) time where H &#8810; G; experiments show that it is remarkably effective in pruning the search space of the full TAG parser.
    Experiments on the Penn WSJ treebank show that the model recovers constituent structures with higher accuracy than the approaches of (Charniak, 2000; Collins, 2000; Petrov and Klein, 2007), and with a similar level of performance to the reranking parser of (Charniak and Johnson, 2005).
    The model also recovers dependencies with significantly higher accuracy than state-of-the-art dependency parsers such as (Koo et al., 2008; McDonald and Pereira, 2006).
    Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing.
    This section describes the relationship between our work and this previous w