ng structures (which almost translates into a preference for dependencies between adjacent words) and a preference for dependencies not to cross a verb.
    In this section we describe how this information can be incorporated into model 1.
    In section 7.2, we describe experiments that evaluate the effect of these features on parsing accuracy.
    Figure 3 A partially completed tree derived depth-first.
    &#8220;???
    ?&#8221; marks the position of the next modifier to be generated&#8212;it could be a nonterminal/headword/head-tag triple, or the STOP symbol.
    The distribution over possible symbols in this position could be conditioned on any previously generated structure, that is, any structure appearing in the figure.
    The next child, R3(r3), is generated with probability P(R3(r3) I P, H, h, distancer(2)).
    The distance is a function of the surface string below previous modifiers R1 and R2.
    In principle the model could condition on any structure dominated by H, R1, or R2 (or, for that mat