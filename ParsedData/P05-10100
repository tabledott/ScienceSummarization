
  Probabilistic CFG With Latent Annotations
  
    This paper defines a generative probabilistic model of parse trees, which we call PCFG-LA.
    This model is an extension of PCFG in which non-terminal symbols are augmented with latent variables.
    Finegrained CFG rules are automatically induced from a parsed corpus by training a PCFG-LA model using an EM-algorithm.
    Because exact parsing with a PCFG-LA is NP-hard, several approximations are described and empirically compared.
    In experiments using the Penn WSJ corpus, our automatically trained model gave a performance of 86.6% (F, sentences 40 words), which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection.
  
  
    Variants of PCFGs form the basis of several broadcoverage and high-precision parsers (Collins, 1999; Charniak, 1999; Klein and Manning, 2003).
    In those parsers, the strong conditional independence assumption made in vanilla treebank PCFGs is weakened by annotating non-terminal sy