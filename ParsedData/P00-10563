eficiency.
    So the main differences of these models lie in the alignment model (which may be zeroorder or first-order), in the existence of an explicit fertility model and whether the model is deficient or not.
    For HMM, IBM-4 and IBM-5 it is straightforward to extend the alignment parameters to include a dependence on the word classes of the words around the alignment position.
    In the HMM alignment model we allow for a dependence from the class E = C(ea,_,).
    Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b).
    The classification of the words into a given number of classes (here: 50) is performed automatically by another statistical learning procedure (Kneser and Ney, 1991).
  
  
    The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(8), e(s)), s = 1, .
    .
    .
    ,S .
    In the Estep the counts for one sentence pair (f, e) are calculated.
    For the lexicon 