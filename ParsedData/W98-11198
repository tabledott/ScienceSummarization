running the Hobbs algorithm on the training data.
    Since the training corpus is tagged with reference information, the probability P(pluic,) is easily obtained.
    In building a statistical parser for the Penn Tree-bank various statistics have been collected P (with, t, 1, a) = P(wolh. t, I) if i = a Then we have, P(W. ih, t, I, a) = P(wilt) P(walh, t, I) .
    .P(tonit) To get the probability for each candidate, we divide the above product by: (Charniak, 1997), two of which are P(walh, t, I) and P(wolt, 1).
    To avoid the sparse-data problem, the heads h are clustered according to how they behave in P(walh, t, I).
    The probability of wa is then computed on the basis of h's cluster c(h).
    Our corpus also contains referents' repetition information, from which we can directly compute P(aIrna).
    The four components in equation (8) can be estimated in a reasonable fashion.
    The system computes this product and returns the antecedent wo for a pronoun p that maximizes this probability.
    More fo