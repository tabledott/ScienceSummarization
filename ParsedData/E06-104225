ting the most significant improvement, &#8220;drown&#8221; and &#8220;grasp&#8221;, had some of the smallest target and feedback set feature sets, supporting the theory that adding cogent features may improve performance.
    With an average of 53.8%, all words but one lie well above our simple-attraction baseline, and some even achieve much higher results than the majority-rules baseline.
    Note also that, using this latter baseline, TroFi boosts the nonliteral f-score from 0% to 42.3%.
  
  
    In this section we discuss the TroFi Example Base.
    First, we examine iterative augmentation.
    Then we discuss the structure and contents of the example base and the potential for expansion.
    After an initial run for a particular target word, we have the cluster results plus a record of the feedback sets augmented with the newly clustered sentences.
    Each feedback set sentence is saved with a classifier weight, with newly clustered sentences receiving a weight of 1.0.
    Subsequent runs may be done to