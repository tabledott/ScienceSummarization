ross domains.
    Thus, we considered the strict boundary matching to be a straightforward and unambiguous evaluation criterion.
    Minor issues like those mentioned above could be handled by simple post-processing rules.
    In conclusion we think that the uncertainty detection community may find more flexible evaluation criteria in the future but the strict scope-level metric is definitely a good starting point for evaluation.
    Participants were invited to submit results in different configurations, where systems were allowed to exploit different kinds of annotated resources.
    The three possible submission categories were: The motivation behind the cross-domain and the open challenges was that in this way, we could assess whether adding extra (i.e. not domainspecific) information to the systems can contribute to the overall performance.
  
  
    Training and evaluation corpora were annotated manually for hedge/weasel cues and their scope by two independent linguist annotators.
    Any differences be