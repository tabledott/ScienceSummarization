inalized in order to yield conditional probability models for both source-to-target and target-tosource machine translation applications.
    The main difference between our work and that of Melamed is that we learn joint probability models of translation equivalence not only between words but also between phrases and we show that these models can be used not only for the extraction of bilingual lexicons but also for the automatic translation of unseen sentences.
    In the rest of the paper, we first describe our model (Section 2) and explain how it can be implemented/trained (Section 3).
    We briefly describe a decoding algorithm that works in conjunction with our model (Section 4) and evaluate the performance of a translation system that uses the joint-probability model (Section 5).
    We end with a discussion of the strengths and weaknesses of our model as compared to other models proposed in the literature.
  
  
    In developing our joint probability model, we started out with a very simple generati