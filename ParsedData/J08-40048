: For example, let us assume a very simple annotation scheme for dialogue acts in information-seeking dialogues which makes a binary distinction between the categories statement and info-request, as in the DAMSL dialogue act scheme (Allen and Core 1997).
    Two coders classify 100 utterances according to this scheme as shown in Table 1.
    Percentage agreement for this data set is obtained by summing up the cells on the diagonal and dividing by the total number of items: Ao = (20 + 50)/100 = 0.7.
    Observed agreement enters in the computation of all the measures of agreement we consider, but on its own it does not yield values that can be compared across studies, because some agreement is due to chance, and the amount of chance agreement is affected by two factors that vary from one study to the other.
    First of all, as Scott (1955, page 322) points out, &#8220;[percentage agreement] is biased in favor of dimensions with a small number of categories.&#8221; In other words, given two coding schemes for 