he model by much, at least with the amount of data available in our experiments.
    The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) (Rabiner and juang 1986).
    The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar (see Section 4), and observation probabilities are given by the local likelihoods P(EilU,).
    We can represent the dependency structure (as well as the implied conditional independences) as a special case of Bayesian belief network (Pearl 1988).
    Figure 1 shows the variables in the resulting HMM with directed edges representing conditional dependence.
    To keep things simple, a first-order HMM (bigram discourse grammar) is assumed.
    The HMM representation allows us to use efficient dynamic programming algorithms to compute relevant aspects of the model,