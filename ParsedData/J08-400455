    Donner and Eliasziw (1987) propose a more general form of significance test for arbitrary levels of agreement.
    In contrast, Krippendorff (2004a, Section 11.4.2) states that the distribution of &#945; is unknown, so confidence intervals must be obtained by bootstrapping; a software package for doing this is described in Hayes and Krippendorff (2007).
    4.1.3 Interpreting the Value of Kappa-Like Coefficients.
    Even after testing significance and establishing confidence intervals for agreement coefficients, we are still faced with the problem of interpreting the meaning of the resulting values.
    Suppose, for example, we establish that for a particular task, K = 0.78 &#177; 0.05.
    Is this good or bad?
    Unfortunately, deciding what counts as an adequate level of agreement for a specific purpose is still little more than a black art: As we will see, different levels of agreement may be appropriate for resource building and for more linguistic purposes.
    The problem is not unlike that of int