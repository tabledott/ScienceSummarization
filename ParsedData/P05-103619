ntences.
    However, neither K&amp;M, nor we, have a large enough body of compressed and original sentences from which to create useful language models, so we both make this simplifying assumption.
    At this point it seems like a reasonable choice to make.
    In fact, it compromises the entire enterprise.
    To see this, however, we must descend into more details.
    Let us consider a simplified version of a K&amp;M example, but as reinterpreted for our model: how the noisy channel model assigns a probability of the compressed tree (A) in Figure 3 given the original tree B.
    We compute the probabilities p(A) and p(B I A) as follows (Figure 4): We have divided the probabilities up according to whether they are contributed by the source or channel models.
    Those from the source model are conditioned on, e.g.
    H(np) the history in terms of the tree structure around the noun-phrase.
    In a pure PCFG this would only include the label of the node.
    In our language model it includes much more, su