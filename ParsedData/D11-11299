exponentially with sentence length hence quickly becoming computationally intractable.
  
  
    Whereas semantic compositional mechanisms for set-theoretic constructions are well understood, there are no obvious corresponding methods for vector spaces.
    To solve this problem, Coecke et al. &#8722;&#8722;&#8594; milk.
    The logical recipe tells us to apply the meaning of the verb to the meanings of subject and object.
    But how can a vector apply to other vectors?
    The solution proposed above implies that one needs to have different levels of meaning for words with different types.
    This is similar to logical models where verbs are relations and nouns are atomic sets.
    So verb vectors should be built differently from noun vectors, for instance as matrices.
    The general information as to which words should be matrices and which words atomic vectors is in fact encoded in the type-logical representation of the grammatical structure of the sentence.
    This is the linear map with word vectors 