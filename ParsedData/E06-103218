able that one entry received a much higher human score than would be anticipated from its low Bleu score.
    The offending entry was unusual in that it was not fully automatic machine translation; instead the entry was aided by monolingual English speakers selecting among alternative automatic translations of phrases in the Arabic source sentences and post-editing the result (Callison-Burch, 2005).
    The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training (Och, 2003) to optimize the weights of their log linear models&#8217; feature functions (Och and Ney, 2002).
    This opens the possibility that in order for Bleu to be valid only sufficiently similar systems should be compared with one another.
    For instance, when measuring correlation using Pearson&#8217;s we get a very low correlation of R2 = 0.14 when