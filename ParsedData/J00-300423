itably primed with English text.
    This path is the correct path, corresponding to the text to be or not to be, shown in bold in Figure 4(b).
    4.1 Markov Modeling with Context Figure 4 can easily be converted into a Markov model for a given order of PPM.
    Suppose we use order 1: then we rewrite Figure 4(a) so that the states are bigrams, as shown in Figure 5(a).
    The interpretation of each state is that it corresponds to the last character of the string that labels the state.
    The very first state, labeled t, has no prior context&#8212;in PPM terms, that character will be transmitted by escaping down to order 0 (or &#8212;1).
    Again, the bold arrows in Figure 5(b) shows the path corresponding to the string with spaces inserted correctly.
    Growing a tree for order 1 modeling of tobeornottobe.
    Similar models could be written for higher-order versions of PPM.
    For example, with an order 3 model, states would be labeled by strings of length four (except for the first few states, where t