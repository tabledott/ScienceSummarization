or word-dependent substitution costs as well.
    Examples of such methods are the introduction of information weights as in the NIST measure or the comparison of stems or synonyms, as in METEOR (Banerjee and Lavie, 2005).
  
  
    The different evaluation measures were assessed experimentally on data from the Chinese&#8211;English and the Arabic&#8211;English task of the NIST 2004 evaluation workshop (Przybocki, 2004).
    In this evaluation campaign, 4460 and 1735 candidate translations, respectively, generated by different research MT systems were evaluated by human judges with regard to fluency and adequacy.
    Four reference translations are provided for each candidate translation.
    Detailed corpus statistics are listed in Table 2.
    For the experiments in this study, the candidate translations from these tasks were evaluated using different automatic evaluation measures.
    Pearson&#8217;s correlation coefficient r between automatic evaluation and the sum of fluency and adequacy was calculated.
