as earthquakes.
    A news story is modeled by first generating a sequence of hidden topics according to a Markov model, with each topic generating an observed sentence according to a topic-specific language model.
    These models capture the sequential structure of news stories, and can be used for summarization tasks such as sentence extraction and ordering.
    Our goals are not so different: we wish to discover the sequential dialogue structure of conversation.
    Rather than learning a disaster&#8217;s location is followed by its death toll, we instead wish to learn that a question is followed by an answer.
    An initial conversation model can be created by simply applying the content modeling framework to conversation data.
    We rename the hidden states acts, and assume each post in a Twitter conversation is generated by a single act.5 During development, we found that a unigram language model performed best as the act emission distribution.
    The resulting conversation model is shown as a plate 