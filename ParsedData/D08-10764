  A downside of this approach is, however, that N-best lists can only capture a very small fraction of the search space.
    As a consequence, the line optimization algorithm needs to repeatedly translate the development corpus and enlarge the candidate repositories with newly found hypotheses in order to avoid overfitting on Cs and preventing the optimization procedure from stopping in a poor local optimum.
    In this paper, we present a novel algorithm that allows for efficiently constructing and representing the unsmoothed error surface for all translations that are encoded in a phrase lattice.
    The number of candidate translations thus taken into account increases by several orders of magnitudes compared to N-best MERT.
    Lattice MERT is shown to yield significantly faster convergence rates while it explores a much larger space of candidate translations which is exponential in the lattice size.
    Despite this vast search space, we show that the suggested algorithm is always efficient in both runni