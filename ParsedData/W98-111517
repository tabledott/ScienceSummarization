 quite small.
    Since the average number of edges required to construct just the (left-factored) test corpus trees is 47.5, our parsing system considers as few as 3 times as many edges as are required to actually produce the output tree.
    Almost as interesting, if i is below 1.4, the precision and recall scores of the first parse are better than those obtained by running the parser to exhaustion, even though the probability of the first parses our algorithm returns cannot be higher than that found by the exhaustive version.
    Furthermore, as seen in Figure 3, running our parser past the first parse by a small amount (150% of the edges required for the first parse) produces still more accurate parses.
    At 150% of the minimum number of edges and I) = 1.2 the precision/recall figures are about 2% above those for the maximum likelihood parse.
    We have two (possibly related) theories of these phenomona.
    It may be that the FOM metric used to select constituents forces our parser to concentrate on e