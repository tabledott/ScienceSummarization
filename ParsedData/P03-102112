ting these phrases and reordering the translations in the target language.
    In addition to the feature functions described in (Och and Ney, 2002), our system includes a phrase penalty (the number of alignment templates used) and special alignment features.
    Altogether, the log-linear model includes different features.
    Note that many of the used feature functions are derived from probabilistic models: the feature function is defined as the negative logarithm of the corresponding probabilistic model.
    Therefore, the feature functions are much more &#8217;informative&#8217; than for instance the binary feature functions used in standard maximum entropy models in natural language processing.
    For search, we use a dynamic programming beam-search algorithm to explore a subset of all possible translations (Och et al., 1999) and extractbest candidate translations using A* search (Ueffing et al., 2002).
    Using an-best approximation, we might face the problem that the parameters trained are good for 