e trees in total.
    There were over 500,000 different features.
    The new algorithm is also applicable, with minor modifications, to boosting approaches for classification problems in which the representation also involves sparse binary features (for example, the text classification problems in Schapire and Singer [2000]).
    As far as we are aware, the new algorithm has not appeared elsewhere in the boosting literature.
    Figure 4 shows the improved boosting algorithm.
    Inspection of the algorithm in Figure 3 shows that only margins on examples in the sets A&#254;k~ and A- are modified when a feature k* is selected.
    The feature space in many NLP problems is very sparse (most features only appear on relatively few training examples, or equivalently, most training examples will have only a few nonzero features).
    It follows that in many cases, the sets A&#254;k~ and A- will be much smaller than the overall size of the training set.
    Therefore when updating the model from a to Upd&#240;&#175