mensions: manual evaluation of MT performance and the correlation between manual metrics and automated metrics.
    This year was also the first time we have introduced quality assessments by non-experts.
    In previous years all assessments were carried out through peer evaluation exclusively consisting of developers of machine translation systems, and thereby people who are used to machine translation output.
    This year we have facilitated Amazon&#8217;s Mechanical Turk to investigate two aspects of manual evaluation: How stable are manual assessments across different assessor profiles (experts vs. non-experts) and how reliable are quality judgments of non-expert users?
    While the intra- and inter-annotator agreements between non-expert assessors are considerably lower than for their expert counterparts, the overall rankings of translation systems exhibit a high degree of correlation between experts and non-experts.
    This correlation can be further increased by applying various filtering strategie