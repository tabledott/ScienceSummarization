en for all functions of that class, with a probability of at least , the risk is bounded by where is a non-negative integer called the Vapnik Chervonenkis (VC) dimension, and is a measure of the complexity of the given decision function.
    The r.h.s. term of (1) is called VC bound.
    In order to minimize the risk, we have to minimize the empirical risk as well as VC dimension.
    It is known that the following theorem holds for VC dimension and margin (Vapnik, 1998).
    Theorem 2 (Vapnik) Suppose as the dimension of given training samples as the margin, and as the smallest diameter which encloses all training sample, then VC dimension of the SVMs are bounded by In order to minimize the VC dimension, we have to maximize the margin , which is exactly the strategy that SVMs take.
    Vapnik gives an alternative bound for the risk.
    Theorem 3 (Vapnik) Suppose is an error rate estimated by Leave-One-Out procedure, is bounded as Leave-One-Out procedure is a simple method to examine the risk of the decision