nd, the goal of active learning is to process (unlabeled) training examples in the order in which they are most useful or informative to the classifier (Cohn et al., 1994).
    Usefulness is commonly quantified as the learner's uncertainty about the class of an example (Lewis and Catlett, 1994).
    This neatly dovetails with the criterion for selecting instances to label in CT. We envision a learner that would alternate between selecting its most certain unlabeled examples to label and present to the human for acknowledgment, and selecting its most uncertain examples to present to the human for annotation.
    Ideally, efficient automatic bootstrapping would be complemented by good coverage of the task.
    We leave evaluation of this possibility to future work.
  
  
    This case study explored issues involved with applying co-training to the natural language processing task of identifying base noun phrases, particularly, the scalability of cotraining for large-scale problems.
    Our experiments indicate 