tructures in figure 2.
    Here we use E(y) and D(y) to respectively refer to the set of spines and dependencies in y.
    The function e maps a sentence x paired with a spine (i, ,q) to a feature vector.
    The function d maps dependencies within y to feature vectors.
    This decomposition is similar to the first-order model of McDonald et al. (2005), but with the addition of the e features.
    We will extend our model to include higherorder features, in particular features based on sibling dependencies (McDonald and Pereira, 2006), and grandparent dependencies, as in (Carreras, 2007).
    If y = (E, D) is a derivation, then: the first modifier to the left of the spine for m. The feature-vector definition then becomes: where s, g and q are feature vectors corresponding to the new, higher-order elements.5 As in TAG approaches, there is a mapping from derivations (E, D) to parse trees (i.e., the type of trees generated by a context-free grammar).
    In our case, we map a spine and its dependencies to a con