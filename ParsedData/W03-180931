 for how good these scores are, we compare them with the level of agreement across human judges.
    We calculated pairwise agreement across all participants on the four classification tasks, resulting in the figures given in Table 4.
    These agreement scores give us an upper bound for classification accuracy on each task, from which it is possible to benchmark the classification accuracy of the classifiers on that same task.
    On TASK 1, three of the four classifiers achieved a classification accuracy of .575.
    On TASK 2, the highest-performing classifier (Method 4), achieved a classification accuracy of .725.
    On TASK 3, Method 2 achieved the highest classification accuracy at .600, and on TASK 4, Method 4 achieved a classification accuracy of .675.
    We can see then that the best classifiers perform only marginally below the upper bound on at least two of the tasks.
    While these results may appear at first glance to be less than conclusive, we must bear in mind that we are working with limit