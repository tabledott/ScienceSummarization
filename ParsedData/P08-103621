 at explaining co-occurrences of words.
    These heuristics influence sampling only during the first iterations of the chain.
    Top words for some of discovered local topics, in9Initial experiments suggested that instead of doing this &#8216;pre-training&#8217; we could start with very large priors &#945;loc and &#945;m , and then reduce them through the course of training.
    However, this is significantly more computationally expensive. cluding the first 3 topics associated with the rated aspects, and also top words for some of global topics are presented in Table 1.
    We can see that the model discovered as its first three topics the correct associated aspects: service, location, and rooms.
    Other local topics, as for the MG-LDA model, correspond to other aspects discussed in reviews (breakfast, prices, noise), and as it was previously shown in Titov and McDonald (2008), aspects for global topics correspond to the types of reviewed items (hotels in Russia, Paris hotels) or background words.
    No