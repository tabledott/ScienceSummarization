te-of-theart supervised learning.
    Ando and Zhang (2005) reported a substantial performance improvement compared with state-of-the-art supervised learning results for syntactic chunking with the CoNLL&#8217;00 shared task data (Tjong Kim Sang and Buchholz, 2000) and NER with the CoNLL&#8217;03 shared task data (Tjong Kim Sang and Meulder, 2003).
    One remaining question is the behavior of SSL when using as much labeled and unlabeled data as possible.
    This paper investigates this question, namely, the use of a large amount of unlabeled data in the presence of (fixed) large labeled data.
    To achieve this, it is paramount to make the SSL method scalable with regard to the size of unlabeled data.
    We first propose a scalable model for SSL.
    Then, we apply our model to widely used test collections, namely Penn Treebank (PTB) III data (Marcus et al., 1994) for POS tagging, CoNLL&#8217;00 shared task data for syntactic chunking, and CoNLL&#8217;03 shared task data for NER.
    We used up to 1G-word