using very small amounts of manually labelled seed data and a much larger amount of unlabelled material, agreement-based co-training can significantly improve POS tagger accuracy.
    We also show that simply re-training on all of the newly labelled data is surprisingly effective, with performance depending on the amount of newly labelled data added at each iteration.
    For certain sizes of newly labelled data, this simple approach is just as effective as the agreement-based method.
    We also show that co-training can still benefit both taggers when the performance of one tagger is initially much better than the other.
    We have also investigated whether co-training can improve the taggers already trained on large amounts of manually annotated data.
    Using standard sections of the WSJ Penn Treebank as seed data, we have been unable to improve the performance of the taggers using selftraining or co-training.
    Manually tagged data for English exists in large quantities, which means that there is no 