aluation metric, TER-Plus (TERp)1 that improves over the existing Translation Edit Rate (TER) metric (Snover et al., 2006), incorporating morphology, synonymy and paraphrases, as well as tunable costs for different types of errors that allow for easy interpretation of the differences between human judgments.
    Section 2 summarizes the TER metric and discusses how TERp improves on it.
    Correlation results with human judgments, including independent results from the 2008 NIST Metrics MATR evaluation, where TERp was consistently one of the top metrics, are presented in Section 3 to show the utility of TERp as an evaluation metric.
    The generation of paraphrases, as well as the effect of varying the source of paraphrases, is discussed in Section 4.
    Section 5 discusses the results of tuning TERp to Fluency, Adequacy and HTER, and how this affects the weights of various edit types.
  
  
    Both TER and TERp are automatic evaluation metrics for machine translation that score a translation, the hypothes