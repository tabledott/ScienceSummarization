(Wallach et al., 2009).
    We perform five estimation runs for each document and then calculate standard errors using a bootstrap method.
    Table 2 shows the log probability of held-out data in nats per word for PLTM and LDA, both trained with 200 topics.
    There is substantial variation between languages.
    Additionally, the predictive ability of PLTM is consistently slightly worse than that of (monolingual) LDA.
    It is important to note, however, that these results do not imply that LDA should be preferred over PLTM&#8212;that choice depends upon the needs of the modeler.
    Rather, these results are intended as a quantitative analysis of the difference between the two models.
    As the number of topics is increased, the word counts per topic become very sparse in monolingual LDA models, proportional to the size of the vocabulary.
    Figure 5 shows the proportion of all tokens in English and Finnish assigned to each topic under LDA and PLTM with 800 topics.
    More than 350 topics in the Finni