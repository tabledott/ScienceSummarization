ct a model 100-word summary, they select factoids that occur in at least 30% of summaries, but within the resulting model summary, they do not differentiate between more and less highly weighted factoids.
    Third, they annotate semantic relations among factoids, such as generalization and implication.
    Finally, they report reliability of the annotation using recall and precision, rather than a reliability metric that factors in chance agreement.
    In (Passonneau, 2004), we note that high recall/precision does not preclude low interannotator reliability on a coreference annotation task.
    Radev et al. (2003) also exploits relative importance of information.
    Evaluation data consists of human relevance judgments on a scale from 0 to 10 on for all sentences in the original documents.
    Again, information is lost relative to the pyramid method because a unique reference summary is produced instead of using all the data.
    The reference summary consists of the sentences with highest relevance judge