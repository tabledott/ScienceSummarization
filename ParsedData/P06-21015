 N. A. Smith and J. Eisner.
    2004.
    Annealing techniques for statistical language learning.
    In pages 486&#8211;493.
  
  
    Researchers in empirical natural language processing have expended substantial ink and effort in developing metrics to evaluate systems automatically against gold-standard corpora.
    The ongoing evaluation literature is perhaps most obvious in the machine translation community&#8217;s efforts to better BLEU (Papineni et al., 2002).
    Despite this research, parsing or machine translation systems are often trained using the much simpler and harsher metric of maximum likelihood.
    One reason is that in supervised training, the log-likelihood objective function is generally convex, meaning that it has a single global maximum that can be easily found (indeed, for supervised generative models, the parameters at this maximum may even have a closed-form solution).
    In contrast to the likelihood surface, the error surface for discrete structured prediction is not only riddled