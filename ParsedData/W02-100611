ide of the hyperplane it lies in.
    Input features can be mapped into high dimensional space before performing the optimization and classification.
    A kernel function (linear by default) can be used to reduce the computational cost of training and testing in high dimensional space.
    If the training examples are nonseparable, a regularization parameter ( by default) can be used to control the trade-off between achieving a large margin and a low training error.
    In WEKA&#8217;s implementation of SVM, each nominal feature with possible values is converted into binary (0 or 1) features.
    If a nominal feature takes the th feature value, then the th binary feature is set to 1 and all the other binary features are set to 0.
    We tried higher order polynomial kernels, but they gave poorer results.
    Our reported results in this paper used the linear kernel.
    AdaBoost (Freund and Schapire, 1996) is a method of training an ensemble of weak learners such that the performance of the whole ensemble is