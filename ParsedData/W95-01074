 the hypothesis that the identification of chunks can be done fairly dependably by finite state methods, postponing the decisions that require higher-level analysis to a parsing phase that chooses how to combine the chunks.
    Existing efforts at identifying chunks in text have been focused primarily on low-level noun group identification, frequently as a step in deriving index terms, motivated in part by the limited coverage of present broad-scale parsers when dealing with unrestricted text.
    Some researchers have applied grammar-based methods, combining lexical data with finite state or other grammar constraints, while others have worked on inducing statistical models either directly from the words or from automatically assigned part-of-speech classes.
    On the grammar-based side, Bourigault (1992) describes a system for extracting &amp;quot;terminological noun phrases&amp;quot; from French text.
    This system first uses heuristics to find &amp;quot;maximal length noun phrases&amp;quot;, and then us