Model, the tagging task is viewed as finding the maximum probability sequence of states in a stochastic finite-state machine.
    The transitions between states emit the words of a sentence with a probability P(w I St), the states St themselves model tags or sequences of tags.
    The transitions are controlled by Markovian state transition probabilities P(St, St1_1).
    Because a sentence could have been generated by a number of different state sequences, the states are considered to be &amp;quot;Hidden.&amp;quot; Although methods for unsupervised training of HMM's do exist, training is usually done in a supervised way by estimation of the above probabilities from relative frequencies in the training data.
    The HMM approach to tagging is by far the most studied and applied (Church 1988; DeRose 1988; Charniak 1993).
    In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward implementation of HMM's, which turned out to have the worst accuracy of the four competing methods.
    In the prese