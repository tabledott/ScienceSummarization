k-Leibler divergence between the model q0 and the empirical distribution p: ratio of Ep[f] to Eq(k)[f], with the restriction that &#8721;j fj(x) = C for each event x in the training data (a condition which can be easily satisfied by the addition of a correction feature).
    We can adapt GIS to estimate the model parameters &#952; rather than the model probabilities q, yielding the update rule: The step size, and thus the rate of convergence, depends on the constant C: the larger the value of C, the smaller the step size.
    In case not all rows of the training data sum to a constant, the addition of a correction feature effectively slows convergence to match the most difficult case.
    To avoid this slowed convergence and the need for a correction feature, Della Pietra et al. (1997) propose an Improved Iterative Scaling (IIS) algorithm, whose update rule is the solution to the equation: The gradient of the log likelihood function, or the Ep[f] = &#8721;w,x p(w)q(k)(x|w)f(x)exp(M(x)&#948;(k)) vector of its 