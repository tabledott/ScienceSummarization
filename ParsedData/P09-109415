 experiments, the development set contains 200 sentences and the test set contains 500 sentences, both of which are randomly selected from the human translations of 2008 NIST Open Machine Translation Evaluation: Chinese to English Task.
    The evaluation metrics for SPG are similar to the human evaluation for MT (Callison-Burch et al., 2007).
    The generated paraphrases are manually evaluated based on three criteria, i.e., adequacy, fluency, and usability, each of which has three scales from 1 to 3.
    Here is a brief description of the different scales for the criteria:
  
  
    We use our method to generate paraphrases for the three applications.
    Results show that the percentages of test sentences that can be paraphrased are 97.2%, 95.4%, and 56.8% for the applications of sentence compression, simplification, and similarity computation, respectively.
    The reason why the last percentage is much lower than the first two is that, for sentence similarity computation, many sentences cannot find unit 