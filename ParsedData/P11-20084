nization and tagging guidelines, and for Stage 2, two annotators reviewed and corrected all of the English tweets tagged in Stage 1.
    A third annotator read the annotation guidelines and annotated 72 tweets from scratch, for purposes of estimating inter-annotator agreement.
    The 72 tweets comprised 1,021 tagged tokens, of which 80 differed from the Stage 2 annotations, resulting in an agreement rate of 92.2% and Cohen&#8217;s r. value of 0.914.
    A final sweep was made by a single annotator to correct errors and improve consistency of tagging decisions across the corpus.
    The released data and tools use the output of this final stage.
    We set out to develop a POS inventory for Twitter that would be intuitive and informative&#8212;while at the same time simple to learn and apply&#8212;so as to maximize tagging consistency within and across annotators.
    Thus, we sought to design a coarse tagset that would capture standard parts of speech3 (noun, verb, etc.) as well as categories for token varie