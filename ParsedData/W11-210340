t makes the different entries appear even more closely-matched is that the ranking changes significantly when ordering systems by their &gt;others score rather than the &gt;others score (i.e. when rewarding only wins, and not rewarding ties).
    NUS-TESLA-F goes from being a bottom entry to being a top entry, with CU-SEMPOS-BLEU also benefiting, changing from the middle to the top rank.
    Either way, we see that a BLEU -tuned system is performing just as well as systems tuned to the other metrics.
    This might be an indication that some work remains to be done before a move away from BLEU-tuning is fully justified.
    On the other hand, the close results might be an artifact of the language pair choice.
    Urdu-English translation is still a relatively difficult problem, and MT outputs are still of a relatively low quality.
    It might be the case that human annotators are simply not very good at distinguishing one bad translation from another bad translation, especially at such a fine-grained level.
