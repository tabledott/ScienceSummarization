 corpus inputs, in all four generation modes (Section 2.4).
    Table 1 shows an example of an input to the systems, along with the three human texts (Corpus, Human1, Human2) and the texts produced by all five NLG systems from this data.
    Automatic evaluations: We used NIST2, BLEU3, and ROUGE4 to automatically evaluate the above systems and texts.
    We computed BLEU-N for N = 1..4 (using BLEU-4 as our main BLEU score).
    We also computed NIST-5 and ROUGE-4.
    As a baseline we used string-edit (SE) distance with substitution at cost 2, and deletion and insertion at cost 1, and normalised to range 0 to 1 (perfect match).
    When multiple reference texts are used, the SE score for a generator forecast is the average of its scores against the reference texts; the SE score for a set of generator forecasts is the average of scores for individual forecasts.
    Human evaluations: We recruited 9 experts (people with experience reading forecasts for offshore oil rigs) and 21 non-experts (people with no such 