
  Intelligent Selection of Language Model Training Data
  
    We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.
    Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.
    We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.
  
  
    Statistical N-gram language models are widely used in applications that produce natural-language text as output, particularly speech recognition and machine translation.
    It seems to be a universal truth that output quality can always be improved by using more language model training data, but only if the training data is reasonably well-matched to the desired output.
    This presents a problem, because in virtually any