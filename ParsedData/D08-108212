r context.
    Model III We make the following assumption: We can view this model, called the Mixgram Model, as an interpolation between Model I and II.
    This model gives us a balanced score for both precision and recall.
    The MR model parameters can be estimated independently from the other two.
    These parameters can be viewed as the &#8220;language model&#8221; parameters for the MR structure, and can be estimated directly from the corpus by simply reading off the counts of occurrences of MR productions in MR structures over the training corpus.
    To resolve data sparseness problem, a variant of the bigram Katz Back-Off Model (Katz, 1987) is employed here for smoothing.
    Learning the remaining two categories ofparameters is more challenging.
    In a conventional PCFG parsing task, during the training phase, the correct correspondence between NL words and syntactic structures is fully accessible.
    In other words, there is a single deterministic derivation associated with each training insta