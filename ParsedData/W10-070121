 explored the potential of MTurk for ranking of computer generated questions about provided texts.
			These questions can be used to test reading comprehension and understanding.
			60 Wikipedia articles were selected, for each of which20 questions were generated.
			Turkers provided 5 ratings for each of the 1,200 questions, creating a sig nificant corpus of scored questions.
			Finally, Gordon et al (2010) relied on MTurk to evaluate the quality and accuracy of automatically extracted common sense knowledge (factoids) fromnews and Wikipedia articles.
			Factoids were pro vided by the KNEXT knowledge extraction system.
			6.2 Speech and Vision.
			While MTurk naturally lends itself to text tasks, several teams explored annotation and collection ofspeech and image data.
			We note that one of the pa pers in the main track described tools for collecting such data (Lane et al, 2010).
			Two teams used MTurk to collect text annotations on speech data.
			Marge et al (2010b) identified easy and hard sections of m