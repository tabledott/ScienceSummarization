is of margins on training examples.
    Both papers assume that a fixed distribution D(x, y) is generating both training and test examples and that the goal is to find a hypothesis with a small number of expected errors with respect to this distribution.
    The form of the distribution is not assumed to be known, and in this sense the guarantees are nonparametric, or &#8220;distribution free.&#8221;Freund and Schapire (1997) show that if the weak learning assumption holds (i.e., roughly speaking, a feature with error rate better than chance can be found for any distribution over the sample space X x {-1, +1}), then the training error for the ExpLoss method decreases rapidly enough for there to be good generalization to test examples.
    Schapire et al. (1998) show that under the same assumption, minimization of ExpLoss using the feature selection method ensures that the distribution of margins on training data develops in such a way that good generalization performance on test examples is guaranteed.
    Th