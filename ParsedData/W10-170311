ement.
    These levels of agreement are higher than in previous years, partially due to the fact that that year we randomly included the references along the system outputs.
    In general, judges tend to rank the reference as the best translation, so people have stronger levels of agreement when it is included.
    That said, even when comparisons involving reference are excluded, we still see an improvement in agreement levels over last year.
    In addition to simply ranking the output of systems, we also had people edit the output of MT systems.
    We did not show them the reference translation, which makes our edit-based evaluation different from the Human-targeted Translation Edit Rate (HTER) measure used in the DARPA GALE program (NIST, 2008).
    Rather than asking people to make the minimum number of changes to the MT output in order capture the same meaning as the reference, we asked them to edit the translation to be as fluent as possible without seeing the reference.
    Our hope was that this w