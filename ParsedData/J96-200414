gory distinctions, the kappa statistic normalizes for the amount of expected chance agreement and allows a single measure to be calculated over multiple coders.
    This makes it applicable to the studies we have described, and more besides.
    However, we have yet to discuss the role of expert coders in such studies.
    KID designate one particular coder as the expert.
    Passonneau and Litman have only naive coders, but in essence have an expert opinion available on each unit classified in terms of the majority opinion.
    Silverman et al. treat all coders indistinguishably, although they do build an interesting argument about how agreement levels shift when a number of less-experienced transcribers are added to a pool of highly experienced ones.
    We would argue that in subjective codings such as these, there are no real experts.
    We concur with Krippendorff that what counts is how totally naive coders manage based on written instructions.
    Comparing naive and expert coding as KID do can be a u