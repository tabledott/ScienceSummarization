am co-occurrence scoring procedure BLEU (Papineni et al 2001).
			A similar metric, NIST, used by NIST (NIST 2002) in a couple of machine translation evaluations in the past two years is based on BLEU.
			The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translations with a numerical metric.
			Although the idea of using objective functions to automatically evaluate machine translation quality is not new (Su et al 1992), the success of BLEU prompts a lot of interests in developing better automatic evaluation metrics.
			For example, Akiba et al (2001) proposed a metric called RED based on edit distances over a set of multiple references.
			Nie?en et al (2000) calculated the length normalized edit distance, called word error rate (WER), between a candidate and multiple reference translations.
			Leusch et al (2003) proposed a related measure called position independent word error rate (PER) that did not consider word position, i.e. using bag-o