ems) are trained.
    It can be also achieved at the model level by combining multiple translation or language models together, often in a weighted manner.
    We explore both categories in this work.
    First, we present three methods for ranking the sentences in a general-domain corpus with respect to an in-domain corpus.
    A cutoff can then be applied to produce a very small&#8211;yet useful&#8211; subcorpus, which in turn can be used to train a domain-adapted MT system.
    The first two data selection methods are applications of language-modeling techniques to MT (one for the first time).
    The third method is novel and explicitly takes into account the bilingual nature of the MT training corpus.
    We show that it is possible to use our data selection methods to subselect less than 1% (or discard 99%) of a large general training corpus and still increase translation performance by nearly 2 BLEU points.
    We then explore how best to use these selected subcorpora.
    We test their combination wit