 vector proportionately!3 Most deterministic annealing procedures, therefore, express a direct preference on the entropy H, and choose &#947; and &#952; accordingly: min Ep-Y,e[L(yi,k)] &#8722; T &#183; H(p&#947;,&#952;) (7) &#947;,&#952; In place of a schedule for raising &#947;, we now use a cooling schedule to lower T from oc to &#8722;oc, thereby weakening the preference for high entropy.
    The Lagrange multiplier T on entropy is called &#8220;temperature&#8221; due to a satisfying connection to statistical mechanics.
    Once T is quite cool, it is common in practice to switch to raising &#947; directly and rapidly (quenching) until some convergence criterion is met (Rao and Rose, 2001).
  
  
    Informally, high temperature or &#947; &lt; 1 smooths our model during training toward higher-entropy conditional distributions that are not so peaked at the desired analyses y* .
    Another reason for such smoothing is simply to prevent overfitting to these training examples.
    A typical way to control ov