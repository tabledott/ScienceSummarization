999).
    In the first experiment, we selected 50 human-written abstracts, consisting of 305 sentences in total.
    A human subject then read the decomposition results of these sentences to judge whether they are correct.
    93.8% of the sentences were correctly decomposed.
    In the second experiment, we tested the system in a summary alignment task.
    We ran the decomposition program to identify the source document sentences that were used to construct the sentences in human-written abstracts.
    Human subjects were also asked to select the document sentences that are semantic-equivalent to the sentences in the abstracts.
    We compared the set of sentences identified by the program with the set of sentences selected by the majority of human subjects, which is used as the gold standard in the computation of precision and recall.
    The program achieved an average 81.5% precision, 78.5% recall, and 79.1% f-measure for 10 documents.
    The average performance of 14 human judges is 88.8% precision, 84