nown, that never occurred in the training data.
    Such words are mapped to the +UNKNOWN+ token outright before parsing.
    Whenever the parser estimates a probability with such a truly unknown word in the history, it will necessarily throw all probability mass to the backed-off estimate (&#732;e1 in our earlier notation), since +UNKNOWN+ effectively never occurred in a history context during training.
    The second consequence is that the mapping scheme yields a &#8220;superficient&#8221;30 model, if all other parts of the model are probabilistically sound (which is actually Back-off structure for PTOPNT and PTOPw, which estimate the probability of generating H(w, t) as the root nonterminal of a parse tree.
    PTOPNT is unsmoothed. n/a: not applicable. not the case here).
    With a parsing model such as Collins&#8217; that uses bilexical dependencies, generating words in the course of parsing is done very much as it is in a bigram language model: Every word is generated conditioning on some previously g