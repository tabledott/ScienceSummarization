t raining corpus may not contain enough sentences to estimate richer models.
  Based on the results of these experiments, it appears likely that significantly increasing the size of the train- ing corpus should result in a corresponding improvement in the accuracy of HBG and richer HBG-like models.
  To check the value of the above detailed history, we tried the simpler model: 1. p(Ht IH~p, H=p, Rp, Ipc) 2. p(H2 IHx, Hxp, H2p, Rp, Ip~) 3. p(Sy  IH1, Rp, I 0) 4. p(sem ISy., H1, Rp, Ipc) 5. p(R ISyn, Sam, H1, H2) This model corresponds to a P-CFG with NTs that are the crude syntax and semantic ategories annotated with the lexical heads.
  The Viterbi rate in this case was 66%, a small improvement over the P-CFG model indicating the value of using more context from the derivation tree.
  Conclusions The success of the HBG model encourages future de- velopment of general history-based grammars as a more promising approach than the usual P-CFG.
  More ex- perimentation is needed with a larger Treebank than was use