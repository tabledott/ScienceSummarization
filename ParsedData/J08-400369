better, although the improvements for German are again smaller than expected.
    For Slovene and Turkish, we find improvement only for two out of three parsers, despite a relatively high share of non-projective dependencies (1.9% for Slovene, 1.5% for Turkish).
    Given that Slovene and Turkish have the smallest training data sets of all languages, this is consistent with previous studies showing that pseudo-projective parsing is sensitive to data sparseness (Nilsson, Nivre, and Hall 2007).
    For languages with a lower percentage of non-projective dependencies, the pseudo-projective technique seems to hurt performance more often than not, possibly as a result of decreasing the labeling accuracy, as noted previously.
    It is worth noting that Chinese is a special case in this respect.
    Because there are no non-projective dependencies in this data set, the projectivized training data set will be identical to the original one, which means that the pseudo-projective parser will behave exactly as the proj