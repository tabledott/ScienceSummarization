itable with even the most modest conditioning.
    For example, in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1, M,1, t, h, H).
    Also, remember that H is a pla,ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.
    In the past few years the maximum entropy, or log-linear, approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1,17].
    A complete review of log-linear models is beyond the scope of this paper.
    Rather, we concentrate on the aspects of these models that most directly influenced the model presented here.
    To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.
    In our work we assume that any fea