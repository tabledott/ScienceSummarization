as an alternative to Gaussian prior.
    Under this prior, This corresponds to the absolute smoothing method in language modeling.
    We set the &#945;k = &#945;; i.e. all features share the same constant whose value can be determined using absolute discounting &#945; = n1 n1+2n2 , where n1 and n2 are the number of features occurring once and twice (Ney et al., 1995).
    Another L1 penalizer is the hyperbolic-L1 prior, described in (Pinto et al., 2003).
    The hyperbolic distribution has log-linear tails.
    Consequently the class of hyperbolic distribution is an important alternative to the class of normal distributions and has been used for analyzing data from various scientific areas such as finance, though less frequently used in natural language processing.
    Under a hyperbolic prior, The hyperbolic prior was also tested with CRFs in McCallum and Li (2003).
    Wise choice of features is always vital the performance of any machine learning solution.
    Feature induction (McCallum, 2003) has been s