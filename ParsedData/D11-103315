 million in the general corpus 5.
    The net effect is that of domain adaptation via threshhold filtering.
    New MT systems were then trained solely on these small subcorpora, and compared against the baseline model trained on the entire 12m-sentence general-domain corpus.
    Table 2 contains BLEU scores of the systems trained on subsets of the general corpus.
    All three methods presented for selecting a subset of the general-domain corpus (Cross-Entropy, Moore-Lewis, bilingual Moore-Lewis) could be used to train a state-of-the-art machine translation system.
    The simplest method, using only the source-side cross-entropy, was able to outperform the general-domain model when selecting 150k out of 12 million sentences.
    The other monolingual method, source-side cross-entropy difference, was able to perform nearly as well as the generaldomain model with only 35k sentences.
    The bilingual Moore-Lewis method proposed in this paper works best, consistently boosting performance by 1.8 BLEU while usin