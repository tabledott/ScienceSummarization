, (3) and (4) are well defined conditional probabilities and that the model halts with probability one.
    Consequently, certain PARSER and WORD-PREDICTOR probabilities must be given specific values: P((adjoin-right, TOP')/WkTk) = 1, if h_O = (&lt;/s&gt;, TOP') and h_{-1}. word 0 &lt;s&gt; ensure that the parse generated by our model is consistent with the definition of a complete parse; The word-predictor model (2) predicts the next word based on the preceding 2 exposed heads, thus making the following equivalence classification: After experimenting with several equivalence classifications of the word-parse prefix for the tagger model, the conditioning part of model (3) was reduced to using the word to be tagged and the tags of the two most recent exposed heads: Model (4) assigns probability to different parses of the word k-prefix by chaining the elementary operations described above.
    The workings of the parser module are similar to those of Spatter (Jelinek et al., 1994).
    The equivalence classific