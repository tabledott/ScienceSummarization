cision to start direct internet sales and comparing it to the strategy of other computer makers.
  
  
    Zero level&#8211;Model: The system was trained on approximately 25,000 news articles from Reuters dated between 1/Jan/1997 and 1/Jun/1997.
    After punctuation had been stripped, these contained about 44,000 unique tokens in the articles and slightly more than 15,000 tokens in the headlines.
    Representing all the pairwise conditional probabilities for all combinations of article and headline words3 added significant complexity, so we simplified our model further and investigated the effectiveness of training on a more limited vocabulary: the set of all the words that appeared in any of the headlines.4 Conditional probabilities for words in the headlines that also appeared in the articles were computed.
    As discussed earlier, in our zero-level model, the system was also trained on bigram transition probabilities as an approximation to the headline syntax.
    Sample output from the system using thi