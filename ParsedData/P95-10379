being tagged, 112 is the variable denoting the tag of the previous word, and H3 is the variable denoting the tag of the word two words back.
    Hence, this 4-gram tagging model is the same as a decisiontree model which always asks the sequence of 3 questions: But can a decision-tree model be represented by an n-gram model?
    No, but it can be represented by an interpolated n-gram model.
    The proof of this assertion is given in the next section.
    The standard approach to estimating an n-gram model is a two step process.
    The first step is to count the number of occurrences of each n-gram from a training corpus.
    This process determines the empirical distribution, The second step is smoothing the empirical distribution using a separate, held-out corpus.
    This step improves the empirical distribution by finding statistically unreliable parameter estimates and adjusting them based on more reliable information.
    A commonly-used technique for smoothing is deleted interpolation._ Deleted interpo