ur approach differs in that each sparse feature is. individually integrated in Eq.
			1.
	
	
			We exploited a large number of binary features for statistical machine translation.
			The model wastrained on a small development set.
			The optimiza tion was carried out by MIRA, which is an onlineversion of the large-margin training algorithm.
			Mil lions of sparse features are intuitively considered prone to overfitting, especially when trained on a small development set.
			However, our algorithm with 771millions of features achieved very significant im provements over a conventional method with a small number of features.
			This result indicates that we can easily experiment many alternative features evenwith a small data set, but we believe that our ap proach can scale well to a larger data set for furtherimproved performance.
			Future work involves scal ing up to larger data and more features.
			Acknowledgements We would like to thank reviewers and our colleagues for useful comment and discussion.
	

