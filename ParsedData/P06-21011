oodman, 1996) and machine translation (Kumar and Byrne, 2004).
    In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000).
    We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder.
    Another training approach that incorporates arloss functions is found in the in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).
    Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones.
    The distinction is in using a loss function to calculate the required margins.
    8 Conclusions Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood&#8212;it produces lower-error systems.
    Differen