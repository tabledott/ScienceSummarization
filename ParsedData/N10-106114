ior 3Of course during learning, the argmax is performed over the entire document collection, rather than a single document.
    P(-r JA) consists of several finite Dirichlet draws for each multinomial, which are incorporated as pseudocounts.4 Given the entity type variational posteriors {qk(&#183;)}, as well as the point estimates of the L and Zr elements, we obtain expected counts from each entity&#8217;s attribute word lists and referring mention usages.
    Updating discourse parameters &#948;d(7r): The learned parameters for the discourse module rely on pairwise antecedent counts for assignments to nominal and pronominal mentions.5 Given these expected counts, which can be easily obtained from other factors, the update reduces to a weighted maximum entropy problem, which we optimize using LBFGS.
    The prior P(7rJ&#963;2) is a zero-centered normal distribution with shared diagonal variance &#963;2, which is incorporated via L2 regularization during optimization.
    Updating referring assignments and wor