
  A Generative Constituent-Context Model For Improved Grammar Induction
  
    We present a generative distributional model for the unsupervised induction of natural language syntax which explicitly models constituent yields and contexts.
    Parameter search with EM produces higher quality analyses than previously exhibited by unsupervised systems, giving the best published unparsing results on the Experiments on Penn treebank sentences of comparalength show an even higher 71% on nontrivial brackets.
    We compare distributionally induced and actual part-of-speech tags as input data, and examine extensions to the basic model.
    We discuss errors made by the system, compare the system to previous models, and discuss upper bounds, lower bounds, and stability for this task.
  
  
    The task of inducing hierarchical syntactic structure from observed yields alone has received a great deal of attention (Carroll and Charniak, 1992; Pereira and Schabes, 1992; Brill, 1993; Stolcke and Omohundro, 1994).
    Rese