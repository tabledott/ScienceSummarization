slations using three different evaluation metrics: BLEU score (Papineni et al., 2002), mWER (multi-reference word error rate), and mPER (multi-reference positionindependent word error rate) (Nie&#223;en et al., 2000).
    Note that BLEU score measures quality, whereas mWER and mPER measure translation errors.
    We will present 95%-confidence intervals for the baseline system which are calculated using bootstrap resampling.
    The metrics are calculated w.r.t. one and four English references: the EuroParl data comes with one reference, the NIST 2004 evaluation set and the NIST section of the 2006 evaluation set are provided with four references each, whereas the GALE section of the 2006 evaluation set comes with one reference only.
    This results in much lower BLEU scores and higher error rates for the translations of the GALE set (see Section 4.2).
    Note that these values do not indicate lower translation quality, but are simply a result of using only one reference.
  
  
    We ran our initial experi