	Finally, we added a bias feature on each edge, which acts as a threshold that allows 2This isn?t quite competitive linking, because we use a non-greedy matching.
			in 19 78 Am er ic an s di vo rc ed 1, 12 2, 00 0 ti me s . en 1978 , on a enregistre?
			1,122,000 divorces sur le continent . in 19 78 Am er ic an s di vo rc ed 1, 12 2, 00 0 ti me s . en 1978 , on a enregistre?
			1,122,000 divorces sur le continent .
			(a) (b)Figure 2: Example alignments showing the ef fects of orthographic cognate features.
			(a) Dice and Distance, (b) With Orthographic Features.
			sparser, higher precision alignments.
			With these features, we got an AER of 15.5 (compare to 19.5 for Model 2 in (Och and Ney, 2003)).
			Note that we already have a capacity that Model 2 does not: we can learn a non-quadratic penalty with linear mixtures of our various components ? this gives a similar effect to learning the variance of the Gaussian for Model 2, but is, at least in principle, more flexible.3 These features fix the to-a` erro