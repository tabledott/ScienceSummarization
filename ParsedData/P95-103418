ead of one: deficit Federal fell deficits (* stands for the empty string.)
    But we run the risk that the n-gram model will pick a non-grammatical path like a large Federal deficits fell.
    So we can produce the following lattice instead: the In this case, we use knowledge about agreement to constrain the choices offered to the statistical model, from eight paths down to six.
    Notice that the sixpath lattice has more states and is more complex than the eight-path one.
    Also, the n-gram length is critical.
    When long-distance features control grammaticality, we cannot rely on the statistical model.
    Fortunately, long-distance features like agreement are among the first that go into any symbolic generator.
    This is our first example of how symbolic and statistical knowledge sources contain complementary information, which is why there is a significant advantage to combining them.
    Now we need an algorithm for converting generator inputs into word lattices.
    Our approach is to assign wor