slation lattice, similar to the construction of TMQ , we collect all the source n-grams, and translate them into target n-grams.
			In either case, we get a set of target n-grams for each source word.
			The structured query model for the whole source sentence is a collection of such subsets of target n grams.
			},,,{ 21 Isssst.
			tttQ vLvv= is t v is a set of target n-grams for the source word is : }}{;},{;},{{ 311211 LLL v gramiiigramiigramis ttttttt i ?+??+?= In our experiments, we consider up to trigram for better retrieval efficiency, but higher order n-grams could be used as will.
			The second simplification is that every source word is equally important, thus each n-gram subset is t v will have an equal contribution to the final retrieval results.
			The last simplification is each n-gram within the set of is t v has an equal weight, i.e. we do not use the translation probabilities of the translation model.
			If the system is a phrase-based translation system, we can encode the phrases using the or