#8217;s example &#8211; is not atypical of EM training for other tasks.
    To quantify the notion of peaked distributions over phrase translations, we compute the entropy of the distribution for each French phrase according to the standard definition.
    The average entropy, weighted by frequency, for the most common 10,000 phrases in the learned table was 1.55, comparable to 3.76 for the heuristic table.
    The difference between the tables becomes much more striking when we consider the histogram of entropies for phrases in figure 2.
    In particular, the learned table has many more phrases with entropy near zero.
    The most pronounced entropy differences often appear for common phrases.
    Ten of the most common phrases in the French corpus are shown in figure 3.
    As more probability mass is reserved for fewer translations, many of the alternative translations under &#966;H are assigned prohibitively small probabilities.
    In translating 1,000 test sentences, for example, no phrase translation 