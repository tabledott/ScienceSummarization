 al., 2004).
    We focus on defining the probabilities of different rules by separating different features (including the language model) out from the rule probabilities and organizing them in a log-linear form.
    This straight way makes it clear how rules are used and what they depend on.
    For the two merging rules straight and inverted, applying them on two consecutive blocks A1 and where the Q is the reordering score of block A1 and A2, an is its weight, and 4pLM(A1,A2) is the increment of the language model score of the two blocks according to their final order, ALM is its weight.
    For the lexical rule, applying it is assigned a where p(&#183;) are the phrase translation probabilities in both directions, plex(&#183;) are the lexical translation probabilities in both directions, and exp(1) and exp(|x|) are the phrase penalty and word penalty, respectively.
    These features are very common in state-of-the-art systems (Koehn et al., 2005; Chiang, 2005) and As are weights of features.
    For the r