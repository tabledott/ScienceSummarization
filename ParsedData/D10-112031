onstraints improve grammar induction.
    We formulated a generative model for dependency structure that models syntactic category refinement and biases inference to cohere with the provided constraints.
    Our experiments showed that encoding a compact, well-accepted set of language-independent constraints significantly improves accuracy on multiple languages compared to the current state-of-the-art in unsupervised parsing.
    While our present work has yielded substantial gains over previous unsupervised methods, a large gap still remains between our method and fully supervised techniques.
    In future work we intend to study ways to bridge this gap by 1) incorporating more sophisticated linguistically-driven grammar rulesets to guide induction, 2) lexicalizing the model, and 3) combining our constraint-based approach with richer unsupervised models (e.g., Headden III et al. (2009)) to benefit from their complementary strengths.
  
  
    The authors acknowledge the support of the NSF (CAREER grant IIS-0