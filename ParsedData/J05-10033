 original model.
    The new model achieved 89.75% F-measure, a 13% relative decrease in Fmeasure error over the baseline model&#8217;s score of 88.2%.
    The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data.
    Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach.
    We argue that the method is an appealing alternative&#8212;in terms of both simplicity and efficiency&#8212;to work on feature selection methods within log-linear (maximum-entropy) models.
    Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.
  
  
    Machine-learning approaches to natural language parsing have recently shown some success in complex domains s