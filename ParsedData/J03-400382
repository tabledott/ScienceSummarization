997), Jelinek et al. (1994), and Magerman (1995).
    A strength of these models is undoubtedly the powerful estimation techniques that they use: maximum-entropy modeling (in Ratnaparkhi 1997) or decision trees (in Jelinek et al. 1994 and Magerman 1995).
    A weakness, we will argue in this section, is the method of associating parameters with transitions taken by bottom-up, shift-reduce-style parsers.
    We give examples in which this method leads to the parameters&#8217; unnecessarily fragmenting the training data in some cases or ignoring important context in other cases.
    Similar observations have been made in the context of tagging problems using maximum-entropy models (Lafferty, McCallum, and Pereira 2001; Klein and Manning 2002).
    We first analyze the model of Magerman (1995) through three common examples of ambiguity: PP attachment, coordination, and appositives.
    In each case a word sequence S has two competing structures, T1 and T2, with associated decision sequences (d1,.
    .
    .
   