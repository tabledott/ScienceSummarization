rsing model that combines the speed of small-state PCFGs with the semantic richness of neural word representations and compositional phrase vectors.
    The compositional vectors are learned with a new syntactically untied recursive neural network.
    This model is linguistically more plausible since it chooses different composition functions for a parent node based on the syntactic categories of its children.
    The CVG obtains 90.44% labeled F1 on the full WSJ test set and is 20% faster than the previous Stanford parser.
  
  
    We thank Percy Liang for chats about the paper.
    Richard is supported by a Microsoft Research PhD fellowship.
    The authors gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) prime contract no.
    FA8750-13-2-0040, and the DARPA Deep Learning program under contract number FA8650-10C-7020.
    Any opinions, findings, and conclusions or re