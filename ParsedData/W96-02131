 POS tags to unseen text.
    The experiments in this paper were conducted on the Wall Street Journal corpus from the Penn Treebank project(Marcus et al., 1994), although the model can train from any large corpus annotated with POS tags.
    Since most realistic natural language applications must process words that were never seen before in training data, all experiments in this paper are conducted on test data that include unknown words.
    Several recent papers(Brill, 1994, Magerman, 1995) have reported 96.5% tagging accuracy on the Wall St. Journal corpus.
    The experiments in this paper test the hypothesis that better use of context will improve the accuracy.
    A Maximum Entropy model is well-suited for such experiments since it cornbines diverse forms of contextual information in a principled manner, and does not impose any distributional assumptions on the training data.
    Previous uses of this model include language modeling(Lau et al., 1993), machine translation(Berger et al., 1996), prepositio