 model.
    To adequately evaluate the quality of any translation is difficult, since it is not entirely clear what the focus of the evaluation should be.
    Surely, a good translation has to adequately capture the meaning of the foreign original.
    However, pinning down all the nuances is hard, and often differences in emphasis are introduced based on the interpretation of the translator.
    At the same time, it is desirable to have fluent output that can be read easily.
    These two goals, adequacy and fluency, are the main criteria in machine translation evaluation.
    Human judges may be asked to evaluate the adequacy and fluency of translation output, but this is a laborious and expensive task.
    Papineni et al. [2002] addressed the evaluation problem by introducing an automatic scoring metric, called BLEU, which allowed the automatic calculation of translation quality.
    The system output is compared against a reference translation of the same source text.
    Formally, the BLEU metric is comp