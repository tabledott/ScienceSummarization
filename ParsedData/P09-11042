euristic weights or using rich, learned features.
    A second advantage of ITG approaches is that they admit a range of training options.
    As with general one-to-one matchings, we can optimize margin-based objectives.
    However, unlike with general matchings, we can also efficiently compute expectations over the set of ITG derivations, enabling the training of conditional likelihood models.
    A major challenge in both cases is that our training alignments are often not one-to-one ITG alignments.
    Under such conditions, directly training to maximize margin is unstable, and training to maximize likelihood is ill-defined, since the target alignment derivations don&#8217;t exist in our hypothesis class.
    We show how to adapt both margin and likelihood objectives to learn good ITG aligners.
    In the case of likelihood training, two innovations are presented.
    The simple, two-rule ITG grammar exponentially over-counts certain alignment structures relative to others.
    Because of this, Wu (1997)