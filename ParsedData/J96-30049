econstruct&amp;quot; the word-boundary information.
    In this paper we present a stochastic finite-state model wherein the basic workhorse is the weighted finite-state transducer.
    The model segments Chinese text into dictionary entries and words derived by various productive lexical processes, and&#8212;since the primary intended application of this model is to text-to-speech synthesis&#8212;provides pronunciations for these words.
    We evaluate the system's performance by comparing its segmentation &amp;quot;judgments&amp;quot; with the judgments of a pool of human segmenters, and the system is shown to perform quite well.
  
  
    Any NLP application that presumes as input unrestricted text requires an initial phase of text analysis; such applications involve problems as diverse as machine translation, information retrieval, and text-to-speech synthesis (US).
    An initial step of any textanalysis task is the tokenization of the input into words.
    For a language like English, this problem is ge