els (LMs) may be found in (Chen and Goodman, 1998; Goodman, 2001).
    Phrasetable smoothing differs from ngram LM smoothing in the following ways: Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 53&#8211;61, Sydney, July 2006. c&#65533;2006 Association for Computational Linguistics c(&#732;s, &#732;t) = 0.1 However, probability mass is To model p(t, a|s), we use a standard loglinear reserved for the set of unseen translations, implying that probability mass is subtracted from the seen translations.
    We propose various ways of dealing with these special features of the phrasetable smoothing problem, and give evaluations of their performance within a phrase-based SMT system.
    The paper is structured as follows: section 2 gives a brief description of our phrase-based SMT system; section 3 presents the smoothing techniques used; section 4 reviews previous work; section 5 gives experimental results; and section 6 concludes and discusses future work.