  Hence, the absence of any other information, the probability that phrasesandare generated by the same concept is given by formula (4).
    Note that the fractional counts returned by equation (4) are only an approximation of the t distribution that we are interested in because the Stirling numbers of the second kind do not impose any restriction on the words that are associated with a given concept be consecutive.
    However, since formula (4) overestimates the numerator and denominator equally, the approximation works well in practice.
    In the second step of the algorithm, we apply equation (4) to collect fractional counts for all unigram and high-frequency n-gram pairs in the cartesian product defined over the phrases in each sentence pair (E, F) in a corpus.
    We sum over all these t-counts and we normalize to obtain an initial joint distribution.
    This step amounts to running the EM algorithm for one step over all possible alignments in the corpus.
    3.3 EM training on Viterbi alignments Give