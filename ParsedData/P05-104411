When Bi contains only xi paired with the current best competitor (&#710;y) to y* , we have a technique that resembles maximum margin training (Crammer and Singer, 2001).
    Note that y&#710; will then change across training iterations, making Bi dynamic.
    The difference between supervised and unsupervised learning is that in the latter case, Ai is forced to sum over label sequences y because they weren&#8217;t observed.
    In the unsupervised case, CE maximizes In terms of Eq.
    5, A = {xi}&#215;&#65533; and B = N(xi)&#215;Y.
    EM&#8217;s objective function (Eq.
    1) is a special case where N(xi) = X, for all i, and the denominator becomes Z(~&#952;).
    An alternative is to restrict the neighborhood to the set of observed training examples rather than all possible examples (Riezler,1999; Johnson et al., 1999; Riezler et al., 2000): Another variant is conditional EM.
    Let xi be a pair (xi,1, xi,2) and define the neighborhood to be N(xi) = {&#175;x = (&#175;x1, xi,2)}.
    This approach has been