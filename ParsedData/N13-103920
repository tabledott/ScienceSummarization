t notes it would be difficult.
    We think this is impossible to handle in the rulebased framework used by English tokenizers, given the huge (and possibly growing) number of large compounds like imma, gonna, w/that, etc.
    These are not rare: the word clustering algorithm discovers hundreds of such words as statistically coherent classes (e.g. clusters E1 and E2 in Fig.
    2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near.
    We do not attempt to do Twitter &#8220;normalization&#8221; into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task.
    In fact, many of Twitter&#8217;s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011).
    For example, we s