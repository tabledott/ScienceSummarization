t methods can be used to attempt this global, non-convex optimization.
    We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.
    It never does significantly worse.
    With such improved methods for minimizing error, we can hope to make better use of task-specific training criteria in NLP.
    References L. R. Bahl, P. F. Brown, P. V. de Souza, and R. L. Mercer.
    1988.
    A new algorithm for the estimation of hidden model parameters.
    In pages 493&#8211;496.
    E. Charniak and M. Johnson.
    2005.
    Coarse-to-fine n-best and maxent discriminative reranking.
    In pages 173&#8211;180.
    S. F. Chen and R. Rosenfeld.
    1999.
    A gaussian prior for smoothing maximum entropy models.
    Technical report, CS Dept., Carnegie Mellon University.
    K. Crammer, R. McDonald, and F. Pereira.
    2004.
    New large algorithms for structure