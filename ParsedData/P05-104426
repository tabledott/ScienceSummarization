te running EM many times and selecting the local maximum that maximizes entropy.
    One might do the same for the local maxima of any CE objective, though theoretical and experimental support for this idea remain for future work.
  
  
    We have presented contrastive estimation, a new probabilistic estimation criterion that forces a model to explain why the given training data were better than bad data implied by the positive examples.
    We have shown that for unsupervised sequence modeling, this technique is efficient and drastically outperforms EM; for POS tagging, the gain in accuracy over EM is twice what we would get from ten times as much data and improved search, sticking with EM&#8217;s criterion (Smith and Eisner, 2004).
    On this task, with certain neighborhoods, contrastive estimation suffers less than EM does from diminished prior knowledge and is able to exploit new features&#8212;that EM can&#8217;t&#8212;to largely recover from the loss of knowledge.
  

