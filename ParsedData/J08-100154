nce contribute towards its meaning).
    We attribute our model&#8217;s superior performance, despite the lack of lexicalization, to three factors: (a) the use of more elaborate linguistic knowledge (coreference and grammatical role information); (b) a more holistic representation of coherence (recall that our entity grids operate over texts rather than individual sentences; furthermore, entity transitions can span more than two consecutive sentences, something which is not possible with the LSA model); and (c) exposure to domain relevant texts (the LSA model used in our experiments was not particularly tuned to the Earthquakes or Accidents corpus).
    Our semantic space was created from a large news corpus (see Section 4.2) covering a wide variety of topics and writing styles.
    This is necessary for constructing robust vector representations that are not extremely sparse.
    We thus expect the grid models to be more sensitive to the discourse conventions of the training/test data.
    The accuracy of th