slations and the translation model which is a generative conditional probability of the source sentence given a candidate translation .
    The lexicon of the single-word based IBM models does not take word context into account.
    This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity.
    Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality.
    Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments.
    Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders.
    In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments 