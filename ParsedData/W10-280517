   Modelling compositionality as a machine learning task implies that a great number of different &#8220;types&#8221; of composition (functions combining vectors) may be learned from natural language samples.
    In principle, any semantic relation instantiated by any syntactic structure could be learned if sufficient data is provided.
    This approach must be confronted with other linguistic phenomena, also of greater complexity than just a set of bigrams.
    Finally, we might wonder if there is an upper limit to the number of compositionality functions that we need to learn in natural language, or if there are types of functions that are more difficult, or even impossible, to learn.
  
  
    Thanks are due to Marco Baroni, Stefan Evert, Roberto Zamparelli and the three anonymous reviewers for their assistance and helpful comments.
  

