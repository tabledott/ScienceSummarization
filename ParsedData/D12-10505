mpositional representations when the tree structure is not given, e.g., by greedily inferring a binary tree.
    Although the type of function used for vector composition has attracted much attention, relatively less emphasis has been placed on the basic distributional representations on which the composition functions operate.
    In this paper, we examine three types of distributional representation of increasing sophistication and their effect on semantic composition.
    These include a simple semantic space, where a word&#8217;s vector represents its co-occurrence with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional tuples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010), and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008).
    Word embeddings are distributed representations, low-dimensional and real-valued.
    Each dimension of the embedding represents a latent feature of 