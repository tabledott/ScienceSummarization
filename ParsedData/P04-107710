been used to report the performance of many machine translation systems and it has been shown to correlate well with human judgments, we will explain BLEU in more detail and point out its limitations in the next section.
    We then introduce a new evaluation method called ROUGE-L that measures sentence-to-sentence similarity based on the longest common subsequence statistics between a candidate translation and a set of reference translations in Section 3.
    Section 4 describes another automatic evaluation method called ROUGE-S that computes skipbigram co-occurrence statistics.
    Section 5 presents the evaluation results of ROUGE-L, and ROUGES and compare them with BLEU, GTM, NIST, PER, and WER in correlation with human judgments in terms of adequacy and fluency.
    We conclude this paper and discuss extensions of the current work in Section 6.
  
  
    To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papine