labeled examples, and observed variables on (seed) labeled examples.
    The model was parameterized such that the joint probability of a (label, feature-set) pair P(yi, xi) is written as The model assumes that (y, x) pairs are generated by an underlying process where the label is first chosen with some prior probability P(yi); the number of features mi is then chosen with some probability P(mi); finally the features are independently generated with probabilities P(xulyi).
    We again assume a training set of n examples {x1 . xri} where the first m examples have labels {y1 ... yin}, and the last (n &#8212; m) examples are unlabeled.
    For the purposes of EM, the &amp;quot;observed&amp;quot; data is {(xi, Ya&#8226; &#8226; &#8226; (xrn, Yrn), xfil, and the hidden data is {ym+i y}.
    The likelihood of the observed data under the model is where P(yi, xi) is defined as in (9).
    Training under this model involves estimation of parameter values for P(y), P(m) and P(x I y).
    The maximum likelihood estimat