ad a more significant decrease in the performance of the system; however, the performance is still impressive even with such a small training set.
    On the other hand, the result also shows that merely annotating more data will not yield dramatic improvement in the performance.
    With increased training data it would be possible to use even more detailed models that require more data and could achieve significantly improved overall system performance with those more detailed models.
    For Spanish we had only 223,000 words of training data.
    We also measured the performance of the system with half the training data or slightly more than 100,000 words of text.
    Figure 4.2 shows the results.
    There is almost no change in performance by using as little as 100,000 words of training data.
    Therefore the results in both languages were comparable.
    As little as 100,000 words of training data produces performance nearly comparable to handcrafted systems.
  
  
    While our initial results have be