rning to generate the past tense of an English verb (Ling &amp; Marinov, 1993; Ling, 1994; Mooney &amp; Califf, 1995).
    It is generally agreed that the philosophical problem of induction (Hume, 1748) means that no inductive algorithm is universally better than any other.
    It can be proven that when averaged over a uniform distribution of all possible classification problems, the generalization performance (predictive accuracy on unseen examples) of any inductive algorithm is zero.
    This has been called the &amp;quot;Conservation Law for Generalization Performance&amp;quot; (Schaffer, 1994) or a &amp;quot;no free lunch&amp;quot; theorem (Wolpert, 1992).
    However, averaging over a uniform distribution of all possible functions is effectively equivalent to assuming a &amp;quot;random universe&amp;quot; in which the past is not predictive of the future.
    If all problems are not equally likely, the expected generalization performance over a distribution of real-world problems can of course be positi