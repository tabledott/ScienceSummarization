6), (Eeg-Olofsson and Knuttson, 2003) 2 .Another method for evaluation is verification ((Ga mon et al, 2008), where a human rater checks over a system?s output.
			The drawbacks of this approach are: 1.
			every time the system is changed, a rater is needed to re-check the output, and 2.
			it is very hard to estimate recall.
			What these two evaluation methods have in common is that they side-step the issue of annotator reliability.
			In this section, we show how relying on only onerater can be problematic for difficult error detec tion tasks, and in section 4, we propose a method(?the sampling approach?)
			for efficiently evaluat ing a system that does not require the amount ofeffort needed in the standard approach to annota tion.
			3.1 Annotation.
			To create a gold-standard corpus of error annotations for system evaluation, and also to deter mine whether multiple raters are better than one, 2(Eeg-Olofsson and Knuttson, 2003) had a small evaluation on 40 preposition contexts and it is unclear whether 