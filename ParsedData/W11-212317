 only provide a state and the word to append, then use the returned state to append another word, etc.
    We have modified Moses (Koehn et al., 2007) to keep our state with hypotheses; to conserve memory, phrases do not keep state.
    Syntactic decoders, such as cdec (Dyer et al., 2010), build state from null context then store it in the hypergraph node for later extension.
    Language models that contain wi must also contain prefixes wi for 1 G i G k. Therefore, when the model is queried for p(wnjwn&#8722;1 1 ) but the longest matching suffix is wnf , it may return state s(wn1) = wnf since no longer context will be found.
    IRSTLM and BerkeleyLM use this state function (and a limit of N &#8722;1 words), but it is more strict than necessary, so decoders using these packages will miss some recombination opportunities.
    State will ultimately be used as context in a subsequent query.
    If the context wnf will never extend to the right (i.e. wnf v is not present in the model for all words v) then no sub