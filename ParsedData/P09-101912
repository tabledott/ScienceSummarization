an be performed by reranking an N-best list of hypotheses generated by an MT system (Kumar and Byrne, 2004).
    This reranking can be done for any sentencelevel loss function such as BLEU (Papineni et al., 2001), Word Error Rate, or Position-independent Error Rate.
    Recently, Tromble et al. (2008) extended MBR decoding to translation lattices under an approximate BLEU score.
    They approximated log(BLEU) score by a linear function of n-gram matches and candidate length.
    If E and E' are the reference and the candidate translations respectively, this linear function is given by: where w is an n-gram present in either E or E', and &#952;0,&#952;1,..., &#952;N are weights which are determined empirically, where N is the maximum ngram order.
    Under such a linear decomposition, the MBR decoder (Equation 1) can be written as Tromble et al. (2008) implement the MBR decoder using Weighted Finite State Automata (WFSA) operations.
    First, the set of n-grams is extracted from the lattice.
    Next, the po