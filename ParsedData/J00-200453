.
    The mean results are plotted in Figures 4 and 5 with 95% confidence intervals.
    All four graphs in these figures are on the same scale to facilitate comparison.
    On both tasks involving the entire vocabulary, each of the biases presented in this article improves the efficiency of modeling the available training data.
    When closed-class words were ignored, Model 1 performed better than Method A, because open-class words are more likely to violate the one-to-one assumption.
    However, the explicit noise model in Methods B and C boosted their scores significantly higher than Model 1 and Method A.
    Method B was better than Method C at choosing the single best open-class links, and the situation was reversed for the whole distribution of open-class links.
    However, the differences in performance between these two methods were tiny on the open-class tasks, because they left only two classes for Method C to distinguish: content words and NULLS.
    Most of the scores on the whole distribution 