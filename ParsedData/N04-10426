py models, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum.
    Traditional maximum entropy learning algorithms, such as GIS and IIS (Pietra et al., 1995), can be used to train CRFs, however, it has been found that a quasi-Newton gradient-climber, BFGS, converges much faster (Malouf, 2002; Sha and Pereira, 2003).
    We use BFGS for optimization.
    In our experiments, we shall focus instead on two other aspects of CRF deployment, namely regularization and selection of different model structure and feature types.
    To avoid over-fitting, log-likelihood is often penalized by some prior distribution over the parameters.
    Figure 1 shows an empirical distribution of parameters, A, learned from an unpenalized likelihood, including only features with non-zero count in the training set.
    Three prior distributions that have shape similar to this empirical distribution are the Gaussian prior, exponential prior, and hyperbolic-L1 prior, e