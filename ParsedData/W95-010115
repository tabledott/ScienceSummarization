at the University of Lancaster.
    When training on one million words of text, test set accuracy peaks at 86.6%.
    In [Elworthy, 1994], similar experiments were run.
    There, a peak accuracy of 92.0% was attained using the LOB corpus.&amp;quot; Using the Penn Treebank corpus, a peak accuracy of 83.6% resulted.
    These results are significantly lower than the results achieved using unsupervised transformation-based learning.
    In [Kupiec, 1992] a novel twist to the Baum-Welch algorithm is presented, where instead of having contextual probabilities for a tag following one or more previous tags, words are pooled into equivalence classes, where all words in an equivalence class have the same set of allowable part of speech assignments.
    Using these equivalence classes greatly reduces the number of parameters that need to be estimated.
    Kupiec ran experiments using the original Brown Corpus.
    When training on 440,000 words, test set accuracy was 95.7%, excluding punctuation.
    As shown above, t