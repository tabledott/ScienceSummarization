he confidence by adding a small value, e, to both W+ and W_, giving at = Plugging the value of at from Equ.
    (5) and ht into Equ.
    (4) gives In order to minimize Zt, at each iteration the final algorithm should choose the weak hypothesis (i.e., a feature xt) which has values for W+ and W_ that minimize Equ.
    (6), with W+ &gt; W_.
    We now describe the CoBoost algorithm for the named entity problem.
    Following the convention presented in earlier sections, we assume that each example is an instance pair of the from (xi ,i, x2,) where xj,, E 2x3 , j E 2}.
    In the namedentity problem each example is a (spelling,context) pair.
    The first m pairs have labels yi, whereas for i = m + 1, , n the pairs are unlabeled.
    We make the assumption that for each example, both xi,. and x2,2 alone are sufficient to determine the label yi.
    The learning task is to find two classifiers : 2x1 { &#8212;1, +1} 12 : 2x2 { &#8212;1, +1} such that (x1,) = f2(x2,t) = yt for examples i = 1, , m, and f1 (x1,) = f2