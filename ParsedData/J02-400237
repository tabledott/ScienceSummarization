otation.
    Table 2 shows a confusion matrix between two annotators.
    The numbers represent absolute sentence numbers, and the diagonal (boldface numbers) are the counts of sentences that were identically classified by both annotators.
    We used Krippendorff&#8217;s diagnostics to determine which particular categories humans had most problems with: For each category, agreement is measured with a new data set in which all categories Distribution of rhetorical categories (entire document). except for the category of interest are collapsed into one metacategory.
    Original agreement is compared to that measured on the new (artificial) data set; high values show that annotators can distinguish the given category well from all others.
    When their results are compared to the overall reproducibility of K = .71, the annotators were good at distinguishing AIM (Krippendorff&#8217;s diagnostics; K = .79) and TEXTUAL (K = .79).
    The high agreement in AIM sentences is a positive result that seems to be at od