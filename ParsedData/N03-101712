rrespondences, they introduced a phrase-based joint probability model that simultaneously generates both the Source and Target sentences in a parallel corpus.
    Expectation Maximization learning in Marcu and Wong&#8217;s framework yields both (i) a joint probability distribution , which reflects the probability that phrases and are translation equivalents; (ii) and a joint distribution , which reflects the probability that a phrase at position is translated into a phrase at position .
    To use this model in the context of our framework, we simply marginalize to conditional probabilities the joint probabilities estimated by Marcu and Wong [2002].
    Note that this approach is consistent with the approach taken by Marcu and Wong themselves, who use conditional models during decoding.
  
  
    We used the freely available Europarl corpus 2 to carry out experiments.
    This corpus contains over 20 million words in each of the eleven official languages of the European Union, covering the proceedings of the 