and a global optimum can be found using gradient descent, with the gradient being where ni is the number of true groundings of clause i.
    The expected count can be approximated as where yk are samples generated by MC-SAT.
    To combat overfitting, a Gaussian prior is imposed on all weights.
    In practice, it is difficult to tune the learning rate for gradient descent, especially when the number of groundings varies widely among clauses.
    Lowd &amp; Domingos (2007) used a preconditioned scaled conjugate gradient algorithm (PSCG) to address this problem.
    This estimates the optimal step size in each step as where g is the gradient, d the conjugate update direction, and A a parameter that is automatically tuned to trade off second-order information with gradient descent.
    H is the Hessian matrix, with the (i, j)th entry being The Hessian can be approximated with the same samples used for the gradient.
    Its negative inverse diagonal is used as the preconditioner.1 The open-source Alchemy package