hem was not a sense induction system.
			We reused the data from the SemEval 2007 English lexical sample subtask of task 17, and.
			set up both clustering-style unsupervised evaluation(using OntoNotes senses as gold-standard) and a su pervised evaluation (using the training part of thedataset for mapping).
			We also provide a compari son to the results of the systems participating in the lexical sample subtask of task 17.Evaluating clustering solutions is not straightfor ward.
			The unsupervised evaluation seems to besensitive to the number of senses in the gold stan dard, and the coarse grained sense inventory usedin the gold standard had a great impact in the results.
			The supervised evaluation introduces a mapping step which interacts with the clustering solu tion.
			In fact, the ranking of the participating systems 3All systems in the case of a random train/test split varies according to the evaluation method used.
			We think the two evaluation results should be taken to be complementary regarding 