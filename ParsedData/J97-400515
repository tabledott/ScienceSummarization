gence is as in Table 2.
    The fit for x2 improves, but that is more than offset by a poorer fit for x1.
    The distribution qi is a better distribution than q', in the sense that qi is more similar (less dissimilar) to the empirical distribution than q' is.
    One reason for adopting minimal KL divergence as a measure of goodness is that minimizing KL divergence maximizes likelihood.
    The likelihood of distribution q is the probability of the training corpus according to q: The expression on the right-hand side is &#8212;1/N times the cross entropy of q with respect top, hence maximizing log likelihood is equivalent to minimizing cross entropy.
    Finally, D(1q) is equal to the cross entropy of q less the entropy of 15, and the entropy of /3 is constant with respect to q; hence minimizing cross entropy (maximizing likelihood) is equivalent to minimizing divergence.
    For stochastic context-free grammars, it can be shown that the ERF method yields the best model for a given training corpus.
    First