ams instead of unigrams.24 All the details of the model remain the same.
    Once a general content distribution over bigrams has been determined by hierarchical topic modeling, the KLSUM criterion is used as before to extract a summary.
    This system, labeled HIERSUM bigram in table 3, yields 9.3 R-2 without stop words, significantly outperforming HIERSUM unigram.
    This model outperforms PYTHY with and without sentence simplification, but not with statistical significance.
    We conclude that both PYTHY variants and HIERSUM bigram are comparable with respect to ROUGE performance.
    24Note that by doing topic modeling in this way over bigrams, our model becomes degenerate as it can generate inconsistent bags of bigrams.
    Future work may look at topic models over n-grams as suggested by Wang et al. (2007).
    In order to obtain a more accurate measure of summary quality, we performed a simple user study.
    For each document set in the DUC 2007 collection, a user was given a reference summary, a P