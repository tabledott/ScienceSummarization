context of the noun, as well as an evaluator-set similarity score between 1 and 7 (along with an evaluator identifier), where 1 is low similarity and 7 is high.
    Evaluation Methodology Candidate models provide a similarity score for each entry.
    The scores of high similarity entries and low similarity entries are averaged to produce a mean High score and mean Low score for the model.
    The correlation of the model&#8217;s similarity judgements with the human judgements is also calculated using Spearman&#8217;s p, a metric which is deemed to be more scrupulous, and ultimately that by which models should be ranked, by Mitchell and Lapata (2008).
    The mean for each model is on a [0, 1] scale, except for UpperBound which is on the same [1, 7] scale the annotators used.
    The p scores are on a [&#8722;1, 1] scale.
    It is assumed that inter-annotator agreement provides the theoretical maximum p for any model for this experiment.
    The cosine measure of the verb vectors, ignoring the noun, is taken