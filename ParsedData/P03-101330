ized baseline; hence it is unclear if lexicalization really improves parsing performance for these languages.
    As Experiment 1 showed, this cannot be taken for granted.
  
  
    We presented the first probabilistic full parsing model for German trained on Negra, a syntactically annotated corpus.
    This model uses lexical sisterhead dependencies, which makes it particularly suitable for parsing Negra&#8217;s flat structures.
    The flatness of the Negra annotation reflects the syntactic properties of German, in particular its semi-free wordorder.
    In Experiment 1, we applied three standard parsing models from the literature to Negra: an unlexicalized PCFG model (the baseline), Carroll and Rooth&#8217;s (1998) head-lexicalized model, and Collins&#8217;s (1997) model based on head-head dependencies.
    The results show that the baseline model achieves a performance of up to 73% recall and 70% precision.
    Both lexicalized models perform substantially worse.
    This finding is at odds with what has 