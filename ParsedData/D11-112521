he weights w' learned by the classifier in iteration t with those from iteration t &#8722; 1 by a factor of &#936;, such that wt = &#936; &#183; w' + (1 &#8722; &#936;) &#183; wt&#8722;1.
    We found &#936; = 0.1 gave good performance across the board.
    We implore the reader to avoid the natural tendency to compare results using baseline vs. extended features or between PBMT and SBMT on the same language pair.
    Such discussions are indeed interesting, and could lead to improvements in feature engineering or sartorial choices due to the outcome of wagers (Goodale, 2008), but they distract from our thesis.
    As can be seen in Table 1, for each of the 12 choices of system, language pair, and feature set, the PRO method performed nearly the same as or better than MIRA and MERT on test data.
    In Figure 5 we show the tune and test BLEU using the weights learned at every iteration for each Urdu-English SBMT experiment.
    Typical of the rest of the experiments, we can clearly see that PRO appears to pro