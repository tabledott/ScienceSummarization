odels, they have efficient procedures for complete, non-greedy finite-state inference and training.
    CRFs have shown empirical successes recently in POS tagging (Lafferty et al., 2001), noun phrase segmentation (Sha and Pereira, 2003) and Chinese word segmentation (McCallum and Feng, 2003).
    Given these models&#8217; great flexibility to include a wide array of features, an important question that remains is what features should be used?
    For example, in some cases capturing a word tri-gram is important, however, there is not sufficient memory or computation to include all word tri-grams.
    As the number of overlapping atomic features increases, the difficulty and importance of constructing only certain feature combinations grows.
    This paper presents a feature induction method for CRFs.
    Founded on the principle of constructing only those feature conjunctions that significantly increase loglikelihood, the approach builds on that of Della Pietra et al (1997), but is altered to work with condi