s.
    It is time-consuming for each feature to figure out a probability when adding a new link, especially when the sentences are very long.
    For our models, gain(a, l) can be obtained in a more efficient way 3: 3We still call the new heuristic function gain to reduce notational overhead, although the gain in Eq.
    13 is not equivalent to the one in Eq.
    12.
    The gain threshold t depends on the added link l. We remove this dependency for simplicity when using it in search algorithm by treating it as a fixed real-valued number.
  
  
    We present in this section results of experiments on a parallel corpus of Chinese-English texts.
    Statistics for the corpus are shown in Table 1.
    We use a training corpus, which is used to train IBM translation models, a bilingual dictionary, a development corpus, and a test corpus. gual dictionary (Dict), development corpus (Dev), and test corpus (Test).
    The Chinese sentences in both the development and test corpus are segmented and POS tagged by ICTCLA