alence (DMV) of Klein and Manning (2004), as obtained by the implementation of Ganchev et al. (2010).
    We trained on sentences of length 10 or less and evaluated on all sentences from the test set.4 For DMV, we reversed the direction of all dependencies if this led to higher performance.
    From this table we can see that direct transfer is a very strong baseline and is over 20% absolute better than the DMV model for both gold and predicted POS tags.
    Table 4, which we will discuss in more detail later, further shows that the direct transfer parser also significantly outperforms stateof-the-art unsupervised grammar induction models, but in a more limited setting of sentences of length less than 10.
    Direct transfer works for a couple of reasons.
    First, part-of-speech tags contain a significant amount of information for parsing unlabeled dependencies.
    Second, this information can be transferred, to some degree, across languages and treebank standards.
    This is because, at least for Indo-Eu