t incorporates arbitrary loss functions is found in the structured prediction literature in the margin-based-learning community (Taskar et al., 2004; Crammer et al., 2004).
    Like other max-margin techniques, these attempt to make the best hypothesis far away from the inferior ones.
    The distinction is in using a loss function to calculate the required margins.
  
  
    Despite the challenging shape of the error surface, we have seen that it is practical to optimize task-specific error measures rather than optimizing likelihood&#8212;it produces lower-error systems.
    Different methods can be used to attempt this global, non-convex optimization.
    We showed that for MT, and sometimes for dependency parsing, an annealed minimum risk approach to optimization performs significantly better than a previous line-search method that does not smooth the error surface.
    It never does significantly worse.
    With such improved methods for minimizing error, we can hope to make better use of task-specific tr