oximative stemming sub-model (sub-model 9) uses the first 4 letters of each vocabulary item as the stem for English and French while for Arabic we use the full word as the stem.
    We also use submodels for backed off fertility, and direct penalization of unaligned English words (&#8220;zero fertility&#8221;) and aligned English words, and unaligned Foreign words (&#8220;NULL-generated&#8221; words) and aligned Foreign words.
    This is a small sampling of the kinds of knowledge sources we can use in this framework; many others have been proposed in the literature.
    Table 4 shows an evaluation of discriminative reranking.
    We observe: 1.
    The first line is the starting point, which is the Viterbi alignment of the 4th iteration of HMM training.
    2.
    The 1-to-many alignments generated by discriminatively reranking Model 4 are better than the 1-to-many alignments of four iterations of Model 4.
    3.
    The 1-to-many alignments of the discriminatively reranked extended model are much better tha