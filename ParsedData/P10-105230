 development (resp. test) data, which compares favorably with some of the best results for this task (for instance (Toutanova et al., 2003; Shen et al., 2007; Suzuki and Isozaki, 2008)).
  
  
    In this paper, we have discussed various ways to train extremely large CRFs with a ' penalty term and compared experimentally the results obtained, both in terms of training speed and of accuracy.
    The algorithms studied in this paper have complementary strength and weaknesses: OWL-QN is probably the method of choice in small or moderate size applications while BCD is most efficient when using very large feature sets combined with limited-size observation alphabets; SGD complemented with fine tuning appears to be the preferred choice in most large-scale applications.
    Our analysis demonstrate that training large-scale sparse models can be done efficiently and allows to improve over the performance of smaller models.
    The CRF package developed in the course of this study implements many algorithmic optimizat