ons were, for instance, the segmentation of Chinese character sequences into Chinese &#8220;words&#8221; and the bracketing of the source sentence into sub-sentential chunks.
    In (Wu, 1996) the baseline ITG constraints were used for statistical machine translation.
    The resulting algorithm is similar to the one presented in Sect.
    3.1, but here, we use monotone translation hypotheses of the full IBM Model 4 as initialization, whereas in (Wu, 1996) a single-word based lexicon model is used.
    In (Vilar, 1998) a model similar to Wu&#8217;s method was considered.
  
  
    We have described the ITG constraints in detail and compared them to the IBM constraints.
    We draw the following conclusions: especially for long sentences the ITG constraints allow for higher flexibility in word-reordering than the IBM constraints.
    Regarding the Viterbi alignment in training, the baseline ITG constraints yield a similar coverage as the IBM constraints on the Verbmobil task.
    On the Canadian Hansards task 