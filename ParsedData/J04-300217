 data from a corpus of Usenet newsgroup messages.
    Table 1 summarizes the data sets and annotations used in this article.
    None of the datasets overlap.
    The annotation types listed in the table are those used in the experiments presented in this article.
    In our first subjectivity annotation project (Wiebe, Bruce, and O&#8217;Hara 1999; Bruce and Wiebe 1999), a corpus of sentences from the Wall Street Journal Treebank Corpus (Marcus, Santorini, and Marcinkiewicz 1993) (corpus WSJ-SE in Table 1) was annotated at the sentence level by multiple judges.
    The judges were instructed to classify a sentence as subjective if it contained any significant expressions of subjectivity, attributed to either the writer or someone mentioned in the text, and to classify the sentence as objective, otherwise.
    After multiple rounds of training, the annotators independently annotated a fresh test set of 500 sentences from WSJ-SE.
    They achieved an average pairwise kappa score of 0.70 over the entire test se