aluation is only fair (as measured by the Kappa score), agreement can be improved by removing the first (up to 50) judgments of each assessor, focusing on the judgments that were made once the assessors are more familiar with the task.
    Inter-annotator agreement with respect to correctness judgments of the edited translations were higher (moderate), which is probably due to the simplified evaluation criterion (binary judgments versus rankings).
    Inter-annotator agreement for both conditions can be increased further by removing the judges with the worst agreement.
    Intra-annotator agreement on the other hand was considerably higher ranging between moderate and substantial.
    In addition to the manual evaluation criteria we applied a large number of automated metrics to see how they correlate with the human judgments.
    There is considerably variation between the different metrics and the language pairs under consideration.
    As in WMT08, the ULC metric had the highest overall correlation with hu