ations (1)&#8211;(4) in this more general setting, we must still require multiplicative or additive decomposability, defining p(d) def = HeEd pe, r(d) def EeEd re, s(d) def EeEd se as before.
    But the H and E operators here now denote appropriate operations within P, R, and S respectively (rather than the usual operations within R).
    Under the first-order expectation semiring ER,R-, the inside algorithm of Figure 2 will return (Z, r) where r is a vector of n feature expectations.
    However, Eisner (2002, section 5) observes that this is inefficient when n is large.
    Why?
    The inside algorithm takes the trouble to compute an inside weight &#946;(v) E R x Rn for each node v in the hypergraph (or lattice).
    The second component of &#946;(v) is a presumably dense vector of all features that fire in all subderivations rooted at node v. Moreover, as &#946;(v) is computed in lines 3&#8211;8, that vector is built up (via the &#174; and &#174; operations of Table 1) as a linear combination of other de