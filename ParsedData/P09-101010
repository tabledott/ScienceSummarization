d &#8212; that is, the reward we expect while acting according to that policy from state s. Formally, we maximize the value function: where the history h is the sequence of states and actions encountered while interpreting a single document d E D. This expectation is averaged over all documents in D. The distribution p(h|&#952;) returns the probability of seeing history h when starting from state s and acting according to a policy with parameters &#952;.
    This distribution can be decomposed into a product over time steps: Input: A document set D, Feature representation &#966;, Reward function r(h), Number of iterations T Our reinforcement learning problem is to find the parameters &#952; that maximize V&#952; from equation 2.
    Although there is no closed form solution, policy gradient algorithms (Sutton et al., 2000) estimate the parameters &#952; by performing stochastic gradient ascent.
    The gradient of V&#952; is approximated by interacting with the environment, and the resulting reward is used to