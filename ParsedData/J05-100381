s&#240;k, &#175;a&#222; 1/4 ExpLoss&#240;Upd&#240;&#175;a, k, BestWt&#240;k, &#175;a&#222;&#222;&#222; The first thing to note is that an update in parameters from a&#175; to Upd&#240;&#175;a, k,d&#222;&#222; results in a simple additive update to the ranking function F: F&#240;xi,j, Upd&#240;&#175;a, k, d&#222;&#222; 1/4 F&#240;xi,j, a&#222; &#254; dhk&#240;xi,j&#222; It follows that the margin on example &#240;i, j&#222; also has a simple update: Next, we note that 1/2hk&#240;xi,1&#222; &#8212; hk&#240;xi,j&#222;] can take on three values: +1, &#8212;1, or 0.
    We split the training sample into three sets depending on this value: A&#254;k 1/4 f&#240;i,j&#222; : 1/2hk&#240;xi,1&#222; &#8212; hk&#240;xi,j&#222;] 1/4 1g To find the value of d that minimizes this loss, we set the derivative of (A.1) with respect to d to zero, giving the following solution: where Z = ExpLoss(&#175;a) = Pi Pni 2 Si,je&#8212;Mi,j(&#175;a) is a constant (for constant &#175;a) which appears in the BestLoss for all features and the