 previously applied to improve word alignments.
    In (CallisonBurch et al., 2004), a generative model for word alignment is trained using unsupervised learning on parallel text.
    In addition, another model is trained on a small amount of hand-annotated word alignment data.
    A mixture model provides a probability for phrase table trained on monolingual Chinese news data.
    Selection step using threshold on confidence scores.
    NIST Chinese&#8211;English. word alignment.
    Experiments showed that putting a large weight on the model trained on labeled data performs best.
    Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences.
    The word alignments are used to train a standard phrasebased SMT system, resulting in increased translation quality .
    In (Callison-Burch, 2002) co-training is applied to MT.
    This approach requires several source languages which are sentenc