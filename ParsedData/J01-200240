T, and trains a maximum entropy model using the improved iterative scaling algorithm for one hundred iterations.
    The final model has a weighting parameter for each feature value that is relevant to the estimation of the probability P(tag I features), and combines the evidence from diverse features in an explicit probability model.
    In contrast to the other taggers, both known and unknown words are processed by the same van Halteren, Zavrel, and Daelemans Combination of Machine Learning Systems model.
    Another striking difference is that this tagger does not have a separate storage mechanism for lexical information about the focus word (i.e., the possible tags).
    The word is merely another feature in the probability model.
    As a result, no generalizations over groups of words with the same set of potential tags are possible.
    In the tagging phase, a beam search is used to find the highest probability tag sequence for the whole sentence.
    3.2.4 Hidden Markov Models.
    In a Hidden Markov 