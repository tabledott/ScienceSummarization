equently, our focus on natural language learning raises the question of how co-training scales when a large number of training examples are required to achieve usable performance levels.
    This case study of co-training for natural language learning addresses the scalability question using the task of base noun phrase identification.
    For this task, co-training reduces by 36% the difference in error between classifiers trained on 500 labeled examples and classifiers trained on 211,000 labeled examples.
    While this result is satisfying, further investigation reveals that deterioration in the quality of the labeled data accumulated by co-training hinders further improvement.
    We address this problem with a moderately supervised variant, corrected co-training, that employs a human annotator to correct the errors made during bootstrapping.
    Corrected co-training proves to be quite successful, bridging the remaining gap in accuracy.
    Analysis of corrected co-training illuminates an interesting ten