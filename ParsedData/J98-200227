t about MDL is that theoretical findings have indeed verified that MDL, as an estimation strategy, is near optimal in terms of the rate of convergence of its estimated models to the true model as data size increases.
    When the true model is included in the class of models considered, the models selected by MDL converge to the true model at the rate of 0(k* 2.1s,I ), where k* is the number of parameters in the true model, and IS I the data size, which is near optimal (Barron and Cover 1991; Yamanishi 1992).
    Thus, in the current problem, MDL provides (a) a way of smoothing probability parameters to solve the data sparseness problem, and at the same time, (b) a way of generalizing nouns in the data to noun classes of an appropriate level, both as a corollary to the near optimal estimation of the distribution of the given data.
    There is a Bayesian interpretation of MDL: MDL is essentially equivalent to the &amp;quot;posterior mode&amp;quot; in the Bayesian terminology (Rissanen 1989).
    Given data S 