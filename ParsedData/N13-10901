 vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions.
    Remarkably, this method outperforms the best previous systems.
  
  
    A defining feature of neural network language models is their representation of words as high dimensional real valued vectors.
    In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network.
    As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors.
    Thus, when the model parameters are adjusted in response to