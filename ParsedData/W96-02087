presentation language are preferred (Blumer, Ehrenfeucht, Haussler, St Warmuth, 1987).
    However, the compactness with which different representation languages (e.g. decision trees, DNF, linear threshold networks) can represent particular functions can vary dramatically (e.g. see Pagallo and Haussler (1990)).
    Therefore, different biases can perform better or worse on specific problems.
    One of the main goals of machine learning is to find biases that perform well on the distribution of problems actually found in the real world.
    As an example, consider the advantage neuralnetworks have on the promoter recognition problem mentioned earlier.
    There are several potential sites where hydrogen bonds can form between the DNA and a protein and if enough of these bonds form, promoter activity can occur.
    This is represented most compactly using an M-of-N classification function which returns true if any subset of size M of N specified features are present in an example (Fisher &amp; McKusick, 1989; 