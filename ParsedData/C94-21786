 words are not good translations of one another, as in Table 6, then a should be small, and b and c should be large.
			4.
			Mutual Information.
			Intuitively, these statements seem to be true, but we need to make them more precise.
			One could have chosen quite a number of similarity metrics for this purpose.
			We use mutual information: 1097 prob ( VI, Vp ) log2 prob(Vf) prob(Vp ) That is, we want to compare the probability of seeing fisheries and p~ches in the same piece to chance.
			The probability of seeing the two words in the same piece is simply: a prob(Vf, Vp) - a+b+c+d The marginal probabilities are: a+b prob(Vf ) - a+b+c+d a+c prob(Vp) = a+b+c+d For fisheries --~ p~ches, prob(Vf, Vp) =prob(Vf) =prob(Vp) =0.2.
			Thus, the mutual information is log25 or 2.32 bits, meaning that the joint probability is 5 times more likely than chance.
			In contrast, for fisheries ~ lections, prob ( V f, V p ) = O, prob(Vf) =0.5 and prob(Vp) = 0.4.
			Thus, the mutual information is log 2 0, meaning that the joi