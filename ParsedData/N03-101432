ethods, which can handle unbounded feature sets.
    However, this causes efficiency problems.
    Collins and Duffy (2002) define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins&#8217; previous work on re-ranking using a finite set of features (Collins, 2000).
    Future work could use the induced history representations from our work to define efficiently computable tree kernels.
    The only other broad coverage neural network parser (Costa et al., 2001) also uses a neural network architecture which is specifically designed for processing structures.
    We believe that their poor performance is due to a network design which does not take into consideration the recency bias discussed in section 4.
    Ratnaparkhi&#8217;s parser (1999) can also be considered a form of neural network, but with only a single layer, since it uses a loglinear model 