E101, LDC2007E46, LDC2007E86, and LDC2008E40.7We combine lexicalized reordering models by simply treat ing them as distinct features, which incidentally increases the number of model parameters that must be tuned with MERT.
			853 30.5 31 31.5 32 32.5 33 33.5 34 0 2 4 6 8 10 12 14 BL EU [%], Ch ines e-E ngli sh distortion limit hierarchicalphrase-based word-basedbaseline 43 43.5 44 44.5 45 45.5 0 2 4 6 8 10 BL EU [%], Arabic Eng lish distortion limit hierarchicalphrase-based word-basedbaseline Figure 5: Performance on the Chinese-English andArabic-English development sets (MT06) with increasing distortion limits for all lexicalized reordering mod els discussed in the paper.
			Our novel hierarchical model systematically outperforms all other models for distortion limit equal to or greater than 4.
			The baseline is Moses with no lexicalized reordering model.
			we used the NIST evaluation sets of 2005 and 2008 (MT05 and MT08) for Chinese-English, and the test set of 2005 (MT05) for Arabic-English.
			Statisti