ken feature.
    The second baseline (word+priorpol) uses the word token and prior polarity features.
    In the second set of experiments, we explore the performance of different sets of features for neutral&#8211;polar classification.
    Research has shown that the performance of learning algorithms for NLP tasks can vary widely depending on their parameter settings, and that the optimal parameter settings can also vary depending on the set of features being evaluated (Daelemans et al. 2003a; Hoste 2005).
    Although the goal of this work is not to identify the optimal configuration for each algorithm and each set of features, we still want to make a reasonable attempt to find a good configuration for each algorithm.
    To do this, we perform 10-fold cross validation of the more challenging baseline classifier (word+priorpol) on the development data, varying select parameter settings.
    The results from those experiments are then used to select the parameter settings for each algorithm.
    For BoosTex