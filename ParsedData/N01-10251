med entity extraction.
    Tokenization and part-of-speech tagging can also be regarded as a chunking task, if we assume each character as a token.
    Machine learning techniques are often applied to chunking, since the task is formulated as estimating an identifying function from the information (features) available in the surrounding context.
    Various machine learning approaches have been proposed for chunking (Ramshaw and Marcus, 1995; Tjong Kim Sang, 2000a; Tjong Kim Sang et al., 2000; Tjong Kim Sang, 2000b; Sassano and Utsuro, 2000; van Halteren, 2000).
    Conventional machine learning techniques, such as Hidden Markov Model (HMM) and Maximum Entropy Model (ME), normally require a careful feature selection in order to achieve high accuracy.
    They do not provide a method for automatic selection of given feature sets.
    Usually, heuristics are used for selecting effective features and their combinations.
    New statistical learning techniques such as Support Vector Machines (SVMs) (Cortes and Va