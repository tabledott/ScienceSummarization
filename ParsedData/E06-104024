sures in NLG in the future: (i) performing similar experiments to the one described here in other domains, and with more subjects and larger test sets; (ii) investigating whether automatic corpus-based techniques can evaluate content determination; (iii) investigating how well both human ratings and corpus-based measures correlate with extrinsic evaluations of the effectiveness of generated texts.
    Ultimately, we would like to move beyond critiques of existing corpus-based metrics to proposing (and validating) new metrics which work well for NLG.
  
  
    Corpus quality plays a significant role in automatic evaluation of NLG texts.
    Automatic metrics can be expected to correlate very highly with human judgments only if the reference texts used are of high quality, or rather, can be expected to be judged high quality by the human evaluators.
    This is especially important when the generated texts are of similar quality to human-written texts.
    In MT, high-quality texts vary less than generally in N