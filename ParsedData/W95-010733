e use of chunk structure tags that encode a form of dependency grammar, where the tag &amp;quot;N+2&amp;quot; might mean that the current word is to be taken as part.of the unit headed by the N two words to the right.
  
  
    By representing text chunking as a kind of tagging problem, it becomes possible to easily apply transformation-based learning.
    We have shown that this approach is able to automatically induce a chunking model from supervised training that achieves recall and precision of 92% for baseNP chunks and 88% for partitioning N and V chunks.
    Such chunking models provide a useful and feasible next step in textual interpretation that goes beyond part-of-speech tagging, and that serve as a foundation both for larger-scale grouping and for direct extraction of subunits like index terms.
    In addition, some variations in the transformation-based learning algorithm are suggested by this application that may also be useful in other settings.
  
  
    We would like to thank Eric Brill for ma