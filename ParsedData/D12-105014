  In this way, the same distributional information can be shared across tasks such as word similarity or analogical learning.
    More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w,v and a connecting link-word l. This representation operates over a dependency-parsed corpus and the scores c are obtained via counting the occurrences of tuples, and weighting the raw counts by mutual information.
    Table 1 presents examples of tensor entries.
    These were taken from a distributional memory tensor1 that Baroni and Lenci obtained via preprocessing several corpora: the web-derived ukWac corpus of about 1.915 billion words, a mid-2009 dump of the English Wikipedia containing about 820 million words, and the BNC.
    Extracting a 3-dimensional tensor from the BNC alone would create very sparse representations.
    We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional subspace, from the same tensor Baroni 