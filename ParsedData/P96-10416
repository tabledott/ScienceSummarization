 often each word occurs in the training data.
    For example, we can take and where Lidstone and Jeffreys advocate 6 = 1.
    Gale and Church (1990; 1994) have argued that this method generally performs poorly.
    The Good-Turing estimate (Good, 1953) is central to many smoothing techniques.
    It is not used directly for n-gram smoothing because, like additive smoothing, it does not perform the interpolation of lower- and higher-order models essential for good performance.
    Good-Turing states that an n-gram that occurs r times should be treated as if it had occurred r* times, where and where n,. is the number of n-grams that occur exactly r times in the training data.
    Katz smoothing (1987) extends the intuitions of Good-Turing by adding the interpolation of higherorder models with lower-order models.
    It is perhaps the most widely used smoothing technique in speech recognition.
    Church and Gale (1991) describe a smoothing method that combines the Good-Turing estimate with bucketing, the techn