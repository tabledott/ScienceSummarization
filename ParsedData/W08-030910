nteers, and a small number of paid annotators.
    More than 100 people participated in the manual evaluation, with 75 people putting in more than an hour&#8217;s worth of effort, and 25 putting in more than four hours.
    A collective total of 266 hours of labor was invested.
    We wanted to ensure that we were using our annotators&#8217; time effectively, so we carefully designed the manual evaluation process.
    In our analysis of last year&#8217;s manual evaluation we found that the NISTstyle fluency and adequacy scores (LDC, 2005) were overly time consuming and inconsistent.4 We therefore abandoned this method of evaluating the translations.
    We asked people to evaluate the systems&#8217; output in three different ways: The manual evaluation software asked for repeated judgments from the same individual, and had multiple people judge the same item, and logged the time it took to complete each judgment.
    This allowed us to measure intra- and inter-annotator agreement, and to analyze the average a