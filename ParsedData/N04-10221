elopment of automatic translation evaluation metrics such as BLEU score (Papineni et al., 2001), NIST score (Doddington, 2002) and Position Independent Word Error Rate (PER) (Och, 2002).
    However, given the many factors that influence translation quality, it is unlikely that we will find a single translation metric that will be able to judge all these factors.
    For example, the BLEU, NIST and the PER metrics, though effective, do not take into account explicit syntactic information when measuring translation quality.
    Given that different Machine Translation (MT) evaluation metrics are useful for capturing different aspects of translation quality, it becomes desirable to create MT systems tuned with respect to each individual criterion.
    In contrast, the maximum likelihood techniques that underlie the decision processes of most current MT systems do not take into account these application specific goals.
    We apply the Minimum Bayes-Risk (MBR) techniques developed for automatic speech recognitio