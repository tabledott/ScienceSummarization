g provides a useful alternative to maximum likelihood and minimum error training.
    In our experiments, it never performed significantly worse 11For information on these corpora, see the CoNLL-X shared task on multilingual dependency parsing: http: //nextens.uvt.nl/~conll/. than either and in some cases significantly helped.
    Note, however, that annealed minimum risk training results in a deterministic classifier just as these other training procedures do.
    The orthogonal technique of minimum Bayes risk decoding has achieved gains on parsing (Goodman, 1996) and machine translation (Kumar and Byrne, 2004).
    In speech recognition, researchers have improved decoding by smoothing probability estimates numerically on heldout data in a manner reminiscent of annealing (Goel and Byrne, 2000).
    We are interested in applying our techniques for approximating nonlinear loss functions to MBR by performing the risk minimization inside the dynamic programming or other decoder.
    Another training approach tha