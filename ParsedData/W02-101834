 iterations.
    The first iterations estimate the alignment probabilities using Model 1.
    The rest of the iterations estimate the alignment probabilities using Model 2.
    During training, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus.
    At the end of the training procedure, we take marginals on the joint probability distributionsand .
    This yields conditional probability distributions and which we use for decoding.
    When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e.
    At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build alignments between the smallest phrases possible.
    However, note that the choice made by our model is quite reasonable.
    After all, in the absence of additio