ass.
    Performance is stated in terms of recall (percentage of correct chunks found) and precision (percentage of chunks found that are correct), where both ends of a chunk had to match exactly for it to be counted.
    The raw percentage of correct chunk tags is also given for each run, and for each performance measure, the relative error reduction compared to the baseline is listed.
    The partitioning chunks do appear to be somewhat harder to predict than baseNP chunks.
    The higher error reduction for the former is partly due to the fact that the part-of-speech baseline for that task is much lower.
    To give a sense of the kinds of rules being learned-, the first 10 rules from the 200K baseNP run are shown in Table 5.
    It is worth glossing the rules, since one of the advantages of transformationbased learning is exactly that the resulting model is easily interpretable.
    In the first of the baseNP rules, adjectives (with part-of-speech tag JJ) that are currently tagged I but that are followed 