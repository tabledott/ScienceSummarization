s is more important than sentiment strength.
			For manually tagged holder and topic, Model 0 has the highest single performance, though Model 1 averages best.
			Which is better, a sentence or a region?
			With manually identified topic and holder, the region window4 (from Holder to sentence end) performs better than other regions.
			How do scores differ from manual to automatic holder identification?
			Table 7 compares the average results with automatic holder identification to manually annotated holders in 40 different models.
			Around 7 more sentences (around 11%) were misclassified by the automatic detection method.
			positive negative total Human1 5.394 1.667 7.060 Human2 4.984 1.714 6.698 Table 7: Average difference between manual and automatic holder detection.
			How does adding the neutral sentiment as a separate category affect the score?
			It is very confusing even for humans to distinguish between a neutral opinion and non opinion bearing sentences.
			In previous research, we built a senten