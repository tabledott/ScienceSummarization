t can be reached for a given task is limited, and not merely by the potential of the learning method used.
    Other limiting factors are the power of the hard- and software used to implement the learning method and the availability of training material.
    Because of these limitations, we find that for most tasks we are (at any point in time) faced with a ceiling to the quality that can be reached with any (then) available machine learning system.
    However, the fact that any given system cannot go beyond this ceiling does not mean that machine learning as a whole is similarly limited.
    A potential loophole is that each type of learning method brings its own 'inductive bias' to the task and therefore different methods will tend to produce different errors.
    In this paper, we are concerned with the question whether these differences between models can indeed be exploited to yield a data driven model with superior performance.
    In the machine learning literature this approach is known as ensemble, 