nguage modeling.
    They obtain a standard language model from a 103-million-word corpus and employ Web-based counts to interpolate unreliable trigram estimates.
    They compare their interpolated model against a baseline trigram language model (without interpolation) and show that the interpolated model yields an absolute reduction in word error rate of .93% over the baseline.
    Zhu and Rosenfeld&#8217;s (2001) results demonstrate that the Web can be a source of data for language modeling.
    It is not clear, however, whether their result carries over to tasks that employ linguistically meaningful word sequences (e.g., head-modifier pairs or predicate-argument tuples) rather than simply adjacent words.
    Furthermore, Zhu and Rosenfeld (2001) do not undertake any studies that evaluate Web frequencies directly (i.e., without a task such as language modeling).
    This could be done, for instance, by comparing Web frequencies to corpus frequencies, or to frequencies re-created by smoothing techniques.
  