onstraints play a role at setting the probability estimates for the model's parameters.
    In Ratnaparkhi (1996), a maximum entropy tagger is presented.
    The tagger uses essentially the same parameters as the transformation-based tagger, but employs them in a different model.
    For our experiments, we used a publicly available implementation of maximum-entropy tagging,3 retrained on our training set.
  
  
    All experiments presented in this paper were run on the Penn Treebank Wall Street Journal corpus (Marcus (1993)).
    The corpus was divided into approximately 80% training and 20% testing, giving us approximately 1.1 million words of training data and 265,000 words of test data.
    The test set was not used in any way in training, so the test set does contain unknown words.
    In Figure 1 we show the relative accuracies of the four taggers.
    In parentheses we include tagger accuracy when only ambiguous and unknown words are considered.4 Next, we examine just how different the errors of the t