s to make subtle distinctions among categories.
    The objectivity of these decisions can be assessed by evaluating the reliability of the tagging, namely, whether the coders reach a satisfying level of agreement when they perform the same coding task.
    Currently, the de facto standard for assessing intercoder agreement is the &#954; coefficient, which factors out expected agreement (Cohen 1960; Krippendorff 1980).
    &#954; had long been used in content analysis and medicine (e.g., in psychiatry to assess how well students&#8217; diagnoses on a set of test cases agree with expert answers) (Grove et al. 1981).
    Carletta (1996) deserves the credit for bringing &#954; to the attention of computational linguists. &#954; is computed as P(A) &#8722; P(E) 1 &#8722; P(E) , where P(A) is the observed agreement among the coders, and P(E) is the expected agreement, that is, P(E) represents the probability that the coders agree by chance.
    The values of &#954; are constrained to the interval [&#8722;1,1].
   