ide useful topical information (Ng and Zelle, 1997).
			Several studies in psychology have also shown that global context can help language comprehension (Hess et al., 1995) and acquisition (Li et al, 2000).We introduce a new neural-network-based lan guage model that distinguishes and uses both local and global context via a joint training objective.
			Themodel learns word representations that better capture the semantics of words, while still keeping syn tactic information.
			These improved representations can be used to represent contexts for clustering wordinstances, which is used in the multi-prototype version of our model that accounts for words with mul tiple senses.
			We evaluate our new model on the standard WordSim-353 (Finkelstein et al, 2001) dataset that includes human similarity judgments on pairs of words, showing that combining both local and global context outperforms using only local orglobal context alone, and is competitive with state of-the-art methods.
			However, one limitation of thi