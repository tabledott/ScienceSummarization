f the decision function In basic SVMs framework, we try to separate the positive and negative examples in the training data by a linear hyperplane written as: (w-x)+b= 0 wERn,bert.
    (1) It is -supposed that the farther the positive and negative examples are separated by the discrimination function, the more accurately we could separate unseen test examples with high generalization performance.
    Let us consider two hyperplanes called separating hyperplanes: Distance from the separating hyperplane to the point xi can be written as: In the case where we cannot separate training examples linearly, &amp;quot;Soft Margin&amp;quot; method forgives some classification errors that may be caused by some noise in the training examples.
    First, we introduce non-negative slack variables, and (2),(3) are rewritten as: In this case, we minimize the following value instead of 111w112&#8226; The first term in (7) specifies the size of margin and the second term evaluates how far the training data are away from the op