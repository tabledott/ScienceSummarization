 impact on comparing systems.
    Understanding the exact causes of those differences still remains an important issue for future research.
    This year&#8217;s evaluation also measured the agreement between human assessors by computing the Kappa coefficient.
    One striking observation is that inter-annotator agreement for fluency and adequacy can be called &#8216;fair&#8217; at best.
    On the other hand, comparing systems by ranking them manually (constituents or entire sentences), resulted in much higher inter-annotator agreement.
  
  
    This work was supported in part by the EuroMatrix project funded by the European Commission (6th Framework Programme), and in part by the GALE program of the US Defense Advanced Research Projects Agency, Contract No.
    HR0011-06C-0022.
    We are grateful to Jes&#180;us Gim&#180;enez, Dan Melamed, Maja Popvic, Ding Liu, Liang Zhou, and Abhaya Agarwal for scoring the entries with their automatic evaluation metrics.
    Thanks to Brooke Cowan for parsing the Spanish