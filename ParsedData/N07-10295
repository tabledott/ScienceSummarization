e targeted reference is generated by human posteditors who make edits to a reference translation so as to minimize the TER between the reference and the MT output without changing the meaning of the reference.
    Computing the HTER is very time consuming due to the human post-editing.
    It is desirable to have an automatic evaluation metric that correlates well with the HTER to allow fast evaluation of the MT systems during development.
    Correlations of different evaluation metrics have been studied (Snover et al., 2006) but according to various internal HTER experiments it is not clear whether TER or BLEU correlates better.
    Therefore it is probably safest to try and not degrade either.
    The TER of a translation is computed as where is the total number of words in the reference translation .
    In the case of multiple references, the edits are counted against all references, is the average number of words in the reference translations and the final TER is computed using the minimum number of edi