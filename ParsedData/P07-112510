ments, we use the labeling of the genetics expert, corrected for negligent instances.
  
  
    In this study we use single terms as features, based on the intuition that many hedge cues are single terms (suggest, likely etc.) and due to the success of &#8216;bag of words&#8217; representations in many classification tasks to date.
    Investigating more complex sample representation strategies is an avenue for future research.
    There are a number of factors that make our formulation of hedge classification both interesting and challenging from a weakly supervised learning perspective.
    Firstly, due to the relative sparsity of hedge cues, most samples contain large numbers of irrelevant features.
    This is in contrast to much previous work on weakly supervised learning, where for instance in the case of text categorization (Blum and Mitchell, 1998; Nigam et al., 2000) almost all content terms are to some degree relevant, and irrelevant terms can often be filtered out (e.g. stopword removal).
    In th