 text categorization, generally outperforming Naive Bayes (Joachims, 1998).
    They are large-margin, rather than probabilistic, classifiers, in contrast to Naive Bayes and MaxEnt.
    In the two-category case, the basic idea behind the training procedure is to find a hyperplane, represented by vector &#57740;w, that not only separates the document vectors in one class from those in the other, but for which the separation, or margin, is as large as possible.
    This search corresponds to a constrained optimization problem; letting cj E 11, &#8722;11 (corresponding to positive and negative) be the correct class of document dj, the solution can be written as where the &#57739;j&#8217;s are obtained by solving a dual opti&#57740; mization problem.
    Those dj such that &#57739;j is greater For instance, a particular feature/class function might fire if and only if the bigram &#8220;still hate&#8221; appears and the document&#8217;s sentiment is hypothesized to be negative.7 Importantly, unlike Naive Bayes, Ma