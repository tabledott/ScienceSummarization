8 test sets.
    These conditions included segment, document and system level correlations with human judgments of preference, fluency, adequacy and HTER.
    The test sets included translations from Arabic-to-English, Chinese-toEnglish, Farsi-to-English, Arabic-to-French, and English-to-French MT systems involved in NIST&#8217;s MTEval 2008, the GALE (Olive, 2005) Phase 2 and Phrase 2.5 program, Transtac January and July 2007, and CESTA run 1 and run 2, covering multiple genres.
    The version of TERp submitted to this workshop was optimized as described in Section 3.1.
    The development data upon which TERp was optimized was not part of the test sets evaluated in the Challenge.
    Due to the wealth of testing conditions, a simple overall view of the official MATR08 results released by NIST is difficult.
    To facilitate this analysis, we examined the average rank of each metric across all conditions, where the rank was determined by their Pearson and Spearman correlation with human judgments.
    To in