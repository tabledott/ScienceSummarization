
  A Hierarchical Bayesian Language Model Based On Pitman-Yor Processes
  
    We propose a new hierarchical Bayesian model of natural languages.
    Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages.
    We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothmethods for language models.
    Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney.
  
  
    Probabilistic language models are used extensively in a variety of linguistic applications, including speech recognition, handwriting recognition, optical character recognition, and machine translation.
    Most language models fall into the class of n-gram models, which approximate the distribution over sentences using the cond