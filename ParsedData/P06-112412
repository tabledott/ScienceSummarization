rlier in a context have (a priori) the least importance in modelling the probability of the current word, which is why they are dropped first at successively higher levels of the model.
  
  
    We describe a generative procedure analogous to the Chinese restaurant process of Section 2 for drawing words from the hierarchical PitmanYor language model with all Gu&#8217;s marginalized out.
    This gives us an alternative representation of the hierarchical Pitman-Yor language model that is amenable to efficient inference using Markov chain Monte Carlo sampling and easy computation of the predictive probabilities for test words.
    The correspondence between interpolated KneserNey and the hierarchical Pitman-Yor language model is also apparent in this representation.
    Again we may treat each Gu as a distribution over the current word.
    The basic observation is that since Gu is Pitman-Yor process distributed, we can draw words from it using the Chinese restaurant process given in Section 2.
    Further, th