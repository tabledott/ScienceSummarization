atio (e.g.
    1.8/0.2) is computed.
    However, in the simplest implementation, satisfactory results may be achieved by adding a small constant a to the numerator and denominator, where a is selected empirically to optimize classification performance.
    For this data, relatively small a (between 0.1 and 0.25) tended to be effective, while noisier training data warrant larger a.
    'Entries marked with f are pruned in Step 5, below. classification.
    See Step 7 for a full description of this process.
    Step 5: Optional Pruning and Interpolation A potentially useful optional procedure is the interpolation of log-likelihood ratios between those computed from the full data set (the global probabilities) and those computed from the residual training data left at a given point in the decision list when all higher-ranked patterns failed to match (i.e. the residual probabilities).
    The residual probabilities are more relevant, but since the size of the residual training data shrinks at each level in the l