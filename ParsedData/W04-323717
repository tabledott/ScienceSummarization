increase of training data loglikelihood and accuracies approaching 100%. count cut-off and &#963;2 variance values; log-likelihood and accuracy on adaptation data CNN-trn as well as accuracy on held-out data CNN-dev; the background model results (no new features added) are the entries corresponding to the cut-off threshold of Finally, Table 6 presents the results on test data for 1-gram, background and adapted MEMM.
    As can be seen, the background MEMM outperforms the 1-gram model on both BN test sets by about 35-40% relative.
    Adaptation improves performance even further by another 20-25% relative.
    Overall, the adapted models achieve 60% relative reduction in capitalization error over the 1-gram baseline on both BN test sets.
    An intuitively satisfying result is the fact that the cross-test set performance (CNN adapted model evaluated on ABC data and the other way around) is worse than the adapted one.
  
  
    The MEMM tagger is very effective in reducing both in-domain and out-of-domain capit