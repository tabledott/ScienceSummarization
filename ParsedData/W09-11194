 have considered three datasets: CoNLL03 shared task data, MUC7 data and a set of Webpages we have annotated manually.
    In the experiments throughout the paper, we test the ability of the tagger to adapt to new test domains.
    Throughout this work, we train on the CoNLL03 data and test on the other datasets without retraining.
    The differences in annotation schemes across datasets created evaluation challenges.
    We discuss the datasets and the evaluation methods below.
    The CoNLL03 shared task data is a subset of Reuters 1996 news corpus annotated with 4 entity types: PER,ORG, LOC, MISC.
    It is important to notice that both the training and the development datasets are news feeds from August 1996, while the test set contains news feeds from December 1996.
    The named entities mentioned in the test dataset are considerably different from those that appear in the training or the development set.
    As a result, the test dataset is considerably harder than the development set.
    Evaluation: