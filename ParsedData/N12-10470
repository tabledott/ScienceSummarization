
  Batch Tuning Strategies for Statistical Machine Translation
  
    There has been a proliferation of recent work on SMT tuning algorithms capable of handling larger feature sets than the traditional MERT approach.
    We analyze a number of these algorithms in terms of their sentencelevel loss functions, which motivates several new approaches, including a Structured SVM.
    We perform empirical comparisons of eight different tuning strategies, including MERT, in a variety of settings.
    Among other results, we find that a simple and efficient batch version of MIRA performs at least as well as training online, and consistently outperforms other options.
  
  
    The availability of linear models and discriminative tuning algorithms has been a huge boon to statistical machine translation (SMT), allowing the field to move beyond the constraints of generative noisy channels (Och and Ney, 2002).
    The ability to optimize these models according to an error metric has become a standard assumption in SMT, du