es L(u).
    A subgradient of a convex function L(u) at u is a vector du such that for all v E R|I|, L(v) &gt; L(u) + du &#183; (v &#8722; u).
    By standard results, argmaxy&#8712;Y h(y) + Ei,j u(k)(i, j)y(i, j).
    Subgradient optimization methods are iterative algorithms with updates that are similar to gradient descent: we omit the details, except to note that when the LP relaxation is not tight, the optimal primal solution to the LP relaxation could be recovered by averaging methods (Nedi&#180;c and Ozdaglar, 2009). where &#945;k is a step size.
    It is easily verified that the algorithm in Figure 1 uses precisely these updates.
    With an appropriate choice of the step sizes &#945;k, the subgradient method can be shown to solve the dual problem, i.e.
    See Korte and Vygen (2008), page 120, for details.
    As mentioned before, the dual provides an upper bound on the optimum of the primal problem (Eq.
    4), However, we do not necessarily have strong duality&#8212;i.e., equality in the above equa