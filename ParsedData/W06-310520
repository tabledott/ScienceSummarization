 the learned table translates the apostrophe to the with probability very near 1. phe, the most common french phrase.
    The learned table contains a highly peaked distribution.
    Such common phrases whose translation depends highly on the context are ripe for producing translation errors.
    The flatness of the distribution of OH ensures that the single apostrophe will rarely be used during decoding because no one phrase table entry has high enough probability to promote its use.
    On the other hand, using the peaked entry OEM(the|') incurs virtually no cost to the score of a translation.
    The final kind of errors stems from interactions between the language and translation models.
    The selection among translation choices via a language model &#8211; a key virtue of the noisy channel framework &#8211; is hindered by the determinism of the translation model.
    This effect appears to be less significant than the previous three.
    We should note, however, that adjusting the language and translat