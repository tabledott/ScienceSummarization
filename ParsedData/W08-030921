    The automatic metrics that were evaluated in this year&#8217;s shared task were the following: some of the allowable variation in translation.
    We use a single reference translation in our experiments. words.
    They calculate Bleu (posbleu) and F-measure (pos4gramFmeasure) by matching part of speech 4grams in a hypothesis translation against the reference translation.
    In addition to the above metrics, which scored the translations on both the system-level5 and the sentence-level, there were a number of metrics which focused on the sentence-level: system translations (svm-rank).
    Features included in Duh (2008)&#8217;s training were sentencelevel BLEU scores and intra-set ranks computed from the entire set of translations.
    &#8226; USaar&#8217;s evaluation metric (alignment-prob) uses Giza++ to align outputs of multiple systems with the corresponding reference translations, with a bias towards identical one-to-one alignments through a suitably augmented corpus.
    The Model4 log probabiliti