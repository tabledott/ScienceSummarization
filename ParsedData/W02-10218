he length of the reference translation.
    In our experiments, we varied the word graph pruning threshold in order to obtain word graphs of different densities, i.e. different numbers of hypotheses.
    The word graph density is computed as the total number of word graph edges divided by the number of reference sentence words &#8212; analogously to the word graph density in speech recognition.
    The effect of pruning on the graph error rate is shown in Table 3.
    The value of the pruning threshold is given as the negative logarithm of the probability.
    Thus, t = 0 refers to pruning everything but the best hypothesis.
    Figure 4 shows the change in graph error rate in relation to the average graph density.
    We see that for graph densities up to 200, the graph error rate significantly changes if the graph is enlarged.
    The saturation point of the GER lies at 13% and is reached for an average graph density about 1000 which relates to a pruning threshold of 20.
  
  
    We have presented a concep