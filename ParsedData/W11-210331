w&#8217; tracks.
  
  
    In addition to allowing us to analyze the translation quality of different systems, the data gathered during the manual evaluation is useful for validating automatic evaluation metrics.
    Our evaluation shared task is similar to the MetricsMATR workshop (Metrics for MAchine TRanslation) that NIST runs (Przybocki et al., 2008; Callison-Burch et al., 2010).
    Table 11 lists the participants in this task, along with their metrics.
    A total of 21 metrics and their variants were submitted to the evaluation task by 9 research groups.
    We asked metrics developers to score the outputs of the machine translation systems and system combinations at the system-level and at the segmentlevel.
    The system-level metrics scores are given in the Appendix in Tables 39&#8211;48.
    The main goal of the evaluation shared task is not to score the systems, but instead to validate the use of automatic metrics by measuring how strongly they correlate with human judgments.
    We used the human