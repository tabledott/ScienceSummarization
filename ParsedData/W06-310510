tion for the Pharaoh decoder and uses many publicly available components (Koehn, 2003b).
    The language model was generated from the Europarl corpus using the SRI Language Modeling Toolkit (Stolcke, 2002).
    Pharaoh performed decoding using a set of default parameters for weighting the relative influence of the language, translation and distortion models (Koehn, 2003b).
    A maximum phrase length of three was used for all experiments.
    To properly compare OEM to OH, all aspects of the translation pipeline were held constant except for the parameters of the phrase translation table.
    In particular, we did not tune the decoding hyperparameters for the different phrase tables. pe
  
  
    Having generated OH heuristically and OEM with EM, we now0compare their performance.
    While the model and training regimen for OEM differ from the model fromMarcu and Wong (2002), we achieved tion maximization algorithm for training OEM was initialized with the heuristic parameters OH, so the heuristic curve can 