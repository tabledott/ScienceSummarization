e sequence of (tag,word) pairs as its right children.
  Each Markov process, whose probabilities depend on the word i and its tag, begins in a speciM STAI{T state; the symbols it generates are added as is children, from closest to farthest, until it re~ches the STOP state, qhe process recurses for each child so generated.
  This is a sort of lexicalized context-free model.
  Suppose that the Markov process, when gem crating a child, remembers just the tag of the childs most recently generated sister, if any.
  Then the probability of drawing a given parse fiom the population is (5), where kid(i, c) denotes the cth- closest right child of word i, and where kid(i, O) = START and kid(i, 1 + #,ight-kids(i)) = STOP.
  342 (a) (b) dachshund ovcr  there  can  rea l ly  phty dachshund ow: r  there  can  rea l ly  p lay I,igure 4: Spans ])~uticipa, ting, in the (:orru(:l. i)a, rsc of 7h, at dachs/*und o+wr there c(+u vcalhl ph+g golf~.
  (st) has one pa,rcnt, lcss cndwor(I; its sul)sl)+tn (b) lists two.
  (c &lt; 0 in