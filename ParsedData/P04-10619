n in figure 3, the resulting parser predicted dependencies at below chance level (measured by choosing a random dependency structure).
    This below-random performance seems to be because the model links word pairs which have high mutual information (such as occurrences of congress and bill) regardless of whether they are plausibly syntactically related.
    In practice, high mutual information between words is often stronger between two topically similar nouns than between, say, a preposition and its object.
    One might hope that the problem with this model is that the actual lexical items are too semantically charged to represent workable units of syntactic structure.
    If one were to apply the Paskin (2002) model to dependency structures parameterized simply on the word-classes, the result would be isomorphic to the &#8220;dependency PCFG&#8221; models described in Carroll and Charniak (1992).
    In these models, Carroll and Charniak considered PCFGs with precisely the productions (discussed above) t