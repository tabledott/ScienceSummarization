esults but once in the unweighted results.
    Using the weighted results, which represent testing conditions more realistically than the unweighted results, both judges found an improvement from NLG1 to NLG2, and from NLG2 to NLG3.
    NLG3 cuts the error rate from NLG1 by at least 33% (counting anything without a rank of Correct as wrong).
    NLG2 cuts the error rate by at least 22% and underperforms NLG3, but requires far less annotation in its training data.
    NLG1 has no chance of generating anything for 3% of the data &#8212; it fails completely on novel attribute sets.
    Using the unweighted results, both judges found an improvement from NLG1 to NLG2, but, surprisingly, judge A found a slight decrease while judge B found an increase in accuracy from NLG2 to NLG3.
    The unweighted results show that the baseline NLG1 does well on the common attribute sets, since it correctly generates only less than 50% of the unweighted cases but over 80% of the weighted cases.
  
  
    The NLG2 and NLG3 systems