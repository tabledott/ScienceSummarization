nt features and joint inference over entire sequences.
    CRFs have been previously applied to other tasks such as name entity extraction (McCallum and Li, 2003), table extraction (Pinto et al., 2003) and shallow parsing (Sha and Pereira, 2003).
    The basic theory of CRFs is now well-understood, but the best-practices for applying them to new, real-world data is still in an early-exploration phase.
    Here we explore two key practical issues: (1) regularization, with an empirical study of Gaussian (Chen and Rosenfeld, 2000), exponential (Goodman, 2003), and hyperbolic-Ll (Pinto et al., 2003) priors; (2) exploration of various families of features, including text, lexicons, and layout, as well as proposing a method for the beneficial use of zero-count features without incurring large memory penalties.
    We describe a large collection of experimental results on two traditional benchmark data sets.
    Dramatic improvements are obtained in comparison with previous SVM and HMM based results.
  
  
    Condi