se a statistical bigram language model, with the usual one-word-per-state emission.
    This means that the number of states in each of the name-class states is equal to the vocabulary size, I VI .
    The generation of words and name-classes proceeds in three steps: These three steps are repeated until the entire observed word sequence is generated.
    Using the Viterbi algorithm, we efficiently search the entire space of all possible name-class assignments, maximizing the numerator of Equation 2.2, Pr(W, NC).
    Informally, the construction of the model in this manner indicates that we view each type of &amp;quot;name&amp;quot; to be its own language, with separate bigram probabilities for generating its words.
    While the number of word-states within each name-class is equal to I VI , this &amp;quot;interior&amp;quot; bigram language model is ergodic, i.e., there is a probability associated with every one of the 1V12transitions.
    As a parameterized, trained model, if such a transition were never obs