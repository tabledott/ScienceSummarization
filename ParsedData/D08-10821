s have developed techniques for learning to map sentences to hierarchical representations of their underlying meaning (Wong and Mooney, 2006; Kate and Mooney, 2006).
    One common approach is to learn some form of probabilistic grammar which includes a list of lexical items that models the meanings of input words and also includes rules for combining lexical meanings to analyze complete sentences.
    This approach performs well but is constrained by the use of a single, learned grammar that contains a fixed set of lexical entries and productions.
    In practice, such a grammar may lack the rules required to correctly parse some of the new test examples.
    In this paper, we develop an alternative approach that learns a model which does not make use of an explicit grammar but, instead, models the correspondence between sentences and their meanings with a generative process.
    This model is defined over hybrid trees whose nodes include both natural language words and meaning representation tokens.
    Ins