 5, and 6 summarize the results of the human evaluation of the quality of the machine translation systems.
    Table 4 gives the results for the manual evaluation which ranked the translations of sentences.
    It shows the average number of times that systems were judged to be better than or equal to any other system.
    Table 5 similarly summarizes the results for the manual evaluation which ranked the translations of syntactic constituents.
    Table 6 shows how many times on average a system&#8217;s translated constituents were judged to be acceptable in the Yes/No evaluation.
    The bolded items indicate the system that performed the best for each task under that particular evaluate metric.
    Table 7 summaries the results for the All-English task that we introduced this year.
    Appendix C gives an extremely detailed pairwise comparison between each of the systems, along with an indication of whether the differences are statistically significant.
    The highest ranking entry for the All-English tas