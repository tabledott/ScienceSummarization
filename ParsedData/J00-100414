the source and target positions, as before.
    We also need parameters P(q0, 911w v) for the probability of choosing a head transition given this pair of headwords.
    To start the derivation, we need parameters P(roots(wo, vo)) for the probability of choosing wo,vo as the root nodes of the two trees.
    These model parameters can be used to generate pairs of synchronized dependency trees starting with the topmost nodes of the two trees and proceeding recursively to the leaves.
    The probability of such a derivation can be expressed as: for a derivation in which the dependents of w and v are generated by n transitions.
    To carry out translation with a dependency transduction model, we apply a dynamic programming search to find the optimal derivation.
    This algorithm can take as input either word strings, or word lattices produced by a speech recognizer.
    The algorithm is similar to those for context-free parsing such as chart parsing (Earley 1970) and the CKY algorithm (Younger 1967).
    Since 