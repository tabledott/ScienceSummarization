ior weak learners, i,e.
    : A weak learner is built at each iteration k with different distributions or weights d(k) = (d(k) The weights are calculated in such a way that hard examples are focused on more than easier examples.
    To use the decision stumps as the weak learner of Boosting, we redefine the gain function (2) as follows: There exist many Boosting algorithm variants, however, the original and the best known algorithm is AdaBoost (Freund and Schapire, 1996).
    We here use Arc-GV (Breiman, 1999) instead of AdaBoost, since Arc-GV asymptotically maximizes the margin and shows faster convergence to the optimal solution than AdaBoost.
  
  
    In this section, we introduce an efficient and practical algorithm to find the optimal rule (&#710;t, &#710;y) from given training data.
    This problem is formally defined as follows.
    Problem 1 Find Optimal Rule Let T = {(x1, y1, d1), ..., (xL, yL, dL)} be training data, where, xi is a labeled ordered tree, yi E {+1} is a class label associated with xi