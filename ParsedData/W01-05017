unlabeled data, such as EM (Dempster et al., 1977; Nigam et al., 2000), may still prove effective even when an explicit feature split is unknown, provided that there is enough implicit redundancy in the data.
    In contrast to previous investigations of the theoretical basis of co-training, this study is motivated by practical concerns about the application of weakly supervised learning to problems in natural language learning (NLL).
    Many NLL tasks contrast in two ways with the web page classification task studied in previous work on co-training.
    First, the web page task factors naturally into page and link views, while other NLL tasks may not have such natural views.
    Second, many NLL problems require hundreds of thousands of training examples, while the web page task can be learned using hundreds of examples.
    Consequently, our focus on natural language learning introduces new questions about the scalability of the co-training paradigm.
    First, can co-training be applied to learning proble