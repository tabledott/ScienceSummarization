ry systems.
    Table 2 lists the six participants in the system combination task.
  
  
    As with past workshops, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores.
    It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality.
    Therefore, we define the manual evaluation to be primary, and use the human judgments to validate automatic metrics.
    Manual evaluation is time consuming, and it requires a large effort to conduct it on the scale of our workshop.
    We distributed the workload across a number of people, including shared-task participants, interested volunteers, and a small number of paid annotators.
    More than 160 people participated in the manual evaluation, with 100 people putting in more than an hour&#8217;s worth of effort, and 30 putting in more than four hours.
    A collective total of 479 hours of labor was invested.
    We asked people to evaluate the systems&#8217; output i