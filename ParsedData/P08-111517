all three segmentations (hs+ss+cs).
    Data and Settings.
    The systems used in these experiments were trained on the NIST MT06 Eval corpus without the UN data (approximatively 950K sentences).
    The corpus was analyzed with the three segmentation schemes.
    For the systems using word lattices, the training data contained the versions of the corpus appropriate for the segmentation schemes used in the input.
    That is, for the hs+ss condition, the training data consisted of two copies of the corpus: one segmented with the Harbin segmenter and the other with the Stanford segmenter.3 A trigram English language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) was trained on the English side of our training data as well as portions of the Gigaword v2 English Corpus, and was used for all experiments.
    The NIST MT03 test set was used as a development set for optimizing the interpolation weights using minimum error rate training (Och, 2003).
    The testing was done on the NIST 2005 and 200