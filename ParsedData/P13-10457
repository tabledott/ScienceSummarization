ting representations.
    Words were represented as one-on vectors, which was feasible since the grammar only included a handful of words.
    Collobert and Weston (2008) showed that neural networks can perform well on sequence labeling language processing tasks while also learning appropriate features.
    However, their model is lacking in that it cannot represent the recursive structure inherent in natural language.
    They partially circumvent this problem by using either independent window-based classifiers or a convolutional layer.
    RNN-specific training was introduced by Goller and K&#168;uchler (1996) to learn distributed representations of given, structured objects such as logical terms.
    In contrast, our model both predicts the structure and its representation.
    Henderson (2003) was the first to show that neural networks can be successfully used for large scale parsing.
    He introduced a left-corner parser to estimate the probabilities of parsing decisions conditioned on the parsing hist