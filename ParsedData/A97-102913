f (1 &#8212; A) to the back-off model, where (3.8) where &amp;quot;old c(Y)&amp;quot; is the sample size of the model from which we are backing off.
    This is a rather simple method of smoothing, which tends to work well when there are only three or four levels of back-off.4 This method also overcomes the problem when a back-off model has roughly the same amount of training as the current model, via the first factor of Equation 3.8, which essentially ignores the back-off model and puts all the weight on the primary model, in such an equi-trained situation.
    As an example&#8212;disregarding the first factor&#8212;if we saw the bigram &amp;quot;come hither&amp;quot; once in training and we saw &amp;quot;come here&amp;quot; three times, and nowhere else did we see the word &amp;quot;come&amp;quot; in the NOT-A-NAME class, when computing Pr(&amp;quot;hither&amp;quot; I &amp;quot;come&amp;quot;, NOT-A-NAME), we would back off to the unigram probability Pr(&amp;quot;hither&amp;quot; I NOT-A-NAME) with a weight