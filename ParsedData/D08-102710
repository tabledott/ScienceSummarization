ons, averaging Pearson correlation across all sets of the five expert labelers (&#8220;NE vs. E&#8221;).
    We then calculate the ITA for each expert vs. the averaged labels from all other experts and non-experts (marked as &#8220;E vs. All&#8221;) and for each non-expert vs. the pool of other non-experts and all experts (&#8220;NE vs. All&#8221;).
    We compute these ITA scores for each emotion task separately, averaging the six emotion tasks as &#8220;Avg.
    Emo&#8221; and the average of all tasks as &#8220;Avg.
    All&#8221;.
    The results in Table 1 conform to the expectation that experts are better labelers: experts agree with experts more than non-experts agree with experts, although the ITAs are in many cases quite close.
    But we also found that adding non-experts to the gold standard (&#8220;E vs. All&#8221;) improves agreement, suggesting that non-expert annotations are good enough to increase the overall quality of the gold labels.
    Our first comparison showed that individual experts we