ificant with respect to the other models, including the &#8220;No LP&#8221; setting.11 Overall, it performs 10.4% better than the hitherto state-of-the-art feature-HMM baseline, and 4.6% better than direct projection, when we macro-average the accuracy over all languages.
    Our full model outperforms the &#8220;No LP&#8221; setting because it has better vocabulary coverage and allows the extraction of a larger set of constraint features.
    We tabulate this increase in Table 3.
    For all languages, the vocabulary sizes increase by several thousand words.
    Although the tag distributions of the foreign words (Eq.
    6) are noisy, the results confirm that label propagation within the foreign language part of the graph adds significant quality for every language.
    Figure 2 shows an excerpt of a sentence from the Italian test set and the tags assigned by four different models, as well as the gold tags.
    While the first three models get three to four tags wrong, our best model gets only one word wron