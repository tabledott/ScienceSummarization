ms without natural factorizations into views?
    Nigam and Ghani's study suggests a qualified affirmative answer to this question, for a text classification task designed to contain redundant information; however, it is desirable to continue investigation of the issue for large-scale NLL tasks.
    Second, how does co-training scale when a large number of training examples are required to achieve usable performance levels?
    It is plausible to expect that the CT algorithm will not scale well, due to mistakes made by the view classifiers.
    To elaborate, the view classifiers may occasionally add incorrectly labeled instances to the labeled data.
    If many iterations of CT are required for learning the task, degradation in the quality of the labeled data may become a problem, in turn affecting the quality of subsequent view classifiers.
    For large-scale learning tasks, the effectiveness of co-training may be dulled over time.
    Finally, we note that the accuracy of automatically accumulated training