 search, we rank all fea- tures in 7 9 by estimating their potential contribution to the log-likelihood of the training set.
  Let q be the conditional probability distribution of the model with the features cur- rently in A,4.
  Then for each f~ 6 79, we compute, by estimat- ing only ~,  the probability distribution p that results when fi is added to the ME model: p(dlh) = q(dlh)e~J,(h, d) 1 E q(wlh) e~J(h?)
  =0 We then compute the increase in (log) likelihood with the new model: 6L, = ~IS(h,  w)lnp(wlh ) - ~e~(h, w)lnq(wlh ) h,w h,w and choose the feature with the highest 6L.
  Features redun- dmlt or correlated to those features already in .A.4 will produce 252 1 0.9 0.~ 0.7 0.6 0.5 ENTROPY: Wall St. Journal Training 0.4 20 A dO dO .
  100 120 140 160 180 200 Figure 2: Entropy of Maximum Entropy Model on Wall St. Journal Data a zero or negligible 6L, and will therefore be outranked by genuinely informative features.
  The chosen feature is added to M and used in the ME Model.
  Growth of Putative Feature 