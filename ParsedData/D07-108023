tures.
			The setting severely overfit to the development data, and therefore resulted in worse results in open tests.
			The word class3 with surfaceform avoided the overfitting problem.
			The digit sequence normalization provides a similar generaliza tion capability despite of the moderate increase in the active feature size.
			By including all token types, we achieved better NIST/BLEU scores for the 2004and 2005 test sets.
			This set of experiments indi cates that a token normalization is useful especially trained on a small data.
			Second, we used all the normalized token types,but incrementally added structural features in Ta ble 2.
			Target bigram features account for only the fluency of the target side without considering thesource/target correspondence.
			Therefore, the in 2We used the tool available at http://www.nist.gov/ speech/tests/mt/ 3We induced 50 classes each for English and Arabic.
			clusion of target bigram features clearly overfit to the development data.
			The problem is resolved 