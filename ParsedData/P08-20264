ticular, it was able to achieve an f-score of 87% on Brown treebank test data when trained and selftrained on WSJ-like data.
    Note this last point.
    It was not the case that it used the self-training to bridge the corpora difference.
    It self-trained on NANC, not Brown.
    NANC is a news corpus, quite like WSJ data.
    Thus the point of that paper was that self-training a WSJ parser on similar data makes the parser more flexible, not better adapted to the target domain in particular.
    It said nothing about the task we address here.
    Thus our claim is that previous results are quite ambiguous on the issue of bridging corpora for parser adaptation.
    Turning briefly to previous results on Medline data, the best comparative study of parsers is that of Clegg and Shepherd (2005), which evaluates several statistical parsers.
    Their best result was an f-score of 80.2%.
    This was on the Lease/Charniak (L/C) parser (Lease and Charniak, 2005).2 A close second (1% behind) was 'This is not a crit