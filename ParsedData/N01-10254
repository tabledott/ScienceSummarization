which one can move the separating hyperplane without any misclassification.
    We call the distance between those parallel dashed lines as margin.
    SVMs find the separating hyperplane which maximizes its margin.
    Precisely, two dashed lines and margin ( ) can be expressed as: .
    To maximize this margin, we should minimize .
    In other words, this problem becomes equivalent to solving the following optimization problem: The training samples which lie on either of two dashed lines are called support vectors.
    It is known that only the support vectors in given training data matter.
    This implies that we can obtain the same decision function even if we remove all training samples except for the extracted support vectors.
    In practice, even in the case where we cannot separate training data linearly because of some noise in the training data, etc, we can build the separating linear hyperplane by allowing some misclassifications.
    Though we omit the details here, we can build an optimal hype