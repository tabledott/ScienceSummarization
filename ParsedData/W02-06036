the segmentation of the corpus2.
    Also de Marcken (1995; 1996) studies the problem of learning a lexicon, but instead of optimizing the cost of the whole corpus, as in (Redlich, 1993; Hua, 2000), de Marcken starts with sentences.
    Spaces are included as any other characters.
    Utterances are also analyzed in (Kit and Wilks, 1999) where optimal segmentation for an utterance is sought so that the compression effect over the segments is maximal.
    The compression effect is measured in what the authors call Description Length Gain, defined as the relative reduction in entropy.
    The Viterbi algorithm is used for searching for the optimal segmentation given a model.
    The input utterances include spaces and punctuation as ordinary characters.
    The method is evaluated in terms of precision and recall on word boundary prediction.
    Brent presents a general, modular probabilistic model structure for word discovery (Brent, 1999).
    He uses a minimum representation length criterion for model optimi