 phrase.
    This work is just a first step toward preferencebased parsing, an empirically motivated alternative to traditional rational approaches such as ATNs, unification parsers, and principle-based parsers.
  
  
    How do we decide if one language model is better than another?
    In the 1940s, Shannon defined entropy, a measure of the information content of a probabilistic source, and used it to quantify such concepts as noise, redundancy, the capacity of a communication channel (e.g., a telephone), and the efficiency of a code.
    The standard unit of entropy is the bit or binary digit.
    See Bell, Cleary, and Witten (1990) for a more discussion on entropy; Section 2.2.5 shows how to compute the entropy of a model, and Section 4 discusses how Shannon and others have estimated the entropy of English.
    From the point of view of speech recognition or OCR, we would like to be able to characterize the size of the search space, the number of binary questions that the recognizer will have to answer on