e uncertainty about the class.
			It is the difference between the entropy of the empirical distribution of the class variable in the training set and the weighted average entropy 778 2:N.Reg p=0.571 p=0.938 p=0.999 0:N.Name 1:ART.Nom 0:N.Name 0:N.Name p=0.948 p=0.998 ....
			1:ADJA.Nom yes yes no noyes no yes no no yesFigure 1: Probability estimation tree for the nomi native case of nouns.
			The test 1:ART.Nom checks if the preceding word is a nominative article.
			assigned to the top node.
			The tree is recursivelyexpanded by selecting the best test for each sub set and so on, until all objects of the current subsetbelong to the same class.
			In a second step, the decision tree may be pruned in order to avoid overfit ting to the training data.
			Our tagger generates a predictor for each feature (such as base POS, number, gender etc.) Instead of using a single tree for the prediction of all possible values of a feature (such as noun, article, etc. for base POS), the tagger builds a separate decision tre