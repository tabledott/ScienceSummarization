different numbers of categories, they are not comparable.
    On the other hand, Passonneau and Litman note that their figures are not properly interpretable and attempt to overcome this failing to some extent by showing that the agreement which they have obtained at least significantly differs from random agreement.
    Their method for showing this is complex and of no concern to us here, since all it tells us is that it is safe to assume that the coders were not coding randomly&#8212;reassuring, but no guarantee of reliability.
    It is more important to ask how different the results are from random and whether or not the data produced by coding is too noisy to use for the purpose for which it was collected.
  
  
    The concerns of these researchers are largely the same as those in the field of content analysis (see especially Krippendorff [1980] and Weber [1985]), which has been through the same problems as we are currently facing and in which strong arguments have been made for using the kappa coeffic