 Gerdemann, 2001) promises to unify much of statistical NLP.
    Such tools make it easy to run most current approaches to statistical markup, chunking, normalization, segmentation, alignment, and noisy-channel decoding,' including classic models for speech recognition (Pereira and Riley, 1997) and machine translation (Knight and Al-Onaizan, 1998).
    Moreover, once the models are expressed in the finitestate framework, it is easy to use operators to tweak them, to apply them to speech lattices or other sets, and to combine them with linguistic resources.
    Unfortunately, there is a stumbling block: Where do the weights come from?
    After all, statistical models require supervised or unsupervised training.
    Currently, finite-state practitioners derive weights using exogenous training methods, then patch them onto transducer arcs.
    Not only do these methods require additional programming outside the toolkit, but they are limited to particular kinds of models and training regimens.
    For example, t