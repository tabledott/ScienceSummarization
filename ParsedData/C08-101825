ing of variables as numbers/non-terminals non-terminal sequence of vars identical after reordering pre-terminal or terminal sequences are identical number/identity of common/inserted/dropped terminals source is shorter/longer than target target is a compression of the source using deletes For every ngram : log p(w i |w i?1 i?(n?1) ) Table 3: The feature set.
			Rules were drawn from the training set, bilingual pivoting and directly from the source trees.
			s andt are the source and target elementary trees in a rule, the sub script R references the root non-terminal, w are the terminals in the target tree.
			features (196,419 in total).
			These are summarised in Table 3.
			We also use a trigram language model trained on the BNC (100 million words) using the SRI Language Modeling toolkit (Stolcke, 2002), with modified Kneser-Ney smoothing.An important parameter in our modeling frame work is the choice of loss function.
			We evaluatedthe loss functions presented in Section 4 on the de velopment set.
			We r