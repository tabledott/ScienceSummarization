
    In each round of self-training or naive co-training 10% of the cache was randomly selected and added to the labelled training data.
    The experiments ran for 40 rounds.
    The performance of the different training regimes is listed in Table 5.
    These results show no significant improvement using either self-training or co-training with very large seed datasets.
    Self-training shows only a slight improvement for C&amp;C1 while naive co-training performance is always worse.
  
  
    We have shown that co-training is an effective technique for bootstrapping POS taggers trained on small amounts of labelled data.
    Using unlabelled data, we are able to improve TNT from 81.3% to 86.0%, whilst C&amp;C shows a much more dramatic improvement of 73.2% to 85.9%.
    Our agreement-based co-training results support the theoretical arguments of Abney (2002) and Dasgupta et al. (2002), that directly maximising the agreement rates between the two taggers reduces generalisation error.
    Examination of the s