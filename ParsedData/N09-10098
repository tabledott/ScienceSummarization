n and Manning (2004) learned the DMV probabilities 0 from a corpus of part-of-speech-tagged sentences using the EM algorithm.
    EM manipulates 0 to locally optimize the likelihood of the observed portion of the data (here, x), marginalizing out the hidden portions (here, y).
    The likelihood surface is not globally concave, so EM only locally optimizes the surface.
    Klein and Manning&#8217;s initialization, though reasonable and language-independent, was an important factor in performance.
    Various alternatives to EM were explored by Smith (2006), achieving substantially more accurate parsing models by altering the objective function.
    Smith&#8217;s methods did require substantial hyperparameter tuning, and the best results were obtained using small annotated development sets to choose hyperparameters.
    In this paper, we consider only fully unsupervised methods, though we the Bayesian ideas explored here might be merged with the biasing approaches of Smith (2006) for further benefit.
  
  
   