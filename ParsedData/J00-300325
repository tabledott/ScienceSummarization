 such as The Viterbi algorithm for HMMs (Viterbi 1967) finds the globally most probable state sequence.
    When applied to a discourse model with locally decomposable likelihoods and Markovian discourse grammar, it will therefore find precisely the DA Stolcke et al. Dialogue Act Modeling sequence with the highest posterior probability: The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, and Mercer 1983) and tagging (Church 1988).
    It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Dermatas and Kokkinakis 1995).
    To minimize the total number of utterance labeling errors, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P(U11E) for each i = 1,. .
    .
    ,n. We can compute the per-utterance posterior DA probabi