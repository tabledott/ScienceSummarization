ives an indication of how understandable the MT output is.
    Appendix A shows some sample questions and answers for an article.
  
  
    Mechanical Turk is an inexpensive way of gathering human judgments and annotations for a wide variety of tasks.
    In this paper we demonstrate that it is feasible to perform manual evaluations of machine translation quality using the web service.
    The low cost of the non-expert labor found on Mechanical Turk is cheap enough to collect redundant annotations, which can be utilized to ensure translation quality.
    By combining the judgments of many non-experts we are able to achieve the equivalent quality of experts.
    The suggests that manual evaluation of translation quality could be straightforwardly done to validate performance improvements reported in conference papers, or even for mundane tasks like tracking incremental system updates.
    This challenges the conventional wisdom which has long held that automatic metrics must be used since manual evaluation is