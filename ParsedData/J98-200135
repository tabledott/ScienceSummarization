ge agreement is calculated.
    The last row in the table shows the total number of descriptions (N), the total number of descriptions assigned to each class and, finally, the total percentage agreement for all descriptions (Z).
    The equations for computing Si, PE, PA, and K are shown in Table 6.
    In these formulas, c is the number of coders; S, the percentage agreement for description i (we show S1 and S2 as examples); m the number of categories; T the total number of classification judgments; PE the percentage agreement expected by chance; PA the total agreement; and K the Kappa coefficient.
    3.3.3 Value of K for the First Experiment.
    For the first experiment, K = 0.68 if we count idioms as a class, K = 0.73 if we take them out.
    The overall coefficient of agreement between the two annotators and our own analysis is K = 0.68 if we count idioms, K = 0.72 if we ignore them.
    3.3.4 Per-Class Agreement.
    K gives a global measure of agreement.
    We also wanted to measure the agreement per