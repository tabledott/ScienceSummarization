lar convolution is an instance of the general multiplicative model which breaks this constraint by allowing uj to contribute to pi: For example, according to (5), the addition of the two vectors representing horse and run in Figure 1 would yield horse + run = [1 14 6 14 4].
    Whereas their product, as given by (6), is horse &#183; run = [0 48 8 40 0].
    Although the composition model in (5) is commonly used in the literature, from a linguistic perspective, the model in (6) is more appealing.
    Simply adding the vectors u and v lumps their contents together rather than allowing the content of one vector to pick out the relevant content of the other.
    Instead, it could be argued that the contribution of the ith component of u should be scaled according to its relevance to v, and vice versa.
    In effect, this is what model (6) achieves.
    As a result of the assumption of symmetry, both these models are &#8216;bag of words&#8217; models and word order insensitive.
    Relaxing the assumption of symme