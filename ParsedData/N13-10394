he processing of millions to billions of messages&#8212;so we make greedy decoding the default in the released software.
    2Although when compared to CRFs, MEMMs theoretically suffer from the &#8220;label bias&#8221; problem (Lafferty et al., 2001), our system substantially outperforms the CRF-based taggers of previous work; and when comparing to Gimpel et al. system with similar feature sets, we observed little difference in accuracy.
    This is consistent with conventional wisdom that the quality of lexical features is much more important than the parametric form of the sequence model, at least in our setting: part-ofspeech tagging with a small labeled training set.
    This greedy tagger runs at 800 tweets/sec.
    (10,000 tokens/sec.) on a single CPU core, about 40 times faster than Gimpel et al.&#8217;s system.
    The tokenizer by itself (&#167;4) runs at 3,500 tweets/sec.3 Training and regularization.
    During training, the MEMM log-likelihood for a tagged tweet (x, y) is the sum over the observed