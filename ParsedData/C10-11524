mple sentence is con ducted by applying a sequence of simplification operations.
			An expectation maximization (EM) algorithm is used to iteratively train our model.
			We also propose a method based on monolingualword mapping which speeds up the training pro cess significantly.
			Finally, a decoder is designed to generate the simplified sentences using a greedy strategy and integrates language models.In order to train our model, we further com pile a large-scale complex-simple parallel dataset(PWKP) from Simple English Wikipedia1 and En glish Wikipedia2, as such datasets are rare.We organize the remainder of the paper as follows: Section 2 describes the PWKP dataset.
			Sec tion 3 presents our TSM model.
			Sections 4 and 5 are devoted to training and decoding, respectively.
			Section 6 details the evaluation.
			The conclusions follow in the final section.
	
	
			We collected a paired dataset from the English Wikipedia and Simple English Wikipedia.
			The targeted audience of Simple Wikipedia includes?ch