y arbitrary questions about the input sequence, including queries about previous words, next words, and conjunctions of all these, and fk(&#183;) can range &#8722;oo...oo.
    CRFs define the conditional probability of a label sequence based on total probability over the state sequences, PA(l|o) = Ps:l(s)=l PA(s|o), where l(s) is the sequence of labels corresponding to the labels of the states in sequences.
    Note that the normalization factor, Zo, is the sum of the &#8220;scores&#8221; of all possible state sequences, Zo = the number of state sequences is exponential in the input sequence length, T. In arbitrarily-structured CRFs, calculating the normalization factor in closed form is intractable, but in linear-chain-structured CRFs, as in forward-backward for hidden Markov models (HMMs), the probability that a particular transition was taken between two CRF states at a particular position in the input sequence can be calculated efficiently by dynamic programming.
    We define slightly modified forward va