
  Wide-Coverage Efficient Statistical Parsing with CCG and Log-Linear Models
  
    This article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar.
    The models are &#8220;full&#8221; parsing models in the sense that probabilities are defined for complete parses, rather than for independent events derived by decomposing the parse tree.
    Discriminative training is used to estimate the models, which requires incorrect parses for each sentence in the training data as well as the correct parse.
    The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank.
    The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement (up to 25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm running on a Beowulf cluster.
    Dynamic programming over a packed