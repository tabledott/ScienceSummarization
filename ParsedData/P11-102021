erage, with a minimum of 10 and a maximum of 31.)
    We also included the source sentence as a reference for itself.
    Overall, all the trained models produce reasonable paraphrase systems, even the model trained on just 28K single parallel sentences.
    Examples of the outputs produced by the models trained on single parallel sentences and on all parallel sentences are shown in Table 2.
    Some of the changes are simple word substitutions, e.g. rabbit for bunny or gun for revolver, while others are phrasal, e.g. frying meat for browning pork or made a basket for scores in a basketball game.
    One interesting result of using videos as the stimulus to collect training data is that sometimes the learned paraphrases are not based on linguistic closeness, but rather on visual similarity, e.g. substituting cricket for baseball.
    To evaluate the results quantitatively, we used the BLEU/PINC metric.
    The performance of all the trained models is shown in Figure 3.
    Unsurprisingly, there is a tradeoff 