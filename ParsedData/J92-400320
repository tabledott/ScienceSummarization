utual information is least.
    At the kth step of the algorithm, we assign the (C + k)th most probable word to a new class.
    This restores the number of classes to C + 1, and we again merge that pair for which the loss in average mutual information is least.
    After V &#8212; C steps, each of the words in the vocabulary will have been assigned to one of C classes.
    We have used this algorithm to divide the 260,741-word vocabulary of Table 1 into 1,000 classes.
    Table 2 contains examples of classes that we find particularly interesting.
    Table 3 contains examples that were selected at random.
    Each of the lines in the tables contains members of a different class.
    The average class has 260 words and so to make the table manageable, we include only words that occur at least ten times and we include no more than the ten most frequent words of any class (the other two months would appear with the class of months if we extended this limit to twelve).
    The degree to which the classes capture