sh LDA model have zero tokens assigned to them, and almost all tokens are assigned to the largest 200 topics.
    English has a larger tail, with non-zero counts in all but 16 topics.
    In contrast, PLTM assigns a significant number of tokens to almost all 800 topics, in very similar proportions in both languages.
    PLTM topics therefore have a higher granularity &#8211; i.e., they are more specific.
    This result is important: informally, we have found that increasing the granularity of topics correlates strongly with user perceptions of the utility of a topic model.
    An important application for polylingual topic modeling is to use small numbers of comparable document tuples to link topics in larger collections of distinct, non-comparable documents in multiple languages.
    For example, a journal might publish papers in English, French, German and Italian.
    No paper is exactly comparable to any other paper, but they are all roughly topically similar.
    If we wish to perform topic-based biblio