itional distribution of each word given a context consisting of only the previous n &#8722; 1 words, with n = 3 (trigram models) being typical.
    Even for such a modest value of n the number of parameters is still tremendous due to the large vocabulary size.
    As a result direct maximum-likelihood parameter fitting severely overfits to the training data, and smoothing methods are indispensible for proper training of n-gram models.
    A large number of smoothing methods have been proposed in the literature (see (Chen and Goodman, 1998; Goodman, 2001; Rosenfeld, 2000) for good overviews).
    Most methods take a rather ad hoc approach, where n-gram probabilities for various values of n are combined together, using either interpolation or back-off schemes.
    Though some of these methods are intuitively appealing, the main justification has always been empirical&#8212;better perplexities or error rates on test data.
    Though arguably this should be the only real justification, it only answers the questio