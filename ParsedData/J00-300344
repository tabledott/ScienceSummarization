xes of the input feature space, unlike standard CART trees, which always split continuous features on one dimension at a time.
    The response function of neural networks is continuous (smooth) at the decision boundaries, allowing them to avoid hard decisions and the complete fragmentation of data associated with decision tree questions.
    Most important, however, related work (Ries 1999a) indicated that similarly structured networks are superior classifiers if the input features are words and are therefore a plug-in replacement for the language model classifiers described in this paper.
    Neural networks are therefore a good candidate for a jointly optimized classifier of prosodic and word-level information since one can show that they are a generalization of the integration approach used here.
    We tested various neural network models on the same six-class downsampled data used for decision tree training, using a variety of network architectures and output layer functions.
    The results are summari