e i and H(C) is the entropy of the class labels.
    Probabilities are estimated from frequency of occurrence in the training data.
    The numerator of this formula determines the knowledge about the distribution of classes that is added by knowing the value of feature i.
    However, this measure can overestimate the value of features with large numbers of possible values.
    To compensate, it is divided by H(v), the entropy of the feature values.
    Word senses are presented to TiMBL in a feature-vector representation, with each sense which was not removed by the part of speech filter being represented by a separate vector.
    The vectors are formed from the following pieces of information in order: headword, homograph number, sense number, rank of sense (the order of the sense in the lexicon), part of speech from lexicon, output from the three partial taggers (simulated annealing, subject codes, and selectional restrictions), surface form of headword from the text, the ten collocates, and an indicator 