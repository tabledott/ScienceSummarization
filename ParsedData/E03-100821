istic methods for choosing which newly parsed sentences to add to the training data can be beneficial.
    We saw that co-training outperformed self-training, that it was most beneficial when the seed set was small, and that co-training was possible even when the seed material was from another distribution to both the unlabelled material or the testing set.
    This final result is significant as it bears upon the general problem of having to build models when little or no labelled training material is available for some new domain.
    Co-training performance may improve if we consider co-training using sub-parses.
    This is because a parse tree is really a large collection of individual decisions, and retraining upon an entire tree means committing to all such decisions.
    Our ongoing work is addressing this point, largely in terms of re-ranked parsers.
    Finally, future work will also track comparative performance between the LTAG and Collins-CFG models.
  
  
    This work has been supported, in par