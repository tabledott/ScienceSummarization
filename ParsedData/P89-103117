ificant differences in performance for different textual types.
    Both of the algorithms performed significantly worse on the task dialogues (x2 = 22.05 for Hobbs, x2 = 21.55 for BFP, p &lt; 0.05).
    We might wonder with what confidence we should view these numbers.
    A significant factor that must be considered is the contribution of FALSE POSITIVES and ERROR CHAINING.
    A FALSE POSITIVE is when an algorithm gets the right answer for the wrong reason.
    A very simple example of this phenomena is illustrated by this sequence from one of the task dialogues.
    The first it in Expi refers to the pump.
    Hobbs algorithm gets the right antecedent for it in Exp3, which is the little handle, but then fails on it in Exp4, whereas the BFP algorithm has the pump centered at Expi and continues to select that as the antecedent for it throughout the text.
    This means BFP gets the wrong co-specifier in Exp3 but this error allows it to get the correct co-specifier in Exp4.
    Another type of false positive