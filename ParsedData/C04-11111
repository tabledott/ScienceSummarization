undreds of terabytes of text data, most of which is in an unstructured format.
			Yet, many NLP algorithms tap into only megabytes or gigabytes of this information.
			In this paper, we make a step towards acquiring semantic knowledge from terabytes of data.
			We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004).
			We show that by simply utilizing more data on this task, we can achieve similar performance to a linguisticallyrich approach.
			The current state of the art co occurrence model requires an estimated 10 years just to parse a 1TB corpus (see Table 1).
			Instead of using a syntactically motivated co-occurrence ap proach as above, our system uses lexico-syntactic rules.
			In particular, it finds lexico-POS patterns by making modifications to the basic edit distance algorithm.
			Once these patterns have been learnt, the algorithm for finding new is-a relations