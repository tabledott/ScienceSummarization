95%L 95%U BLEU1 0.86 0.83 0.89 0.81 0.75 0.86 BLEU4 0.77 0.72 0.81 0.86 0.81 0.90 BLEU12 0.66 0.60 0.72 0.87 0.76 0.93 NIST 0.89 0.86 0.92 0.81 0.75 0.87 WER 0.47 0.41 0.53 0.69 0.62 0.75 PER 0.67 0.62 0.72 0.79 0.74 0.85 GTM10 0.82 0.79 0.85 0.73 0.66 0.79 GTM20 0.77 0.73 0.81 0.86 0.81 0.90 GTM30 0.74 0.70 0.78 0.87 0.81 0.91 Adequacy Fluency Table 1.
			Pearson's correlation analysis of 8 machine translation systems in 2003 NIST Chinese-English machine translation evaluation.
			generating more human like translations should also be improved.
			Before we demonstrate how to use ORANGE to evaluate automatic metrics, we briefly introduce three new metrics in the next section.
	
	
			ROUGE-L and ROUGE-S are described in details in Lin and Och (2004).
			Since these two metrics are relatively new, we provide short summaries of them in Section 3.1 and Section 3.3 respectively.
			ROUGE-W, an extension of ROUGE-L, is new and is explained in details in Section 3.2.
			3.1 ROUGE-L: Longest Common Sub-.
			sequence