hat uses many more features than just singular/plural frequency and det-noun frequency.
  
  
    We showed that simple, unsupervised models using web counts can be devised for a variety of NLP tasks.
    The tasks were selected so that they cover both syntax and semantics, both generation and analysis, and a wider range of n-grams than have been previously used.
    For all but two tasks (candidate selection for MT and noun countability detection) we found that simple, unsupervised models perform significantly better when ngram frequencies are obtained from the web rather than from a standard large corpus.
    This result is consistent with Keller and Lapata&#8217;s (2003) findings that the web yields better counts than the BNC.
    The reason for this seems to be that the web is much larger than the BNC (about 1000 times); the size seems to compensate for the fact that simple heuristics were used to obtain web counts, and for the noise inherent in web data.
    Our results were less encouraging when it come