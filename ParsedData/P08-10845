n and between languages.
    We test our model on a multilingual corpus of short parallel phrases drawn from the Hebrew Bible and Arabic, Aramaic, and English translations.
    The Semitic language family, of which Hebrew, Arabic, and Aramaic are members, is known for a highly productive morphology (Bravmann, 1977).
    Our results indicate that cross-lingual patterns can indeed be exploited successfully for the task of unsupervised morphological segmentation.
    When modeled in tandem, gains are observed for all language pairs, reducing relative error by as much as 24%.
    Furthermore, our experiments show that both related and unrelated language pairs benefit from multilingual learning.
    However, when common structures such as phonetic correspondences are explicitly modeled, related languages provide the most benefit.
  
  
    Multilingual Language Learning Recently, the availability of parallel corpora has spurred research on multilingual analysis for a variety of tasks ranging from morphology to sem