other tasks.
    The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings.
    These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology.
    There have been many attempts to incorporate syntax into alignment; we will not present a complete list here.
    Some methods parse two flat strings at once using a bitext grammar (Wu, 1997).
    Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001).
    The statistical models associated with syntactic aligners tend to be very different from their IBM counterparts.
    They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near