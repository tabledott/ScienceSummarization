).
    This manner of specifying prior knowledge about the task has several advantages.
    First, is it certainly compact (though it remains to be proven that it is effective).
    Second, it is more or less the minimum one would have to provide to a human annotator in order to specify a new annotation task and policy (compare, for example, with the list in figure 2, which suggests an entirely different task).
    Indeed, prototype lists have been used pedagogically to summarize tagsets to students (Manning and Sch&#168;utze, 1999).
    Finally, natural language does exhibit proform and prototype effects (Radford, 1988), which suggests that learning by analogy to prototypes may be effective for language tasks.
    In this paper, we consider three sequence modeling tasks: part-of-speech tagging in English and Chinese and a classified ads information extraction task.
    Our general approach is to use distributional similarity to link any given word to similar prototypes.
    For example, the word reported may