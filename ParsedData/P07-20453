ors from other source factors.
    However, Moses can have ambiguous input in the form of confusion networks.
    This input type has been used successfully for speech to text translation (Shen et al. 2006).
    Every factor on the target language can have its own language model.
    Since many factors, like lemmas and POS tags, are less sparse than surface forms, it is possible to create a higher order language models for these factors.
    This may encourage more syntactically correct output.
    In Figure 3 we apply two language models, indicated by the shaded arrows, one over the words and another over the lemmas.
    Moses is also able to integrate factored language models, such as those described in (Bilmes and Kirchhoff 2003) and (Axelrod 2006).
    4 Confusion Network Decoding Machine translation input currently takes the form of simple sequences of words.
    However, there are increasing demands to integrate machine translation technology into larger information processing systems with upstream NLP/