o complex decoding in which all the translated wordsare memorized for each hypothesis, and thus not in tegrated in our feature set.
			3.2.3 Target Bigram Features Target side bigram features are also included todirectly capture the fluency as in the n-gram language model (Roark et al, 2004).
			For instance, bi gram features of (ei?1, ei), (ei, ei+1), (ei+1, ei+2)... are observed in Figure 1.
			3.2.4 Hierarchical Features In addition to the phrase motivated features, weincluded features inspired by the hierarchical struc ture.
			Figure 2 shows an example of hierarchical phrases in the source side, consisting of X 1 ? ?
			f j?1X 2 f j+3 ? , X 2 ? ?
			f j f j+1X 3 ? and X 3 ? ?
			f j+2 ? .
			Hierarchical features capture the dependency of the source words in a parent phrase to the source words in child phrases, such as ( f j?1, f j), ( f j?1, f j+1),( f j+3, f j), ( f j+3, f j+1), ( f j, f j+2) and ( f j+1, f j+2) as in dicated by the arrows in Figure 2.
			The hierarchical features are extracted only fo