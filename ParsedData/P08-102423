 estimated as relative frequencies (pd, pr); lexical translation scores (plex d , plex r ), a binary flag for the glue rule which allows the model to (dis)favour monotone translation (gr); and rule and target word counts (rc, wc).
    Table 3 shows the results of our system on the test set.
    Firstly we show the relative scores of our model against Hiero without using reverse translation or lexical features.7 This allows us to directly study the differences between the two translation models without the added complication of the other features.
    As well as both modelling the same distribution, when our model is trained with a single parameter per-rule these systems have the same parameter space, differing only in the manner of estimation.
    Additionally we show the scores achieved by MERT training the full set of features for Hiero, with and without a language model.8 We provide these results for reference.
    To compare our model directly with these systems we would need to incorporate additional fea