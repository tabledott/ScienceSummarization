xpectation of r(d) def = EeEd re.
    We now show how to compute a few other quantities by choosing re appropriately.
    Entropy on a Hypergraph The entropy of the distribution of derivations in a hypergraph14 is where the first term Zq can be computed using the inside algorithm with hyperedge weights qe, and the numerator and denominator of the second term using an expectation semiring with hydef peredge weights (pe, pere) with re = log qe.
    The KL divergence to p from q can be computed as KL(p II q) = H(p, q) &#8722; H(p).
    Expected Loss (Risk) Given a reference sentence y*, the expected loss (i.e., Bayes risk) of the hypotheses in the hypergraph is defined as, where Y(d) is the target yield of d and L(y, y*) is the loss of the hypothesis y with respect to the reference y*.
    The popular machine translation metric, BLEU (Papineni et al., 2001), is not additively decomposable, and thus we are not able to compute the expected loss for it.
    Tromble et al. (2008) develop the following loss function,