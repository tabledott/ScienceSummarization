he greatest barrier to accurate word segmentation is in recognizing words that are not in the lexicon of the segmenter.
    Such a problem is dependent both on the source of the lexicon as well as the correspondence (in vocabulary) between the text in question and the lexicon.
    Wu and Fung (1994) demonstrate that segmentation accuracy is significantly higher when the lexicon is constructed using the same type of corpus as the corpus on which it is tested.
    We argue that rather than attempting to construct a single exhaustive lexicon or even a series of domain-specific lexica, it is more practical to develop a robust trainable means of compensating for lexicon inadequacies.
    Furthermore, developing such an algorithm will allow us to perform segmentation in many different languages without requiring extensive morphological resources and domain-specific lexica in any single language.
    For these reasons, we address the problem of word segmentation from a different direction.
    We introduce a rule-ba