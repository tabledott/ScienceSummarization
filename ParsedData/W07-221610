 easily shown that this formulation includes the projective model of Paskin (2001) and the nonprojective model of McDonald et al. (2005b).
    The definition of wk ij depends on the context in which it is being used.
    For example, in the work of McDonald et al. (2005b) it is simply a linear classifier that is a function of the words in the dependency, the label of the dependency, and any contextual features of the words in the sentence.
    In a generative probabilistic model (such as Paskin (2001)) it could represent the conditional probability of a word wj being generated with a label lk given that the word being modified is wi (possibly with some other information such as the orientation of the dependency or the number of words between wi and wj).
    We will attempt to make any assumptions about the form wk ij clear when necessary.
    For the remainder of this section we discuss three crucial problems for learning and inference while showing that each can be computed tractably for the non-projective c