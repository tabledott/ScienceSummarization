ight to shorter segments as compared to the segment level optimization.
  
  
    The optimization of the TERp edit costs, and comparisons against several standard automatic evaluation metrics, using human judgments of Adequacy is first described in Section 3.1.
    We then summarize, in Section 3.2, results of the NIST Metrics MATR workshop where TERp was evaluated as one of 39 automatic metrics using many test conditions and types of human judgments.
    As part of the 2008 NIST Metrics MATR workshop (Przybocki et al., 2008), a development subset of translations from eight Arabic-to-English MT systems submitted to NIST&#8217;s MTEval 2006 was released that had been annotated for Adequacy.
    We divided this development set into an optimization set and a test set, which we then used to optimize the edit costs of TERp and compare it against other evaluation metrics.
    TERp was optimized to maximize the segment level Pearson correlation with adequacy on the optimization set.
    The edit costs determined by