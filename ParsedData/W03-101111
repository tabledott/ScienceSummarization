is data is then available to the application task which will first have to compute the similarity for each pair of nouns based on current parameter settings and select nearest neighbours accordingly.
    We converted each noun-verb pair (n, vi) in the set-aside test data into a noun-verb-verb triple (n, vi, v2) where P(vi) is approximately equal to P(v2) over all the training data and (n, v2) has not been seen in the test or training data.
    A high frequency noun test set and a low frequency noun test set, each containing 10,000 test instances, were then constructed by selecting ten test instances for each noun in a two step process of 1) whilst more than ten triples remained, discarding duplicate triples and 2) randomly selecting ten triples from those remaining after step 1.
    Each set of test triples was split into five disjoint subsets, containing two triples for each noun, so that average performance and standard error could be computed.
    Additionally, three of the five subsets were used as a deve