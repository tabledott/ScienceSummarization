gorithm has a performance of 49.0% and 45.6% F-score, when using 18 labels and 110 labels, respectively.
    These results represent an error reduction of 18.8% (F-score improvement from 37.2% to 49.0%) over a state-of-the-art discourse parser (Marcu, 2000) when using 18 labels, and an error reduction of 15.7% (F-score improvement from 35.5% to 45.6%) when using 110 labels.
    The performance ceiling for sentence-level discourse structure derivation is given by the human annotation agreement F-score of 77.0% and 71.9%, when using 18 labels and 110 labels, respectively.
    The performance gap between the results of and human agreement is still large, and it can be attributed to three possible causes: errors made by the syntactic parser, errors made by the discourse segmenter, and the weakness of our discourse model.
    In order to quantitatively asses the impact in performance of each possible cause of error, we perform further experiments.
    We replace the syntactic parse trees produced by Charniak&#8217