 as the horizontal dashed line.
    These results show that for all tasks except &#8220;Fear&#8221; we are able to achieve expert-level ITA with the held-out set of experts within 9 labelers, and frequently within only 2 labelers.
    Pooling judgments across all 7 tasks we find that on average it requires only 4 non-expert annotations per example to achieve the equivalent ITA as a single expert annotator.
    Given that we paid US$2.00 in order to collect the 7000 non-expert annotations, we may interpret our rate of 3500 non-expert labels per USD as at least 875 expert-equivalent labels per USD.
    This task replicates the word similarity task used in (Miller and Charles, 1991), following a previous task initially proposed by (Rubenstein and Goodenough, 1965).
    Specifically, we ask for numeric judgments of word similarity for 30 word pairs on a scale of [0,10], allowing fractional responses4.
    These word pairs range from highly similar (e.g., {boy, lad}), to unrelated (e.g., {noon, string}).
    Numer