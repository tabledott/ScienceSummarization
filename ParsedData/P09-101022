sequences, as defined in Section 5.2.
    As shown there, policy gradient with this reward is equivalent to stochastic gradient ascent with a maximum likelihood objective. when only a subset of training documents is annotated, and environment reward is used for the remainder.
    Our method seamlessly combines these two kinds of rewards. sider two naive baselines.
    Both scan through each sentence from left to right.
    A command c is executed on the object whose name is encountered first in the sentence.
    This command c is either selected randomly, or set to the majority command, which is leftclick.
    This procedure is repeated until no more words match environment objects.
  
  
    Table 2 presents evaluation results on the test sets.
    There are several indicators of the difficulty of this task.
    The random and majority baselines&#8217; poor performance in both domains indicates that naive approaches are inadequate for these tasks.
    The performance of the fully supervised approach provides