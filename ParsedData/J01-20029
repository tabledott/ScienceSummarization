  In boosting, the errors made by a classifier learned from a training set are used to construct a new training set in which the misclassified examples get more weight.
    By sequentially performing this operation, an ensemble is constructed (e.g., ADABOOST, [Freund and Schapire 1996]).
    This class of methods is also called arcing (for adaptive resampling and combining).
    In general, boosting obtains better results than bagging, except when the data is noisy (Dietterich 1997).
    Another way to create multiple classifiers is to train classifiers on different sources of information about the task by giving them access to different subsets of the available input features (Cherkauer 1996).
    Still other ways are to represent the output classes as bit strings where each bit is predicted by a different component classifier (error correcting output coding [Dietterich and Bakiri 1995]) or to develop learning-method-specific methods for ensuring (random) variation in the way the different classifiers of an 