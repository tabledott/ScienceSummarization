4-best, the relative performance of each automatic evaluation metric group stays the same.
			ROUGE-S4 is still the best performer.
			Figure 1 shows the trend of ORANGE scores for these metrics over N-best list of N from 1 to 16384 with length increment of 64.
			It is clear that relative performance of these metrics stay the same over the entire range.
			5 Conclusion.
			In this paper we introduce a new automatic evaluation method, ORANGE, to evaluate automatic evaluation metrics for machine translations.
			We showed that the new method can be easily implemented and integrated with existing statistical machine translation frameworks.
			ORANGE assumes a good automatic evaluation metric should assign high scores to good translations and assign low scores to bad translations.
			Using reference translations as examples of good translations, we measure the quality of an automatic evaluation metric based on the average rank of the references within a list of alternative machine translations.
			Comparing with