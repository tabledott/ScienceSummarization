Charniak et al., 1993; Weischedel et al., 1993; Schutze and Singer, 1994; Lin et al., 1994; Elworthy, 1994; Merialdo, 19951.2 For a Markov-model based tagger, training consists of learning both lexical probabilities (P(worclItag)) and contextual probabilities (P(tagiltagi_i tagi_n)).
    Once trained, a sentence can be tagged by searching for the tag sequence that maximizes the product of lexical and contextual probabilities.
    The most accurate stochastic taggers use estimates of lexical and contextual probabilities extracted from large manually annotated corpora (eg.
    [Weischedel et al., 1993; Charniak et al., 1993]).
    It is possible to use unsupervised learning to train stochastic taggers without the need for a manually annotated corpus by using the Baum-Welch algorithm [Baum, 1972; Jelinek, 1985; Cutting et al., 1992; Kupiec, 1992; Elworthy, 1994; Merialdo, 19951.
    This algorithm works by iteratively adjusting the lexical and contextual probabilities to increase the overall probability of the t