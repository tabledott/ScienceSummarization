he initial corpus.
    Starting with 100k words of parallel data, we eventually collect 20M words of in-domain Arabic-English data and 90M words of in-domain Chinese-English data.
    Figures 12 and 13 show the BLEU scores of these MT systems.
    For comparison purposes, we also plotted on each graph the performance of our best MT system for that language pair, trained on all our available parallel data (Table 7).
    As we can see, bootstrapping allows us to extract significantly larger amounts of data, which leads to significantly higher BLEU scores.
    Starting with as little as 100k English tokens of parallel data, we obtain MT systems that come within 7&#8211;10 BLEU points of systems trained on parallel corpora of more than 100M English tokens.
    This shows that using our method, a good-quality MT system can be built from very little parallel data and a large amount of comparable, non-parallel data.
  
  
    We conclude the description of our method by presenting a few sentence pairs extracted by o