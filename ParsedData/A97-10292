text encountered when decoding as though it had passed through a noisy channel, where it had been originally marked with named entities.'
    The job of the generative model is to model the original process which generated the name-class&#8211;annotated words, before they went through the noisy channel.
    More formally, we must find the most likely sequence of name-classes (NC) given a sequence of words (W): Pr(NC I W) (2.1) In order to treat this as a generative model (where it generates the original, name-class&#8211;annotated words), and since the a priori probability of the word sequence&#8212;the denominator&#8212;is constant for any given sentence, we can maxi-mize Equation 2.2 by maximizing the numerator alone.
    I See (Cover and Thomas, 1991), ch.
    2, for an excellent overview of the principles of information theory.
    (2.2) Previous approaches have typically used manually constructed finite state patterns (Weischodel, 1995, Appelt et al., 1995).
    For every new language and every new class