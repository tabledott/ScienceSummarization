  Interestingly, at the word where the failure occurred, the sum of the probabilities was 0.9301.
    In order to get a sense of whether these perplexity reduction results can translate to improvement in a speech recognition task, we performed a very small preliminary experiment on n-best lists.
    The DARPA '93 HUB1 test setup consists of 213 utterances read from the Wall Street Journal, a total of 3,446 words.
    The corpus comes with a baseline trigram model, using a 20,000-word open vocabulary, and trained on approximately 40 million words.
    We used Ciprian Chelba's A* decoder to find the 50 best hypotheses from each lattice, along with the acoustic and trigram scores.'
    Given the idealized circumstances of the production (text read in a lab), the lattices are relatively sparse, and in many cases 50 distinct string hypotheses were not found in a lattice.
    We reranked an average of 22.9 hypotheses with our language model per utterance.
    One complicating issue has to do with the tokenization i