e LCS over the maximum string length between ill-formed word and the candidate, since the ill-formed word can be either longer or shorter than (or the same size as) the standard form.
    For example, mve can be restored to either me or move, depending on context.
    We normalise these ratios following Cook and Stevenson (2009).
    For context inference, we employ both language model- and dependency-based frequency features.
    Ranking by language model score is intuitively appealing for candidate selection, but our trigram model is trained only on clean Twitter data and illformed words often don&#8217;t have sufficient context for the language model to operate effectively, as in bt &#8220;but&#8221; in say 2 sum1 bt nt gonna say &#8220;say to someone but not going to say&#8221;.
    To consolidate the context modelling, we obtain dependencies from the dependency bank used in ill-formed word detection.
    Although text messages are of a different genre to edited newswire text, we assume they form similar 