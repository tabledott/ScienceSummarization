rom 2003&#8211;2006 and from GALE development data.
    The test set, which contains both web and newswire data, is the evaluation set from the NIST 2008 MT evaluation.
    We trained a 4-gram English language model on the English side of the training data.
    For Chinese-English we used 173M words of training data from GALE 2008.
    For SBMT we used a 32M word subset for extracting rules and building a language model, but used the entire training data for alignments, and for all PBMT training.
    The tune and test sets both contain web and newswire data.
    The tune set is selected from NIST MT evaluation sets from 2003&#8211;2006.
    The test set is the evaluation set from the NIST 2008 MT evaluation.
    We trained a 3-gram English language model on the English side of the training data.
    For each of our systems we identify two feature sets: baseline, which correspond to the typical small feature set reported in current MT literature, and extended, a superset of baseline, which adds hundreds or tho