as is often done, that the occurrence of each cue is independent of the others, then this term can be replaced with: In TLC, we have made this assumption and have estimated p(ci I si) from the training.
    Of course, the sparse data problem affects these probabilities too, and so TLC uses the Good-Turing formula (Good 1953; Chiang, Lin, and Su 1995), to smooth the values of p(cj s,), including providing probabilities for cues that did not occur in the training.
    TLC actually uses the mean of the Good-Turing value and the training-derived value for p(cj s,).
    When cues do not appear in training, it uses the mean of the GoodTuring value and the global probability of the cue p(ci), obtained from a large text corpus.
    This approach to smoothing has yielded consistently better performance than relying on the Good-Turing values alone. tuation.
    For this cue type, p(cj I s,) is the probability that item cl appears precisely at location j for sense Si.
    Positions j = &#8212;2, &#8212;1, 1,2 are used.
