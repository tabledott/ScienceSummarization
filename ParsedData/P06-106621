rence were extracted.
    Finally, we ran the MaxEnt toolkit by Zhang 4 to tune the feature weights.
    We set iteration number to 100 and Gaussian prior to 1 for avoiding overfitting.
    We dropped unknown words (Koehn et al., 2005) of translations for both tasks before evaluating their BLEU scores.
    To be consistent with the official evaluation criterions of both tasks, casesensitive BLEU-4 scores were computed For the NIST MT-05 task and case-insensitive BLEU-4 scores were computed for the IWSLT-04 task 5.
    Experimental results on both tasks are shown in Table 1.
    Italic numbers refer to results for which the difference to the best result (indicated in bold) is not statistically significant.
    For all scores, we also show the 95% confidence intervals computed using Zhang&#8217;s significant tester (Zhang et al., 2004) which was modified to conform to NIST&#8217;s definition of the BLEU brevity penalty.
    We observe that if phrasal reordering is totally dependent on the language model (NONE) 