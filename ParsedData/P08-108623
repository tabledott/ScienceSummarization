e explanation could be that the domain of the ar gigaword corpus is much closer to the domain of the test data than that of other training data sets used.
    However, further investigation is required to explain the improvements.
    The clusters produced by the distributed algorithm vary in their size and number of occurrences.
    In a clustering of the en target data set with 1,024 clusters, the cluster sizes follow a typical longtailed distribution with the smallest cluster containing 13 words and the largest cluster containing 20,396 words.
    Table 4 shows some examples of the generated clusters.
    For each cluster we list all words occurring more than 1,000 times in the corpus.
  
  
    In this paper, we have introduced an efficient, distributed clustering algorithm for obtaining word classifications for predictive class-based language models with which we were able to use billions of tokens of training data to obtain classifications for millions of words in relatively short amounts of time.
    T