erefore kept the instructions simple: You are shown a source sentence followed by several candidate translations.
    Your task is to rank the translations from best to worst (ties are allowed).
    Each screen for this task involved judging translations of three consecutive source segments.
    For each source segment, the annotator was shown the outputs of five submissions, and asked to rank them.
    We refer to each of these as ranking tasks or sometimes blocks.
    Every language task had more than five participating systems &#8212; up to a maximum of 16 for the German-English task.
    Rather than attempting to get a complete ordering over the systems in each ranking task, we instead relied on random selection and a reasonably large sample size to make the comparisons fair.
    We use the collected rank labels to assign each system a score that reflects how highly that system was usually ranked by the annotators.
    The score for some system A reflects how frequently it was judged to be better than oth