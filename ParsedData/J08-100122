, the probability of the transition [S &#8211;] in the grid from Table 1 is 0.08 (computed as a ratio of its frequency [i.e., six] divided by the total number of transitions of length two [i.e., 75]).
    Each text can thus be viewed as a distribution defined over transition types.
    We can now go one step further and represent each text by a fixed set of transition sequences using a standard feature vector notation.
    Each grid rendering j of a document di corresponds to a feature vector &#934;(xij) = (p1(xij), p2(xij), ... , pm(xij)), where m is the number of all predefined entity transitions, and pt(xij) the probability of transition t in grid xij.
    This feature vector representation is usefully amenable to machine learning algorithms (see our experiments in Sections 4&#8211;6).
    Furthermore, it allows the consideration of large numbers of transitions which could potentially uncover novel entity distribution patterns relevant for coherence assessment or other coherence-related tasks.
    Note tha