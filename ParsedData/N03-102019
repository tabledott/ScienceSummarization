the Document Understanding Conference.
    We then discussed the IBM BLEU MT evaluation metric, its application to summary evaluation, and the difference between precisionbased BLEU translation evaluation and recall-based DUC summary evaluation.
    The discrepancy led us to examine the effectiveness of individual n-gram cooccurrence statistics as a substitute for expensive and error-prone manual evaluation of summaries.
    To evaluate the performance of automatic scoring metrics, we proposed two test criteria.
    One was to make sure system rankings produced by automatic scoring metrics were similar to human rankings.
    This was quantified by Spearman's rank order correlation coefficient and three other parametric correlation coefficients.
    Another was to compare the statistical significance test results between automatic scoring metrics and human assessments.
    We used recall and precision of the agreement between the test statistics results to identify good automatic scoring metrics.
    According