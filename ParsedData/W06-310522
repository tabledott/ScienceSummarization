of 0.005.
    We might address the determinization in OEM without resorting to interpolation by modifying the training procedure to retain entropy.
    By imposing a non-uniform segmentation model that favors shorter phrases over longer ones, we hope to prevent the error-causing effects of EM training outlined above.
    In principle, this change will encourage EM to explain training sentences with shorter sentences.
    In practice, however, this approach has not led to an improvement in BLEU.
    Another approach to maintaining entropy during the training process is to smooth the probabilities generated by EM.
    In particular, we can use the following smoothed update equation during the training loop, which reserves a portion of probability mass for unseen translations.
  
  
    We would like to thank the anonymous reviewers for their valuable feedback on this paper.
    In the equation above, l is the length of the French phrase and k is a tuning parameter.
    This formulation not only serves to reduce