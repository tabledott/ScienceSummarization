, and so on.
    Perhaps more surprisingly, we find that this is also the case for a variety of semantic relations, as measured by the SemEval 2012 task of measuring relation similarity.
    Atlanta, Georgia, 9&#8211;14 June 2013. c&#65533;2013 Association for Computational Linguistics The remainder of this paper is organized as follows.
    In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7.
  
  
    Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990).
    More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the &#8220;next&#8221; word, given some preceding words.
    These 