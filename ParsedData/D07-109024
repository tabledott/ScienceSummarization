a sets, and 200 for the web data set.All terms below the threshold are mapped to a spe cial term UNK, representing the unknown word.Figure 3 shows the number of n-grams for language models trained on 13 million to 2 trillion to kens.
			Both axes are on a logarithmic scale.
			The right scale shows the approximate size of the served language models in gigabytes.
			The numbers above the lines indicate the relative increase in languagemodel size: x1.8/x2 means that the number of n grams grows by a factor of 1.8 each time we doublethe amount of training data.
			The values are simi lar across all data sets and data sizes, ranging from 1.6 to 1.8.
			The plots are very close to straight lines in the log/log space; linear least-squares regression finds r2  0.99 for all four data sets.
			The web data set has the smallest relative increase.
			This can be at least partially explained by the highervocabulary cutoff.
			The largest language model gen erated contains approx.
			300 billion n-grams.
			Table 2 shows s