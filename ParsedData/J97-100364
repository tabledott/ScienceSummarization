nt boundaries, may be inherently more difficult than many previous types of content analysis (for instance, dividing newspaper articles based on subject matter)&amp;quot; and so implies that the levels of agreement needed to indicate good reliability for TextTiling may be justified in being lower.
    For my test texts, the judges placed boundaries on average 39.1% of the time, and nonboundaries 60.9%.
    Thus the expected chance agreement P(E) is .524 (since P(Boundary) = .391 and P(Nonboundary) ----- .609, (.3912 + .6092) = .524).
    To compute K, each judge's decision was compared to the group decision, where a paragraph gap was considered a &amp;quot;true&amp;quot; boundary if at least three out of seven judges placed a boundary mark there, as in Litman and Passonneau (1995).11 The remaining gaps are considered nonboundaries.
    The average K for these texts was .647.
    This score is at the low end of the stated acceptability range but is comparable with those of other interreliability results (with 