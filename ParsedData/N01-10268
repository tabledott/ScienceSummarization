g final function word assignments.
    Given the lower frequency of most content words, the potential risks of using these 1-to-n alignments are greater, but so are the benefits given that the 1-to-1 alignments tend to be both sparse and somewhat biased.
    Several options are under investigation for combining these two P(t1w) estimators, but the simplest, and currently most effective, is to perform basic interpolation between the tag distributions estimated from 1-to-1 alignments only and from the entire set of 1-to-n alignments (including 1-to-1) as follows: While this does indeed introduce substantial spurious tag probabilities initially, the aggressive smoothing towards the majority tag(s) described above tends to eliminate most of this noise.
    The major reason for estimating the lexical priors and tag sequence model separately is that a tag sequence bigram (or even trigram) model has far fewer parameters than the lexical prior model and thus can be estimated on a very conservatively chosen set of fil