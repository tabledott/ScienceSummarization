information content (Resnik, 1995).
			As in Resnik?s defini tion, this value for non-word nodes is equal to thesum of all the frequencies of hyponym words.
			Un like Resnik, we do not divide frequency among all senses of a word; each sense of a word contributes its full frequency to ?.
	
	
			As described above, the problem of WSD corresponds to posterior inference: determining the probability distribution of the hidden variables given ob served words and then selecting the synsets of themost likely paths as the correct sense.
			Directly com puting this posterior distribution, however, is not tractable because of the difficulty of calculating the normalizing constant in Equation 1.To approximate the posterior, we use Gibbs sampling, which has proven to be a successful approx imate inference technique for LDA (Griffiths and Steyvers, 2004).
			In Gibbs sampling, like all Markov chain Monte Carlo methods, we repeatedly sample from aMarkov chain whose stationary distribution is the posterior of interest (Robe