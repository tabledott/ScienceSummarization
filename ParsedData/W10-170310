uld agree by chance.
    For inter-annotator agreement for the ranking tasks we calculated P(A) by examining all pairs of systems which had been judged by two or more judges, and calculated the proportion of time that they agreed that A &gt; B, A = B, or A &lt; B. Intraannotator agreement was computed similarly, but we gathered items that were annotated on multiple occasions by a single annotator.
    Table 4 gives K values for inter-annotator and intra-annotator agreement.
    These give an indication of how often different judges agree, and how often single judges are consistent for repeated judgments, respectively.
    The exact interpretation of the kappa coefficient is difficult, but according to Landis and Koch (1977), 0 &#8722;.2 is slight, .2 &#8722;.4 is fair, .4 &#8722; .6 is moderate, .6 &#8722; .8 is substantial and the rest is almost perfect.
    Based on these interpretations the agreement for sentence-level ranking is moderate for interannotator agreement and substantial for intraannotator agre