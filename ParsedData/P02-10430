
	Generative Models For Statistical Parsing With Combinatory Categorial Grammar
		This paper compares a number of generative probability models for a widecoverage Combinatory Categorial Gram mar (CCG) parser.
		These models are trained and tested on a corpus obtained by translating the Penn Treebank trees into CCG normal-form derivations.
		According to an evaluation of unlabeled word-word dependencies, our best model achieves a performance of 89.9%, comparable to thefigures given by Collins (1999) for a lin guistically less expressive grammar.
		Incontrast to Gildea (2001), we find a significant improvement from modeling word word dependencies.
	
	
			The currently best single-model statistical parser (Charniak, 1999) achieves Parseval scores of over 89% on the Penn Treebank.
			However, the grammar underlying the Penn Treebank is very permissive, and a parser can do well on the standard Parsevalmeasures without committing itself on certain se mantically significant decisions, such as predicting null element