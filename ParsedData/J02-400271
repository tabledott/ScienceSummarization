t another BACKGROUND sentence will follow after a BACKGROUND sentence.
    Each of these features, however, still contributes to the final result: If any of them is taken out of the feature pool, classification performance decreases.
    How can this be, given that the individual features perform worse than chance?
    As the classifier derives the posterior probability by multiplying evidence from each feature, even slight evidence coming from one feature can direct the decision in the right direction.
    A feature that contributes little evidence on its own (too little to break the prior probability, which is strongly biased toward OWN) can thus, in combination with others, still help in disambiguating.
    For the naive Bayesian classification method, indeed, it is most important that the features be as independent of each other as possible.
    This property cannot be assessed by looking at the feature&#8217;s isolated performance, but only in combination with others.
    It is also interesting to see th