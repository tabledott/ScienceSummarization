nderlying information extraction system.)
    It is important to note some important caveats in making this comparison, the most prominent being that DIRT was not designed with sentence-paraphrase generation in mind &#8212; its templates are much shorter than ours, which may have affected the evaluators&#8217; judgments &#8212; and was originally implemented on much larger data sets.7 The point of this evaluation is simply to determine whether another corpusbased paraphrase-focused approach could easily achieve the same performance level.
    In brief, the DIRT system works as follows.
    Dependency trees are constructed from parsing a large corpus.
    Leaf-to-leaf paths are extracted from these dependency 7To cope with the corpus-size issue, DIRT was trained on an 84MB corpus of Middle-East news articles, a strict superset of the 9MB we used.
    Other issues include the fact that DIRT&#8217;s output needed to be converted into English: it produces paths like &#8220;N:of:N tide N:nn:N&#8221;, which we tran