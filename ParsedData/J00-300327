ously recorded conversations.
    However, the HMM formulation used here also supports computing posterior DA probabilities based on partial evidence, e.g., using only the utterances preceding the current one, as would be required for on-line processing.
  
  
    The statistical discourse grammar models the prior probabilities P(U) of DA sequences.
    In the case of conversations for which the identities of the speakers are known (as in Switchboard), the discourse grammar should also model turn-taking behavior.
    A straightforward approach is to model sequences of pairs (U1, TO where Ili is the DA label and T, represents the speaker.
    We are not trying to model speaker idiosyncrasies, so conversants are arbitrarily identified as A or B, and the model is made symmetric with respect to the choice of sides (e.g., by replicating the training sequences with sides switched).
    Our discourse grammars thus had a vocabulary of 42 x 2 = 84 labels, plus tags for the beginning and end of conversations.
    For e