  This is a 0.23 reduction in the average number of bracket errors per sentence.
    The &#8216;Other&#8217; category includes VP, PRN and other attachments, appositives and internal structures of modifiers and QPs.
    Analysis of Composition Matrices.
    An analysis of the norms of the binary matrices reveals that the model learns a soft vectorized notion of head words: Head words are given larger weights and importance when computing the parent vector: For the matrices combining siblings with categories VP:PP, VP:NP and VP:PRT, the weights in the part of the matrix which is multiplied with the VP child vector dominates.
    Similarly NPs dominate DTs.
    Fig.
    5 shows example matrices.
    The two strong diagonals are due to the initialization described in Sec.
    3.7.
    Semantic Transfer for PP Attachments.
    In this small model analysis, we use two pairs of sentences that the original Stanford parser and the CVG did not parse correctly after training on the WSJ.
    We then continue to train bo