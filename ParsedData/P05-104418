are consistently the best, far out-stripping EM.
    These gains dwarf the performance of EM on over 1.1M words (66.6% as reported by Smith and Eisner (2004)), even when the latter uses improved search (70.0%).
    DEL1WORD and DEL1SUBSEQ, on the other hand, are poor, even worse than EM on larger datasets.
    An important result is that neighborhoods do not succeed by virtue of approximating log-linear EM; if that were so, we would expect larger neighborhoods (like DEL1SUBSEQ) to out-perform smaller ones (like TRANS1)&#8212;this is not so.
    DEL1SUBSEQ and DEL1WORD are poor because they do not give helpful classes of negative evidence: deleting a word or a short subsequence often does very little damage.
    Put another way, models that do a good job of explaining why no word or subsequence should be deleted do not do so using the familiar POS categories.
    The LENGTH neighborhood is as close to loglinear EM as it is practical to get.
    The inconsistencies in the LENGTH curve (Fig.
    2) are notable a