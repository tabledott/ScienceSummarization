bead by bead, using the 1:1 word beads to capture the dependence between English and French words.
    As a first cut, consider the following &amp;quot;model&amp;quot;: where B = {b1,...,b1} is a multiset of word beads, p(1) is the probability that an English sentence and a French sentence contain 1 word beads, and p(b) denotes the frequency of the word bead bi.
    This simple model captures lexical dependencies between English and French sentences.
    However, this &amp;quot;model&amp;quot; does not satisfy the constraint that EB P*(B) = 1; because beadings B are unordered multisets, the sum is substantially less than one.
    To force this model to sum to one, we simply normalize by a constant so that we retain the qualitative aspects of the model.
    We take While a beading B describes an unordered multiset of English and French words, sentences are in actuality ordered sequences of words.
    We need to model word ordering, and ideally the probability of a sentence bead should depend on the ordering of