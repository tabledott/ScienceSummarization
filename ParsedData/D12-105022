l (NLM), for each phrase combination: adjective noun (Adj-N), nounnoun (N-N) and verb object (V-Obj).
    For each phrase type we report results for each compositional model, namely additive (+), multiplicative (0) and recursive autoencoder (RAE).
    The table also shows the dimensionality of the input vectors next to the vector representation.
    As can be seen, for SDS the best performing model is multiplication, as it is mostly for DM.
    With regard to NLM, vector addition yields overall better results.
    In general, neither DM or NLM in any compositional configuration are able to outperform SDS with multiplication.
    All models in Table 3 are significantly correlated with the human similarity judgments (p &lt; 0.01).
    Spearman&#8217;s p differences of 0.3 or more are significant at the 0.01 level, using a ttest (Cohen and Cohen, 1983).
  
  
    Although the phrase similarity task gives a fairly direct insight into semantic similarity and compositional representations, it is somewhat limited in