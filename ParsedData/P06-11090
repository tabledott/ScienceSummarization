
  An All-Subtrees Approach To Unsupervised Parsing
  
    We investigate generalizations of the allsubtrees &amp;quot;DOP&amp;quot; approach to unsupervised parsing.
    Unsupervised DOP models assign all possible binary trees to a set of sentences and next use (a large random subset of) all subtrees from these binary trees to compute the most probable parse trees.
    We will test both a relative frequency estimator for unsupervised DOP and a maximum likelihood estimator which is known to be statistically consistent.
    We report state-ofthe-art results on English (WSJ), German (NEGRA) and Chinese (CTB) data.
    To the best of our knowledge this is the first paper which tests a maximum likelihood estimator for DOP on the Wall Street Journal, leading the surprising result that unsupervised parsing model beats a widely used supervised model (a treebank PCFG).
  
  
    The problem of bootstrapping syntactic structure from unlabeled data has regained considerable interest.
    While supervised parsers suffer