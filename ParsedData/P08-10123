ammar framework, and by only learning small noncompositional phrases.
    We address the tendency of EM to overfit by using Bayesian methods, where sparse priors assign greater mass to parameter vectors with fewer non-zero values therefore favoring shorter, more frequent phrases.
    We test our model by extracting longer phrases from our model&#8217;s alignments using traditional phrase extraction, and find that a phrase table based on our system improves MT results over a phrase table extracted from traditional word-level alignments.
  
  
    We use a phrasal extension of Inversion Transduction Grammar (Wu, 1997) as the generative framework.
    Our ITG has two nonterminals: X and C, where X represents compositional phrase pairs that can have recursive structures and C is the preterminal over terminal phrase pairs.
    There are three rules with X on the left-hand side: The first two rules are the straight rule and inverted rule respectively.
    They split the left-hand side constituent which represents a