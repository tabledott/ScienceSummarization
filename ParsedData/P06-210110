, i.e., the expected loss under p&#952; across all analyses yi: This &#8220;smoothed&#8221; objective is now continuous and differentiable.
    However, it no longer exactly mimics test conditions, and it typically remains nonconvex, so that gradient descent is still not guaranteed to find a global minimum.
    Och (2003) found that such smoothing during training &#8220;gives almost identical results&#8221; on translation metrics.
    The simplest possible loss function is 0/1 loss, where L(y) is 0 if y is the true analysis y&#8727;i and 1 otherwise.
    This loss function does not attempt to give partial credit.
    Even in this simple case, assuming P =6 NP, there exists no general polynomial-time algorithm for even approximating (2) to within any constant factor, even for Ki = 2 (Hoffgen et al., 1995, from Theorem 4.10.4).1 The same is true for for (3), since for Ki = 2 it can be easily shown that the min 0/1 risk is between 50% and 100% of the min 0/1 loss.
    Rather than minimizing a loss function suite