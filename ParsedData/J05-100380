  Other NLP tasks are likely to have similar characteristics in terms of sparsity.
    Experiments show an efficiency gain of a factor of over 2,600 on the parsing data for the new algorithm over the obvious implementation of the boosting approach.
    We would argue that the improved boosting algorithm is a natural alternative to maximum-entropy or (conditional) log-linear models.
    The article has drawn connections between boosting and maximum-entropy models in terms of the optimization problems that they involve, the algorithms used, their relative efficiency, and their performance in empirical tests.
  
  
    This appendix gives a derivation of the optimal updates for ExpLoss.
    The derivation is very close to that in Schapire and Singer (1999).
    Recall that for parameter values &#175;a, we need to compute BestWt&#240;k, &#175;a&#222; and BestLoss&#240;k, &#175;a&#222; for k 1/4 1, ... , m, where BestWt&#240;k, &#175;a&#222; 1/4 arg min ExpLoss&#240;Upd&#240;&#175;a, k, d&#222;&#222; d and BestLos