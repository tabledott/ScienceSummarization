 the mix ture of a factored, multinomial-based base model and a potentially very rich log-linear model.
			Thebase model gives our model robustness, and the log 29 test set training decoding MAP MRR ? ?
			0.6029 0.6852 ? max 0.5822 0.6489 max ? 0.5559 0.6250 max max 0.5571 0.6365Table 3: Experimental results on comparing sum ming over alignments (?)
			with maximizing (max) over alignments on the test set.
			Boldface marks the best score in a column and any scores in that columnnot significantly worse under a a two-tailed paired t test (p &lt; 0.03).linear model allows us to throw in task- or domainspecific features.
			Using a mixture gives the advantage of smoothing (in the base model) without hav ing to normalize the log-linear model by summing over large sets.
			This powerful combination leads us to believe that our model can be easily ported to other semantic processing tasks where modeling syntactic and semantic transformations is the key,such as textual entailment, paraphrasing, and cross lingual QA