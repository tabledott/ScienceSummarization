s given the induced tags.
    We use the symbol W to refer to the random variable related to the word, G for the associated gold standard tag, and T for the tag produced by one of our algorithms.
    Recall that Thus low conditional entropy means that the mutual information between the gold and induced tags will be high.
    If we have a random set of tags the mutual information will be zero and the conditional entropy will be the same as the entropy of the tag set.
    Again, this approach has several weaknesses: there is not a unique well-defined set of part-ofspeech tags, but rather many different possible sets that reflect rather arbitrary decisions by the annotators.
    To put the scores we present below in context, we note that using some data sets prepared for the AMALGAM project (Atwell et al., 2000) the conditional entropies between some data manually tagged with different tag sets varied from 0.22 (between Brown and LOB tag sets) to 1.3 (between LLC and Unix Parts tag sets).
    Secondly, because o