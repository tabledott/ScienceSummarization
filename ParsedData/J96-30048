 names.
    We will evaluate of the segmentation, as well as the performance.
    This latter evaluation compares the performance of the system with that of several human judges since, as we shall show, even people do not agree on a single correct way to segment a text.
    Finally, this effort is part of a much larger program that we are undertaking to develop stochastic finite-state methods for text analysis with applications to TTS and other areas; in the final section of this paper we will briefly discuss this larger program so as to situate the work discussed here in a broader context.
  
  
    The initial stage of text analysis for any NLP task usually involves the tokenization of the input into words.
    For languages like English one can assume, to a first approximation, that word boundaries are given by whit espace or punctuation.
    In various Asian languages, including Chinese, on the other hand, whites pace is never used to delimit words, so one must resort to lexical information to &amp;quot;r