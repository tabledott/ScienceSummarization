moothing parameter, and k is the number of possible labels.
    In this paper k = 3 (the three labels are person, organization, location), and we set a = 0.1.
    Equation 2 is an estimate of the conditional probability of the label given the feature, P(yjx).
    2 We now introduce a new algorithm for learning from unlabeled examples, which we will call DLCoTrain (DL stands for decision list, the term Cotrain is taken from (Blum and Mitchell 98)).
    The 2(Yarowsky 95) describes the use of more sophisticated smoothing methods.
    It's not clear how to apply these methods in the unsupervised case, as they required cross-validation techniques: for this reason we use the simpler smoothing method shown here. input to the unsupervised algorithm is an initial, &amp;quot;seed&amp;quot; set of rules.
    In the named entity domain these rules were Each of these rules was given a strength of 0.9999.
    The following algorithm was then used to induce new rules: Let Count' (x) be the number of times feature x is seen