
  Paraphrasing For Automatic Evaluation
  
    This paper studies the impact of paraphrases on the accuracy of automatic evaluation.
    Given a reference sentence and a machine-generated sentence, we seek to find a paraphrase of the reference sentence that is closer in wording to the machine output than the original reference.
    We apply our paraphrasing method in the context of machine translation evaluation.
    Our experiments show that the use of a paraphrased synthetic reference refines the accuracy of automatic evaluation.
    We also found a strong connection between the quality of automatic paraphrases as judged by humans and their contribution to automatic evaluation.
  
  
    The use of automatic methods for evaluating machine-generated text is quickly becoming mainstream in natural language processing.
    The most notable examples in this category include measures such as BLEU and ROUGE which drive research in the machine translation and text summarization communities.
    These methods asses