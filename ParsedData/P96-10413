ely simple to implement, we show that these methods yield good performance in bigram models and superior performance in trigram models.
    We take the performance of a method in to be its cross-entropy on test data where Pin (ti) denotes the language model produced with method 711 and where the test data T is composed of sentences (ti, , and contains a total of NT words.
    The entropy is inversely related to the average probability a model assigns to sentences in the test data, and it is generally assumed that lower entropy correlates with better performance in applications.
    In n-gram language modeling, the probability of a string P(s) is expressed as the product of the probabilities of the words that compose the string, with each word probability conditional on the identity of the last n &#8212; 1 words, i.e., ifs 'tut &#8226; &#8226; wi we have where tul denotes the words wi &#8226; &#8226;w1.
    Typically, n is taken to be two or three, corresponding to a bigram or trigram model, respectively.'
   