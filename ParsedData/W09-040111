n two different ways: The total number of judgments collected for the different modes of annotation is given in Table 3.
    In all cases, the output of the various translation outputs were judged on equal footing; the output of system combinations was judged alongside that of the individual system, and the constrained and unconstrained systems were judged together.
    Ranking translations relative to each other is a reasonably intuitive task.
    We therefore kept the instructions simple: Rank translations from Best to Worst relative to the other choices (ties are allowed).
    In our the manual evaluation, annotators were shown at most five translations at a time.
    For most language pairs there were more than 5 systems submissions.
    We did not attempt to get a complete ordering over the systems, and instead relied on random selection and a reasonably large sample size to make the comparisons fair.
    Relative ranking is our official evaluation metric.
    Individual systems and system combinations a