he state-of-the-art in noun phrase (NP) coreference resolution is typically quantified based on system performance on manually annotated text corpora.
    In spite of the availability of several benchmark data sets (e.g.
    MUC-6 (1995), ACE NIST (2004)) and their use in many formal evaluations, as a field we can make surprisingly few conclusive statements about the state-of-theart in NP coreference resolution.
    In particular, it remains difficult to assess the effectiveness of different coreference resolution approaches, even in relative terms.
    For example, the 91.5 F-measure reported by McCallum and Wellner (2004) was produced by a system using perfect information for several linguistic subproblems.
    In contrast, the 71.3 F-measure reported by Yang et al. (2003) represents a fully automatic end-to-end resolver.
    It is impossible to assess which approach truly performs best because of the dramatically different assumptions of each evaluation.
    Results vary widely across data sets.
    Corefe