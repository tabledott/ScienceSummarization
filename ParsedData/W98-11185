the history h (the token whose tag we are trying to determine) has an initial capitalized letter.
    Given a set of features and some training data, the maximum entropy estimation process produces a model in which every feature gi has associated with it a parameter ai.
    This allows us to compute the conditional probability as follows (Berger et al., 1996): The maximum entropy estimation technique guarantees that for every feature gi, the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus.
    In other words: Here P is an empirical probability and PmE is the probability assigned by the M.E. model.
    More complete discussions of M.E. as applied to computational linguistics, including a description of the M.E. estimation procedure can be found in (Berger et al., 1996) and (Della Pietra et al., 1995).
    The following are some additional references which are useful as introductions and examples of applications: (Ratnaparkhi, 1997b) (Ristad, 1.