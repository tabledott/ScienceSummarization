ons, this is not necessarily a concern.
    But for others, where space comes at a premium, obtaining the gains that come with a billion words of training data may not be viable without an effort made to compress information.
    In such cases, one could look at numerous methods for compressing data (e.g.
    Dagan and Engleson, 1995, Weng, et al, 1998).
  
  
    Voting has proven to be an effective technique for improving classifier accuracy for many applications, including part-of-speech tagging (van Halteren, et al, 1998), parsing (Henderson and Brill, 1999), and word sense disambiguation (Pederson, 2000).
    By training a set of classifiers on a single training corpus and then combining their outputs in classification, it is often possible to achieve a target accuracy with less labeled training data than would be needed if only one classifier was being used.
    Voting can be effective in reducing both the bias of a particular training corpus and the bias of a specific learner.
    When a training corpu