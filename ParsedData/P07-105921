 a cornhusk doll&#8221;, lem of the question-answer database.
    Evaluation was performed by manual labeling of top 20 answers retrieved for each of 60 queries for each system by two independent judges.
    For the sake of consistency, we chose not to use the assessments provided by Jijkoun and de Rijke.
    Instead, the judges were asked to find agreement on the examples on which they disagreed after each evaluation round.
    The ratings together with the question-answer pair id were stored and merged into the retrieval results for the next system evaluation.
    In this way consistency across system evaluations could be ensured, and the effort of manual labeling could be substantially reduced.
    The quality of retrieval results was assessed according to Jijkoun and de Rijke&#8217;s (2005) three point scale: The evaluation measure used in Jijkoun and de Rijke (2005) is the success rate at 10 or 20 answers, i.e., S20n is the percentage of queries with at least one adequate answer in the top n retrieved qu