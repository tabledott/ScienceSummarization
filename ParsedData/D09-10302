 included word sense disambiguation, word similarity, textual entailment, and temporal ordering of events, but not machine translation.
    Snow et al. measured the quality of non-expert annotations by comparing them against labels that had been previously created by expert annotators.
    They report inter-annotator agreement between expert and non-expert annotators, and show that the average of many non-experts converges on performance of a single expert for many of their tasks.
    Although it is not common for manual evaluation results to be reported in conference papers, several large-scale manual evaluations of machine translation quality take place annually.
    These include public forums like the NIST MT Evaluation Workshop, IWSLT and WMT, as well as the project-specific Go/No Go evaluations for the DARPA GALE program.
    Various types of human judgments are used.
    NIST collects 5-point fluency and adequacy scores (LDC, 2005), IWSLT and WMT collect relative rankings (Callison-Burch et al., 2008; 