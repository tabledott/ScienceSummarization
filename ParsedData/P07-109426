r transition probabilities.
    The problem of junk clusters in BHMM2 might be alleviated by using a non-uniform prior over the hyperparameters to encourage some degree of sparsity in all clusters.
  
  
    In this paper, we have demonstrated that, for a standard trigram HMM, taking a Bayesian approach to POS tagging dramatically improves performance over maximum-likelihood estimation.
    Integrating over possible parameter values leads to more robust solutions and allows the use of priors favoring sparse distributions.
    The Bayesian approach is particularly helpful when learning is less constrained, either because less data is available or because dictionary information is limited or absent.
    For knowledgefree clustering, our approach can also be extended through the use of infinite models so that the number of clusters need not be specified in advance.
    We hope that our success with POS tagging will inspire further research into Bayesian methods for other natural language learning tasks.
  

