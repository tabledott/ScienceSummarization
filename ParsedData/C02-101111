n D are independently generated from CcccP ?),~|( . In our implementation, we use an equivalent ??
			)~|(log)()~(logminarg ~ ~ ccPcfcP Cc E Cc ?
			(4) where 1??
			is an additional parameter used to emphasize the prior information.
			If we ignore the first term in Equation (4), then the use of one EM-NBC turns out to select the candidate whose frequency vector is the closest to the transformed vector D in terms of KL divergence (cf., Cover and Tomas 1991).
			EM-NBC-Ensemble To further improve performance, we use an ensemble (i.e., a linear combination) of EM-NBCs (EM-NBC-Ensemble), while the classifiers are constructed on the basis of the data in different contexts with different window sizes.
			More specifically, we calculate where s),1,(i, L=iD denotes the data in different contexts.
			3.3 Translation Selection -- EM-TF-IDF.
			We view the translation selection problem as that of calculating similarities between context vectors and use as context vectors TF-IDF vectors constructed with the EM Algorith