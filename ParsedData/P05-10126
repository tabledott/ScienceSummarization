licity.
    The feature weights in the weight vector w are the parameters that will be learned during training.
    Our training algorithms are iterative.
    We denote by w(i) the weight vector after the ith training iteration.
    Finally we define dt(x) as the set of possible dependency trees for the input sentence x and bestk(x; w) as the set of k dependency trees in dt(x) that are given the highest scores by weight vector w, with ties resolved by an arbitrary but fixed rule.
    Three basic questions must be answered for models of this form: how to find the dependency tree y with highest score for sentence x; how to learn an appropriate weight vector w from the training data; and finally, what feature representation f(i, j) should be used.
    The following sections address each of these questions.
    Given a feature representation for edges and a weight vector w, we seek the dependency tree or trees that maximize the score function, s(x, y).
    The primary difficulty is that for a given sentence of le