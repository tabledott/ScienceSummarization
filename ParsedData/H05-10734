t reveal emotion classification accuracy, nor how well the model generalizes on a large data set.
			Whereas work on emotion classification fromthe point of view of natural speech and human computer dialogues is fairly extensive, e.g.
			(Scherer,2003), (Litman and Forbes-Riley, 2004), this appears not to be the case for text-to-speech synthe sis (TTS).
			A short study by (Sugimoto et al, 2004) addresses sentence-level emotion recognition forJapanese TTS.
			Their model uses a composition as sumption: the emotion of a sentence is a function of the emotional affinity of the words in the sentence.
			They obtain emotional judgements of 73 adjectives and a set of sentences from 15 human subjects andcompute words?
			emotional strength based on the ra tio of times a word or a sentence was judged to fall into a particular emotion bucket, given the number of human subjects.
			Additionally, they conducted aninteractive experiment concerning the acoustic ren dering of emotion, using manual tuning of prosodicparamet