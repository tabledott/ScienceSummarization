 minimum-Bayes-risk decoding (Kumar and Byrne, 2004; Goodman, 1998),where, assuming f ? defines a probability distribution over all candi dates, one seeks the candidate with the highest expected score according to an arbitrary metric (e.g., PARSEVALor BLEU); since in general the metric will not be com patible with the parsing algorithm, the k-best lists canbe used to approximate the full distribution f ?.
			A simi lar situation occurs when the parser can produce multiple derivations that are regarded as equivalent (e.g., multiplelexicalized parse trees corresponding to the same unlexi calized parse tree); if we want the maximum a posteriori parse, we have to sum over equivalent derivations.
			Again,the equivalence relation will in general not be compati ble with the parsing algorithm, so the k-best lists can be used to approximate f ?, as in Data Oriented Parsing (Bod, 2000) and in speech recognition (Mohri and Riley, 2002).
			Another instance of this k-best approach is cascadedoptimization.
			NLP systems