 parameters.
    In their approach, parameters are estimated using a modified EM algorithm, where the E-step minimizes the KL-divergence between the model posterior and the set of distributions that satisfies the constraints.
    Our approach also expresses constraints as expectations on the posterior; we utilize the machinery of their framework within a variational inference algorithm with a mean field approximation.
    Generalized expectation criteria, another technique for declaratively specifying expectation constraints, has previously been successfully applied to the task of dependency parsing (Druck et al., 2009).
    This objective expresses constraints in the form of preferences over model expectations.
    The objective is penalized by the square distance between model expectations and the prespecified values of the expectation.
    This approach yields significant gains compared to a fully unsupervised counterpart.
    The constraints they studied are corpus- and languagespecific.
    Our work demo