 of , since the number of unique outcomes for the word-state for &amp;quot;come&amp;quot; would be two, and the total number of times &amp;quot;come&amp;quot; had been the preceding word in a bigram would be four (a 4 Any more levels of back-off might require a more sophisticated smoothing technique, such as deleted interpolation.
    No matter what smoothing technique is used, one must remember that smoothing is the art of estimating the probability of that which is unknown (i.e., not seen in training).
    Unlike a traditional HMM, the probability of generating a particular word is 1 for each word-state inside each of the name-class states.
    An alternative&#8212; and more traditional&#8212;model would have a small number of states within each name-class, each having, perhaps, some semantic signficance, e.g., three states in the PERSON name-class, representing a first, middle and last name, where each of these three states would have some probability associated with emitting any word from the vocabulary.
