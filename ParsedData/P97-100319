erest in using dependency-based parsing models in speech recognition, for example (Stolcke 96).
    It is interesting to note that Models 1, 2 or 3 could be used as language models.
    The probability for any sentence can be estimated as P(S) = ET P(T, s), or (making a Viterbi approximation for efficiency reasons) as P(S) P(Tbest, S).
    We intend to perform experiments to compare the perplexity of the various models, and a structurally similar 'pure' PCFG1&#176;.
  
  
    This paper has proposed a generative, lexicalised, probabilistic parsing model.
    We have shown that linguistically fundamental ideas, namely subcategorisation and wh-movement, can be given a statistical interpretation.
    This improves parsing performance, and, more importantly, adds useful information to the parser's output.
  
  
    I would like to thank Mitch Marcus, Jason Eisner, Dan Melamed and Adwait Ratnaparkhi for many useful discussions, and comments on earlier versions of this paper.
    This work has also benefited greatl