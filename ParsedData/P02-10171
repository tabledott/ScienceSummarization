archers have explored this problem for a variety of reasons: to argue empirically against the poverty of the stimulus (Clark, 2001), to use induction systems as a first stage in constructing large treebanks (van Zaanen, 2000), or to build better language models (Baker, 1979; Chen, 1995).
    In previous work, we presented a conditional model over trees which gave the best published results for unsupervised parsing of the ATIS corpus (Klein and Manning, 2001b).
    However, it suffered from several drawbacks, primarily stemming from the conditional model used for induction.
    Here, we improve on that model in several ways.
    First, we construct a generative model which utilizes the same features.
    Then, we extend the model to allow multiple constituent types and multiple prior distributions over trees.
    The new model gives a 13% reduction in parsing error on WSJ sentence experiments, including a positive qualitative shift in error types.
    Additionally, it produces much more stable results, does no