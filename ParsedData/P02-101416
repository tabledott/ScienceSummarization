 one sentence apart from each other.
    This rule covers 38 examples, but has 18 exceptions.
    In comparison, the Baseline system obtains much better precision on common nouns (i.e.
    53.3 for MUC-6/RIPPER and 61.0 for MUC7/RIPPER with lower recall in both cases) where the primary mechanism employed by the classifiers for common noun resolution is its high-precision string matching facility.
    Our results also suggest that data fragmentation is likely to have contributed to the drop in performance (i.e. we increased the number of features without increasing the size of the training set).
    For example, the decision tree induced from the MUC-6 data set using the Soon feature set (Learning Framework results) has 16 leaves, each of which contains 1728 instances on average; the tree induced from the same data set using all of the 53 features, on the other hand, has 86 leaves with an average of 322 instances per leaf.
    Hand-selected feature sets.
    As a result, we next evaluate a version of the syste