  The same conclusion is reached by Turney (2007), who uses essentially the same method (with some differences in implementation) to tackle the TOEFL task, and obtains more than 10% improvement in accuracy with respect to the corresponding raw tensor.
    At least as a trend, tensor decomposition appears to be better than matrix decomposition, but only marginally so (Turney does not perform this comparison).
    Still, even if the tensor- and matrix-based decompositions turned out to have comparable effects, tensor-based smoothing is more attractive in the DM framework because we could perform the decomposition once, and use the smoothed tensor as our stable underlying DM (modulo, of course, memory problems with computing such a large tensor decomposition).
    Beyond smoothing, tensor decomposition might provide some novel avenues for distributional semantics, while keeping to the DM program of a single model for many tasks.
    Van de Cruys (2009) used tensor decomposition to find commonalities in latent di