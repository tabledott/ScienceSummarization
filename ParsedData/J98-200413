gram model.
    (Technically, this is not a trigram model but a tritag model, since we are considering sequences of tags, not words.)
    Our tritag probabilities p(ta I ta-2, ta_i ) were learned from the training data used for the grammar, using Nonzero-length edges for 95% of the probability mass for the 0 estimates. the deleted interpolation method for smoothing.
    Our figure of merit uses: We refer to this figure of merit as the trigram estimate.
    The results for the three figures of merit introduced in the last section according to the measurements given in Section 2.2 are shown in Table 1 (the time to fully parse using the &amp;quot;stack&amp;quot; model is included for easy reference).
    Figure 4 expands the %non-0 E data to show the percent of nonzero-length edges needed to get 95% of the probability mass for each sentence length.
    Straight 13 performs quite poorly on this measure.
    In order to find 95% of the probability mass for a sentence, a parser using this figure of merit typically 