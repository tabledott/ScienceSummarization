tem actually does the best &#8211; even better than humans.
    The most likely reason for this is that our model returns longer sentences and is thus less likely to prune away important information.
    For example, consider the sentence The chemical etching process used for glare protection is effective and will help if your office has the fluorescent-light overkill that&#8217;s typical in offices The human compression was Glare protection is effective, whereas our model compressed the sentence to The chemical etching process used for glare protection is effective.
    A primary reason that our model does better than the decision tree model of Knight and Marcu is that on a handful of sentences, the decision tree compressions were a single word or noun-phrase.
    For such sentences the evaluators typically rated the compression a 1 for both grammaticality and importance.
    In contrast, our model never failed in such drastic ways and always output something reasonable.
    This is quantified in the standar