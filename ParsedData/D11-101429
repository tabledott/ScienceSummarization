is able to better generalize to novel combinations of previously seen constituents.
    One of the major shortcomings of previous applications of recursive autoencoders to natural language sentences was their binary word representation as discussed in Sec.
    2.1.
    Recently, (Socher et al., 2010; Socher et al., 2011) introduced a max-margin framework based on recursive neural networks (RNNs) for labeled structure prediction.
    Their models are applicable to natural language and computer vision tasks such as parsing or object detection.
    The current work is related in that it uses a recursive deep learning model.
    However, RNNs require labeled tree structures and use a supervised score at each node.
    Instead, RAEs learn hierarchical structures that are trying to capture as much of the the original word vectors as possible.
    The learned structures are not necessarily syntactically plausible but can capture more of the semantic content of the word vectors.
    Other recent deep learning methods