before.
    Again, because the total probability assigned to all translations for each source word was one, precision =recall = percent correct on this task.
    The mean scores over the five gold standard annotations are graphed in Figure 6, where the right edge of the figure corresponds to the means of Figure 5(a).
    The figure supports the hypothesis in Melamed (to appear, Chapter 7) that the biases presented in this article are even more valuable when the training data are more sparse.
    The one-to-one assumption is useful, even though it forces us to use a greedy approximation to maximum likelihood.
    In relative terms, the advantage of the one-to-one assumption is much more pronounced on smaller training sets.
    For example, Model A is 102% more accurate than Model 1 when trained on only 250 verse pairs.
    The explicit noise model buys a considerable gain in accuracy across all sizes of training data, as do the link classes of Model C. In concert, when trained and tested only on the gold stand