 only evaluate English sentences produced by translation systems by comparing them to English reference sentences)* This has the effect of reducing the Fmean by the maximum of 50% if there are no bigram or longer matches* For a single system translation, METEOR computes the above score for each reference translation, and then reports the best score as the score for the translation* The overall METEOR score for a system is calculated based on aggregate statistics accumulated over the entire test set, similarly to the way this is done in BLEU* We calculate aggregate precision, aggregate recall, an aggregate penalty, and then combine them using the same formula used for scoring individual segments*
  
  
    We evaluated the METEOR metric and compared its performance with BLEU and NIST on the DARPA/TIDES 2003 Arabic-to-English and Chinese-to-English MT evaluation data released through the LDC as a part of the workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, at the Annual Meeti