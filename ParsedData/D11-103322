 amounts of this pseudo in-domain data can prove more useful than the entire general-domain corpus for the purposes of domain-targeted translation tasks.
    This paper has also explored three simple yet effective methods for extracting these pseudo indomain sentences from a general-domain corpus.
    A translation model trained on any of these subcorpora can be comparable &#8211; or substantially better &#8211; than a translation system trained on the entire corpus.
    In particular, the new bilingual Moore-Lewis method, which is specifically tailored to the machine translation scenario, is shown to be more efficient and stable for MT domain adaptation.
    Translation models trained on data selected in this way consistently outperformed the general-domain baseline while using as few as 35k out of 12 million sentences.
    This fast and simple technique for discarding over 99% of the general-domain training corpus resulted in an increase of 1.8 BLEU points.
    We have also shown in passing that the linear 