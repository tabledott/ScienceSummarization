reate a document by concatenating 300 segments.
    This strategy is commonly used in MT evaluation, because of BLEU&#8217;s well-known problems with documents of small size (Papineni et al., 2002; Koehn, 2004).
    For each of the ten MT system translations, the evaluation metric score is calculated on the document and the corresponding human adequacy score is calculated as the average human score over the segments.
    The Pearson correlation is calculated over these ten pairs (Papineni et al., 2002; Stent et al., 2005).
    This process is repeated for ten different documents created by the same process.
    Finally, a paired t-test is calculated over these ten different correlation scores to compute statistical significance.
    Table 4 shows Pearson correlation scores for BLEU and the four paraphrased augmentations, averaged over ten runs.5 In all ten tests, our method based on contextual rewriting (ContextWN) improves the correlation with human scores over BLEU.
    Moreover, in nine out of ten tests Co