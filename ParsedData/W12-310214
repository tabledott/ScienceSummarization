use of a qualification.
    The Turkers completed paid tasks, and we used controls to filter out fraudulent and unconscientious workers.
    Agreement rates vary widely across languages.
    For inter-annotator agreements, the range is 0.176 to 0.336, while intra-annotator agreement ranges from 0.279 to 0.648.
    We note in particular the low agreement rates among judgments in the English-Spanish task, which is reflected in the relative lack of statistical significance Table 4.
    The agreement rates for this year were somewhat lower than last year.
    We used the results of the manual evaluation to analyze the translation quality of the different systems that were submitted to the workshop.
    In our analysis, we aimed to address the following questions: Table 4 shows the system ranking for each of the translation tasks.
    For each language pair, we define a system as &#8216;winning&#8217; if no other system was found statistically significantly better (using the Sign Test, at p &lt; 0.10).
    In some