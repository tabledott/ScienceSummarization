n this sample headline-annotation pair: For our experiment we select a 100-headline sample from the original SemEval test set, and collect 10 affect annotations for each of the seven label types, for a total of 7000 affect labels.
    We then performed two comparisons to evaluate the quality of the AMT annotations.
    First, we asked how well the non-experts agreed with the experts.
    We did this by comparing the interannotator agreement (ITA) of individual expert annotations to that of single non-expert and averaged non-expert annotations.
    In the original experiment ITA is measured by calculating the Pearson correlation of one annotator&#8217;s labels with the average of the labels of the other five annotators.
    For each expert labeler, we computed this ITA score of the expert against the other five; we then average these ITA scores across all expert annotators to compute the average expert ITA (reported in Table 1 as &#8220;E vs. E&#8221;.
    We then do the same for individual non-expert annotati