 0.001) according to Dan Bikel&#8217;s parse comparison script and using the Sign test (p &lt; 0.001).
    Now we give a little insight into how our results compare with the rest of the community.
    The reported state-of-the-art parser of Malouf and van Noord (2004) achieves 84.4% labelled accuracy which is very close numerically to our baseline.
    However, they use a subset of the CoNLL Alpino treebank with a higher average number of tokens per sentences and also evaluate control relations, thus results are not directly comparable.
    We have also run our parser on the relatively small (approximately 400 sentences) CoNNL test data.
    The best performing system (McDonald et al. 2006; note: this system is different to our baseline) achieves 79.2% labelled accuracy while our baseline system achieves 78.6% and our constrained version 79.8%.
    However, a significant difference is only observed between our baseline and our constraintbased system.
    Examining the errors produced using the dev set highlig