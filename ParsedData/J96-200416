f coders.
    This variation could be achieved by interpreting P(A) as the proportion of times that the naive coders agree with the expert and P(E) as the proportion of times we would expect the naive coders to agree with the expert by chance.
  
  
    We have shown that existing measures of reliability in discourse and dialogue work are difficult to interpret, and we have suggested a replacement measure, the kappa statistic, which has a number of advantages over these measures.
    Kappa is widely accepted in the field of content analysis.
    It is interpretable, allows different results to be compared, and suggests a set of diagnostics in cases where the reliability results are not good enough for the required purpose.
    We suggest that this measure be adopted more widely within our own research community.
  
  
    This work was supported by grant number Human-Computer Interaction and an G9111013 of the U.K. Joint Councils Interdisciplinary Research Centre Grant
  

