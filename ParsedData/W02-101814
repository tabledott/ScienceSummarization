g, we apply smoothing so we can associate non-zero values to phrase-pairs that do not occur often in the corpus.
    3.4 Derivation of conditional probability model At the end of the training procedure, we take marginals on the joint probability distributionsand .
    This yields conditional probability distributions and which we use for decoding.
    3.5 Discussion When we run the training procedure in Figure 2 on the corpus in Figure 1, after four Model 1 iterations we obtain the alignments in Figure 1.d and the joint and conditional probability distributions shown in Figure 1.e.
    At prima facie, the Viterbi alignment for the first sentence pair appears incorrect because we, as humans, have a natural tendency to build alignments between the smallest phrases possible.
    However, note that the choice made by our model is quite reasonable.
    After all, in the absence of additional information, the model can either assume that &#8220;a&#8221; and &#8220;y&#8221; mean the same thing or that phrases &#8220