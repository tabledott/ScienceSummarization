distribution that is closest to the distribution of labels that people would assign to a story, we evaluate using KL divergence: KL(g||p) = Ei gi log(gi/pi), where g is the gold distribution and p is the predicted one.
    We report the average KL divergence, where a smaller value indicates better predictive power.
    To get an idea of the values of KL divergence, predicting random distributions gives a an average of 1.2 in KL divergence, predicting simply the average distribution in the training data give 0.83.
    Fig.
    4 shows that our RAE-based model outperforms the other baselines.
    Table 2 shows EP example entries with predicted and gold distributions, as well as numbers of votes.
    In order to compare our approach to other methods we also show results on commonly used sentiment datasets: movie reviews4 (MR) (Pang and Lee, 2005) and opinions5 (MPQA) (Wiebe et al., 2005).We give statistical information on these and the EP corpus in Table 1.
    We compare to the state-of-the-art system of (Nakag