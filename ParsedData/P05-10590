
  Stochastic Lexicalized Inversion Transduction Grammar For Alignment
  
    We present a version of Inversion Transduction Grammar where rule probabilities are lexicalized throughout the synchronous parse tree, along with pruning techniques for efficient training.
    Alignment results improve over unlexicalized ITG on short sentences for which full EM is feasible, but pruning seems to have a negative impact on longer sentences.
  
  
    The Inversion Transduction Grammar (ITG) of Wu (1997) is a syntactically motivated algorithm for producing word-level alignments of pairs of translationally equivalent sentences in two languages.
    The algorithm builds a synchronous parse tree for both sentences, and assumes that the trees have the same underlying structure but that the ordering of constituents may differ in the two languages.
    This probabilistic, syntax-based approach has inspired much subsequent reasearch.
    Alshawi et al. (2000) use hierarchical finite-state transducers.
    In the tree-to-string