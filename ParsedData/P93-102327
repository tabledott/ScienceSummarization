dard deviation of 1.74).
    Note that this evaluation method differs significantly from the alternative method of asking the humans to directly estimate the goodness of the system's results (e.g.
    (Matsukawa, 1993)).
    It requires an explicit construction of a model from the human judge and places the burden of the comparison between the model and the system's output on the system instead of the judge.
    It has been repeatedly demonstrated that in complex evaluation tasks humans can easily find arguments to support observed data, leading to biased results and to an inflation of the evaluation scores.
    To score our results, we converted the comparison of two partitions to a series of yes-no questions, each of which has a correct answer (as dictated by the model) and an answer assigned by the system.
    For each pair of adjectives, we asked if they fell in the same cluster (&amp;quot;yes&amp;quot;) or not (&amp;quot;no&amp;quot;).
    Since human judges did not always agree, we used fractional value