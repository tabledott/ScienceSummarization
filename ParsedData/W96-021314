m Entropy (MaxEnt) tagger presented in this paper combines the advantages of all these methods.
    It uses a rich feature representation, like TBL and SDT, and generates a tag probability distribution for each word, like Decision Tree and Markov Model techniques.
    (Weischedel et al., 1993) provide the results from a battery of &amp;quot;tri-tag&amp;quot; Markov Model experiments, in which the probability P(W,T) of observing a word sequence W = {wi, w2, &#8226; &#8226; &#8226; , wn} together with a tag sequence T = is given by: Furthermore, p(wiiti) for unknown words is computed by the following heuristic, which uses a set of 35 pre-determined endings: This approximation works as well as the MaxEnt model, giving 85% unknown word accuracy(Weischedel et al., 1993) on the Wall St. Journal, but cannot be generalized to handle more diverse information sources.
    Multiplying together all the probabilities becomes less convincing of an approximation as the information sources become less independent.
    In con