a) with respect to this one parameter.
    All other parameter values are left fixed.
    Return to Step 2.
    The difference with this latter &#8220;boosting&#8221;approach is that in Step 3, only one parameter value is adjusted, namely, the parameter corresponding to the newly chosen feature.
    Note that in this framework, the same feature may be chosen at more than one iteration.5 The maximum-entropy feature selection method can be quite inefficient, as the entire model is updated at each step.
    For example, Ratnaparkhi (1998) quotes times of around 30 hours for 500 rounds of feature selection on a prepositionalphrase attachment task.
    These experiments were performed in 1998, when processors were no doubt considerably slower than those available today.
    However, the PP attachment task is much smaller than the parsing task that we are addressing: Our task involves around 1,000,000 examples, with perhaps a few hundred features per example, and 100,000 rounds of feature selection; this compares t