iii) trials to examine the effect of beam variation on these performance measures.
    Before presenting the results, we will introduce the methods of evaluation.
    Perplexity is a standard measure within the speech recognition community for comparing language models.
    In principle, if two models are tested on the same test corpus, the model that assigns the lower perplexity to the test corpus is the model closest to the true distribution of the language, and thus better as a prior model for speech recognition.
    Perplexity is the exponential of the cross entropy, which we will define next.
    Given a random variable X with distribution p and a probability model q, the cross entropy, H(p, q) is defined as follows: Let p be the true distribution of the language.
    Then, under certain assumptions, given a large enough sample, the sample mean of the negative log probability of a model will converge to its cross entropy with the true mode1.14 That is where w'ol is a string of the language L. In practice