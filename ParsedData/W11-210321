 likely to find statistically significant differences between the systems in the same groups.
    To that same end, we also eliminated the editing/acceptability task that was featured in last year&#8217;s evaluation, instead we had annotators focus solely on the system ranking task.
    Ranking translations relative to each other is a reasonably intuitive task.
    We therefore kept the instructions simple: You are shown a source sentence followed by several candidate translations.
    Your task is to rank the translations from best to worst (ties are allowed).
    Each screen for this task involved judging translations of three consecutive source segments.
    For each source segment, the annotator was shown the outputs of five submissions, and asked to rank them.
    With the exception of a few tasks in the system combination track, there were many more than 5 systems participating in any given task&#8212;up to 23 for the English-German individual systems track.
    Rather than attempting to get a complete 