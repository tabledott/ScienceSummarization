the a priori probability estimates bp(&#969;i) of the new data set that will maximize the likelihood of (3) with respect to p(&#969;i), we can apply the iterative procedure of the EM algorithm.
    In effect, through maximizing the likelihood of (3), we obtain the a priori probability estimates as a by-product.
    Let us now define some notations.
    When we apply a classifier trained on DL on an instance xk drawn from the new data set DU, we get bpL(&#969;i|xk), which we define as the probability of instance xk being classified as class &#969;i by the classifier trained on DL.
    Further, let us define bpL(&#969;i) as the a priori probability of class &#969;i in DL.
    This can be estimated by the class frequency of &#969;i in DL.
    We also define bp(s)(&#969;i) and bp(s)(&#969;i|xk) as estimates of the new a priori and a posteriori probabilities at step s of the iterative EM procedure.
    Assuming we initialize bp(&#65533;)(&#969;i) = bpL(&#969;i), then for each instance xk in DU and each class &#969