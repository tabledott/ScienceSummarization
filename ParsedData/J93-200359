hese alignments have a probability much smaller than that of the Viterbi alignment.
    The column headed Alignments in Table 1 shows the average number of alignments for which the probability is within a factor of 25 of the probability of the Viterbi alignment in each iteration.
    As this number drops, the model concentrates more and more probability onto fewer and fewer alignments so that the Viterbi alignment becomes ever more dominant.
    The last column in the table shows the perplexity of the French text given the English text for the In model of the iteration.
    We expect the likelihood of the training data to increase with each iteration.
    We can think of this likelihood as arising from a product of factors, one for each French word in the training data.
    We have 28,850, 104 French words in our training data, so the 28,850, 104th root of the likelihood is the average factor by which the likelihood is reduced for each additional French word.
    The reciprocal of this root is the perplexity 