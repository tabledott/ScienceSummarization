i, 1996) as a preprocessing stage may compensate for its smaller vocabulary.
    Also, the main reason for using a smaller vocabulary is the computational complexity of computing probabilities for the shift(wi) actions on-line, which other models do not require.
  
  
    Johnson (2001) investigated similar issues for parsing and tagging.
    His maximal conditional likelihood estimate for a PCFG takes the same approach as our generative model trained with a discriminative criteria.
    While he shows a non-significant increase in performance over the standard maximal joint likelihood estimate on a small dataset, because he did not have a computationally efficient way to train this model, he was not able to test it on the standard datasets.
    The other models he investigates conflate changes in the probability models with changes in the training criteria, and the discriminative probability models do worse.
    In the context of part-of-speech tagging, Klein and Manning (2002) argue for the same distinctions