
			b? c?c (!,f 1 ,a 1): X a Y b b?
			a? c?c (!,f 2 ,a 2): Figure 3: Example corpus.
			from pi.
			A simple example suffices to demonstrate it is not tight without normalization.
			Figure 3 contains a sample corpus from which four rules can be extracted: r1: X(a, Y(b, c)) ? a?, b?, c?
			r2: X(a, Y(b, c)) ? b?, a?, c?
			r3: X(a, x0:Y) ? a?, x0 r4: Y(b, c) ? b?, c?
			From Equation 4, the probabilities of r3 and r4 must be 1, and those of r1 and r2 must sum to 1.
			Thus, the total probability mass, which is dis-.
			tributed across two possible output strings a?b?c? and b?a?c?, is: p(a?b?c?|pi) + p(b?a?c?|pi) = p1 + p3 ? p4 + p2 = 2, where pi = p(rhs(ri)|lhs(ri)).It is relatively easy to prove that the probabil ities of all derivations that correspond to a givendecomposition ?i ? ?
			sum to 1 (the proof is omitted due to constraints on space).
			From this prop erty we can immediately conclude that the model described by Equation 3 is tight.5 We examine two estimates p(rhs(r)|lhs(r)).
			The first one is