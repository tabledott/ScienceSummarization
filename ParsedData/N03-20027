 were applied to two corpora and their perplexity was compared with standard optimized vanilla biand trigram language models.
    In the following, we consider as a &#8220;bigram&#8221; a language model with a temporal history that includes information from no longer than one previous time-step into the past.
    Therefore, if factors are deterministically derivable from words, a &#8220;bigram&#8221; might include both the previous words and previous factors as a history.
    From a decoding state-space perspective, any such bigram would be relatively cheap.
    In CallHome-Arabic, words are accompanied with deterministically derived factors: morphological class (M), stems (S), roots (R), and patterns (P).
    Training data consisted of official training portions of the LDC CallHome ECA corpus plus the CallHome ECA supplement (100 conversations).
    For testing we used the official 1996 evaluation set.
    Results are given in Table 1 and show perplexity for: 1) the baseline 3-gram; 2) a FLM 3-gram using mor