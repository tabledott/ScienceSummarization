rs to all variables other than si.
    Eventually, the distribution over samples drawn from this process will converge to the f p(fi |l, S, 0) &#183; p(O |f-i, Rt)dO _ #(fi, si) + Rl For the likelihood term p(fi|s, f &#8722;i,R), integrating over all possible values of the multinomial featuresense distribution &#65533; gives us the rightmost term in Equation 3, which has an intuitive interpretation.
    The term #(fi,si) indicates the number of times the feature-value fi was assigned sense si in the rest of the data.
    Similarly, #(si) indicates the number of times the sense assignment si was observed in the data.
    Rl is the Dirichlet prior for the featuresense distribution &#65533; in the current layer l, and Vl is the size of the vocabulary of that layer, i.e., the number of possible feature values in the layer.
    Intuitively, the probability of a feature-value given a sense is directly proportional to the number of times we have seen that value and that senseassignment together in the data, taking i