uting the probability of the test tuples given the training tuples and inferring latent topic assignments for test documents.
    These tasks can either be accomplished by averaging over samples of &#934;1, .
    .
    .
    , &#934;L and &#945;m from P(&#934;1, ... , &#934;L, &#945;m  |W', &#946;) or by evaluating a point estimate.
    We take the latter approach, and use the MAP estimate for &#945;m and the predictive distributions over words for &#934;1, .
    .
    .
    , &#934;L.The probability of held-out document tuples W' given training tuples W is then approximated by Topic assignments for a test document tuple sampling.
    Gibbs sampling involves sequentially resampling each zln from its conditional posterior: where z\l,n is the current set of topic assignments for all other tokens in the tuple, while (Nt)\l,n is the number of occurrences of topic t in the tuple, excluding zln, the variable being resampled.
  
  
    Our first set of experiments focuses on document tuples that are known to consist