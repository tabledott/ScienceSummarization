 transition probabilities.
    By iterating the algorithm with the same corpus, the parameters of the model can be made to converge on values which are locally optimal for the given text.
    The degree of convergence can be measured using a perplexity measure, the sum of plog2p for hypothesis probabilities p, which gives an estimate of the degree of disorder in the model.
    The algorithm is again described by Cutting et ad. and by Sharman, and a mathematical justification for it can be found in Huang et at.
    (1990).
    The first major use of HMMs for part of speech tagging was in CLAWS (Garside et a/., 1987) in the 1970s.
    With the availability of large corpora and fast computers, there has been a recent resurgence of interest, and a number of variations on and alternatives to the FB, Viterbi and BW algorithms have been tried; see the work of, for example, Church (Church, 1988), Brill (Brill and Marcus, 1992; Brill, 1992), DeRose (DeRose, 1988) and Kupiec (Kupiec, 1992).
    One of the most effectiv