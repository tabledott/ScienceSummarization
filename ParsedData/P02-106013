discarded.
  
  
    Given the model in section 2 and word feature in section 3, the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate.
    Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data, especially considering the complex word feature described above.
    In order to resolve the sparseness problem, two levels of back-off modeling are applied to approximate ( / 1 ) 1) First level back-off scheme is based on different contexts of word features and words themselves, and n descending order of fi &#8722;2 fi &#8722;1 fi wi , fi w ifi+1fi+2 , fi&#8722;1fiwi , fiwifi+1 , f i &#8722; 1 wi&#8722; 1 f i , fifi+1wi+1 , fi&#8722;2fi&#8722;1 f i , f i f i +1 f i+2 , fi wi , fi &#8722;2fi &#8722;1fi , fifi +1 and fi .
    2) The second level back-off scheme is based on different combinations of the four sub-features described in section 3, and fk is approximated in the descending order of 12 3 