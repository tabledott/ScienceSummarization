.
    IBM Model 5 was sequentially bootstrapped with Model 1, an HMM Model, and Model 3 (Och and Ney, 2000).
    Each preceding model and the final Model 5 were trained with five iterations (total 20 iterations).
    The training procedure resulted in the tables of estimated model parameters.
    Table 1 in Section 2.1 shows part of those parameters obtained by the training above.
    To evaluate performance, we let the models generate the most probable alignment of the training corpus (called the Viterbi alignment).
    The alignment shows how the learned model induces the internal structure of the training data.
    Figure 2 shows alignments produced by our model and IBM Model 5.
    Darker lines indicates that the particular alignment link was judged correct by humans.
    Three humans were asked to rate each alignment as okay (1.0 point), not sure (0.5 point), or wrong (0 point).
    The darkness of the lines in the figure reflects the human score.
    We obtained the average score of the first 50 sentenc