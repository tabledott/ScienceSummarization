s before moving to the specific application to geographical variation.
    Cascading topic models generate text from a chain of random variables.
    Each element in the chain defines a distribution over words, and acts as the mean of the distribution over the subsequent element in the chain.
    Thus, each element in the chain can be thought of as introducing some additional corruption.
    All words are drawn from the final distribution in the chain.
    At the beginning of the chain are the priors, followed by unadulerated base topics, which may then be corrupted by other factors (such as geography or time).
    For example, consider a base &#8220;food&#8221; topic that emphasizes words like dinner and delicious; the corrupted &#8220;food-California&#8221; topic would place weight on these words, but might place extra emphasis on other words like sprouts.
    The path through the cascade is determined by a set of indexing variables, which may be hidden or observed.
    As in standard latent Dirichlet alloc