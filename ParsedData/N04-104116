 between a set of judges' assessments correcting for chance agreements: where P(A) is the probability of agreement between the judges and P(E) is the probability that the judges agree by chance on an assessment.
    An experiment with K &#8805; 0.8 is generally viewed as reliable and 0.67 &lt; K &lt; 0.8 allows tentative conclusions.
    The Kappa statistic for our experiment is K = 0.72.
    The human labeling is at a disadvantage since only one label was generated per concept.
    Therefore, the human scores either 1 or 0 for each concept.
    Our system's highest ranking name was correct 72% of the time.
    Table 4 shows the percentage of semantic classes with a correct label in the top 1-5 ranks returned by our system.
    Overall, 41.8% of the top-5 names extracted by our system were judged correct.
    The overall accuracy for the top-4, top-3, top-2, and top-1 names are 44.4%, 48.8%, 58.5%, and 72% respectively.
    Hence, the name ranking of our algorithm is effective.
    The 1432 CBC concepts conta