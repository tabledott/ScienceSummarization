e structural ambiguities that are difficult to learn from data alone &#8212; for example, our rules prefer analyses in which verbs are dependents of auxiliaries, even though analyzing auxiliaries as dependents of verbs is also consistent with the data.
    Leveraging these universal rules has the potential to improve parsing performance for a large number of human languages; this is particularly relevant to the processing of low-resource languages.
    Furthermore, these universal rules are compact and well-understood, making them easy to manually construct.
    In addition to these universal dependencies, each specific language typically possesses its own idiosyncratic set of dependencies.
    We address this challenge by requiring the universal constraints to only hold in expectation rather than absolutely, i.e., we permit a certain number of violations of the constraints.
    We formulate a generative Bayesian model that explains the observed data while accounting for declarative linguistic rules during in