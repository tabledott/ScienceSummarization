i = 1, ...M} is written L?
			= ? i logP?(yi|xi) = ? i ( T?
			t=1 ? k ?kfk(yt?1, yt,x, t)?
			logZxi ) . Traditional maximum entropy learning algorithms, such as GIS and IIS (della Pietra et al, 1995), canbe used to train CRFs.
			However, our implemen tation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (Malouf, 2002; Sha and Pereira, 2003).
			The gradient of the likelihood is ?P?(y|x)/??k = ? i,t fk(yt?1, y(i)t ,x(i), t) ? ?
			i,y,t P?(y|x(i))fk(yt?1, yt,x(i), t) CRFs share many of the advantageous properties of standard maximum entropy classifiers, including their convex likelihood function, which guarantees that the learning procedure converges to the global maximum.
			2.1 Regularization in CRFs.
			To avoid over-fitting, log-likelihood is usually penalized by some prior distribution over the parameters.
			A commonly used prior is a zero-mean Gaussian.
			With a Gaussian prior, log-likelihood is penal ized as follows.
			L? = ? i logP?(yi|xi)