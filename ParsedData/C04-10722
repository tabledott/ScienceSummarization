f-words instead.
			Turian et al (2003) introduced General Text Matcher (GTM) based on accuracy measures such as recall, precision, and F-measure.
			With so many different automatic metrics available, it is necessary to have a common and objective way to evaluate these metrics.
			Comparison of automatic evaluation metrics are usually conducted on corpus level using correlation analysis between human scores and automatic scores such as BLEU, NIST, WER, and PER.
			However, the performance of automatic metrics in terms of human vs. system correlation analysis is not stable across different evaluation settings.
			For example, Table 1 shows the Pearson?s linear correlation coefficient analysis of 8 machine translation systems from 2003 NIST Chinese English machine translation evaluation.
			The Pearson?
			correlation coefficients are computed according to different automatic evaluation methods vs. human assigned adequacy and fluency.
			BLEU1, 4, and 12 are BLEU with maximum n-gram lengths of 1, 4, and 12 res