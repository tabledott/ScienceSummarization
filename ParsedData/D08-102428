 phrase-based approach.
    By capturing how reordering depends on constituent length, these features improve translation quality significantly.
    In sum, we have shown that removing the bottleneck of MERT opens the door to many possibilities for better translation.
  
  
    Thanks to Michael Bloodgood for performing initial simulations of parallelized perceptron training.
    Thanks also to John DeNero, Kevin Knight, Daniel Marcu, and Fei Sha for valuable discussions and suggestions.
    This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies and HR0011-06-02-001 under subcontract to IBM.
  

