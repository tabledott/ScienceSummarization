 labelers may contribute to a single set of non-expert annotations, the annotator diversity within the single set of labels may have the effect of reducing annotator bias and thus increasing system performance.
  
  
    We demonstrate the effectiveness of using Amazon Mechanical Turk for a variety of natural language annotation tasks.
    Our evaluation of non-expert labeler data vs. expert annotations for five tasks found that for many tasks only a small number of nonexpert annotations per item are necessary to equal the performance of an expert annotator.
    In a detailed study of expert and non-expert agreement for an affect recognition task we find that we require an average of 4 non-expert labels per item in order to emulate expert-level label quality.
    Finally, we demonstrate significant improvement by controlling for labeler bias.
  
  
    Thanks to Nathanael Chambers, Annie Zaenen, Rada Mihalcea, Qi Su, Panos Ipeirotis, Bob Carpenter, David Vickrey, William Morgan, and Lukas Biewald for useful d