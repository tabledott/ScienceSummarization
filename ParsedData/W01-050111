strates a few instances taken from the example sentence.
    We chose naive bayes classifiers for the study, first, because they are convenient to use and, indeed, have been used in previous co-training studies; and second, because they are particularly well-suited to co-training by virtue of calculating probabilities for each prediction.
    For an instance x, the classifier determines the maximum a posteriori label as follows.
    In experiments with these naive bayes JOB classifiers, we found that very little accuracy was sacrificed when the word information (i.e. wi) was ignored by the classifier.2 We therefore substitute the simpler term P(ti 1/) for P(wiltill) above.
    The probabilities P(ti 1/) are estimated from the training data by determining the fraction of the instances labeled 1 that have syntactic Here N(x) denotes the frequency of event x in the training data.
    This estimate smoothes the training probability by including virtual (unseen) samples for each part-of-speech tag (of which there 