into the hypothesis-search algorithm rather than a follow-on re-scoring phase.While it is generally recognized that two-pass de coding can be very effective in practice, single-pass decoding remains conceptually attractive because it eliminates a source of potential information loss.
	
	
			Traditionally, statistical language models have been designed to assign probabilities to strings of words (or tokens, which may include punctuation, etc.).
			Let wL1 = (w1, . . .
			, wL) denote a string of L tokens over a fixed vocabulary.
			An n-gram language model assigns a probability to wL1 according to P (wL1 ) = L ? i=1 P (wi|wi?11 ) ? L ? i=1 P?
			(wi|wi?1i?n+1) (2)where the approximation reflects a Markov assumption that only the most recent n ? 1 tokens are rele vant when predicting the next word.
			858 For any substring wji of wL1 , let f(w j i ) denote the frequency of occurrence of that substring in another given, fixed, usually very long target-language string called the training data.
			The maximum-like