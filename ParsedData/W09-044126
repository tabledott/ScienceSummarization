erate a more fluent translation although at the potential loss of meaning.
  
  
    We introduced a new evaluation metric, TER-Plus, and showed that it is competitive with state-of-theart evaluation metrics when its predictions are correlated with human judgments.
    The inclusion of stem, synonym and paraphrase edits allows TERp to overcome some of the weaknesses of the TER metric and better align hypothesized translations with reference translations.
    These new edit costs can then be optimized to allow better correlation with human judgments.
    In addition, we have examined the use of other paraphrasing techniques, and shown that the paraphrase probabilities estimated by the pivot-method may not be fully adequate for judgments of whether a paraphrase in a translation indicates a correct translation.
    This line of research holds promise as an external evaluation method of various paraphrasing methods.
    However promising correlation results for an evaluation metric may be, the evaluation of the f