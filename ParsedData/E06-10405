own that some of these metrics are highly correlated with human judgments.
    The BLEU metric (Papineni et al., 2002) in MT has been particularly successful; for example MT05, the 2005 NIST MT evaluation exercise, used BLEU-4 as the only method of evaluation.
    BLEU is a precision metric that assesses the quality of a translation in terms of the proportion of its word ngrams (n = 4 has become standard) that it shares with one or more high-quality reference translations.
    BLEU scores range from 0 to 1, 1 being the highest which can only be achieved by a translation if all its substrings can be found in one of the reference texts (hence a reference text will always score 1).
    BLEU should be calculated on a large test set with several reference translations (four appears to be standard in MT).
    Properly calculated BLEU scores have been shown to correlate reliably with human judgments (Papineni et al., 2002).
    The NIST MT evaluation metric (Doddington, 2002) is an adaptation of BLEU, but where BLEU