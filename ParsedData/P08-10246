the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets.
    Our model addresses all three of the above problems within a global model, without resorting to nbest lists or local independence assumptions.
    Furthermore, our model explicitly accounts for spurious ambiguity without altering the model structure or arbitrarily selecting one derivation.
    Instead we model the translation distribution with a latent variable for the derivation, which we marginalise out in training and decoding. and X, which performs the transduction: il ne vas pas =:&gt;. he does not go This itself provides robustness to noisy data, in addition to the explicit regularisation from a prior over the model parameters.
    For example, in many cases there is no one perfect derivation, but rather many imperfect ones which each include some good translation fragments.
    The model can learn from many of these derivations 