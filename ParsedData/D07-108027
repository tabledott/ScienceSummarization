ann and Zhang (2006) and Lianget al (2006).
			In their approach, training was per formed on a large corpus using the sparse features ofphrase translation pairs, target n-grams and/or bag of-word pairs inside phrases.
			In Tillmann and Zhang 4We split data by document, not by sentence.
			(2006), k-best list generation is approximated by a step-by-step one-best merging method that separates the decoding and training steps.
			The weight vector update scheme is very similar to MIRA but basedon a convex loss function.
			Our method directly em ploys the k-best list generated by the fast decoding method (Watanabe et al, 2006b) at every iteration.One of the benefits is that we avoid the rather expen sive cost of merging the k-best list especially when handling millions of features.Liang et al (2006) employed an averaged percep tron algorithm.
			They decoded each training instance and performed a perceptron update to the weight vector.
			An incorrect translation was updated towardan oracle translation found in 