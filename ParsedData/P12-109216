SA Wiki.
			75 Tiered Pruned tf-idf Wiki.
			76.9 Table 3: Spearman?s ? correlation on WordSim-353, showing our model?s improvement over previous neural models for learning word embeddings.
			C&amp;W* is the word embeddings trained and provided by C&amp;W. OurModel* is trained without stop words, while Our Model g uses only global context.
			Pruned tf-idf (Reisinger and Mooney, 2010b) and ESA (Gabrilovich and Markovitch, 2007) are also included.
			our re-implementation of C&amp;W?s model trained onWikipedia, showing the large effect of using a dif ferent corpus.
			Our model is able to learn more semantic word embeddings and noticeably improves upon C&amp;W?smodel.
			Note that our model achieves higher corre lation (64.2) than either using local context alone (C&amp;W: 55.3) or using global context alone (Our Model-g: 22.8).
			We also found that correlation can be further improved by removing stop words (71.3).Thus, each window of text (training example) contains more information but still preserves some