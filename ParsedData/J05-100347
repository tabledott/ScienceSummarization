 for ExpLoss.
    We use an array of values to indicate the gain of each feature (i.e., the impact that choosing this feature will have on the ExpLoss function).
    The features are ranked by this quantity.
    It can be seen that almost all of the computation involves the calculation of Z and W&#254;k and Wk for each feature hk.
    Once these values have been computed, the optimal feature and its update can be chosen.
    4.5 A New, More Efficient Algorithm for ExpLoss This section presents a new algorithm which is equivalent to the ExpLoss algorithm in Figure 3, but can be vastly more efficient for problems with sparse feature spaces.
    In the experimental section of this article we show that it is almost 2,700 times more efficient for our task than the algorithm in Figure 3.
    The efficiency of the different algorithms is important in the parsing problem.
    The training data we eventually used contained around 36,000 sentences, with an average of 27 parses per sentence, giving around 1,000,000 pars