set contains 945 documents, and approximately 203,000 tokens.
    The development set has 216 documents and approximately 51,000 tokens, and the test set has 231 documents and approximately 46,000 tokens.
    We evaluate performance on this task in the manner dictated by the competition so that results can be properly compared.
    Precision and recall are evaluated on a per-entity basis (and combined into an F1 score).
    There is no partial credit; an incorrect entity boundary is penalized as both a false positive and as a false negative.
    This dataset was developed as part of Dayne Freitag&#8217;s dissertation research Freitag (1998).5 It consists of 485 emails containing seminar announcements at Carnegie Mellon University.
    It is annotated for four fields: speaker, location, start time, and end time.
    Sutton and McCallum (2004) used 5-fold cross validation when evaluating on this dataset, so we obtained and used their data splits, so that results can be properly compared.
    Because the entire 