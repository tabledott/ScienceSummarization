antial linguistic patterns from surface yields alone does speak to the strength of support for these patterns in the data, and hence undermines arguments based on &#8220;the poverty of the stimulus&#8221; (Chomsky, 1965).
  
  
    Most recent progress in unsupervised parsing has come from tree or phrase-structure grammar based models (Clark, 2001; Klein and Manning, 2002), but there are compelling reasons to reconsider unsupervised dependency parsing.
    First, most state-ofthe-art supervised parsers make use of specific lexical information in addition to word-class level information &#8211; perhaps lexical information could be a useful source of information for unsupervised methods.
    Second, a central motivation for using tree structures in computational linguistics is to enable the extraction of dependencies &#8211; function-argument and modification structures &#8211; and it might be more advantageous to induce such structures directly.
    Third, as we show below, for languages such as Chinese, which