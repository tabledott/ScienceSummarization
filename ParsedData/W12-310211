wn from ones completed by the same annotator and from different annotators.
    We measured pairwise agreement among annotators using Cohen&#8217;s kappa coefficient (n) (Cohen, 1960), which is defined as where P(A) is the proportion of times that the annotators agree, and P(E) is the proportion of time that they would agree by chance.
    Note that n is basically a normalized version of P(A), one which takes into account how meaningful it is for annotators to agree with each other, by incorporating P(E).
    Note also that n has a value of at most 1 (and could possibly be negative), with higher rates of agreement resulting in higher n. We calculate P(A) by examining all pairs of systems which had been judged by two or more judges, and calculating the proportion of time that they agreed that A &gt; B, A = B, or A &lt; B.
    In other words, P(A) is the empirical, observed rate at which annotators agree, in the context of pairwise comparisons.
    P(A) is computed similarly for intraannotator agreement (i.e. s