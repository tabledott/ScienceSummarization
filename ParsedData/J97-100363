ignificantly.
    They suggest using the kappa coefficient (K) for this purpose.
    According to Carletta (1996), K measures pairwise agreement among a set of coders making category judgments, correcting for expected chance agreement as follows: where P(A) is the proportion of times that the coders agree and P(E) is the proportion of times that they would be expected to agree by chance.
    The coefficient can be computed by making pairwise comparisons against an expert or by comparing to a group decision.
    Carletta (1996) also states that in the behavioral sciences, K&gt; .8 signals good replicability, and .67 &lt; K &lt; .8 allows tentative conclusions to be drawn.
    The kappa coefficients found in Isard and Carletta (1995) ranged from .43 to .68 for four coders placing transaction boundaries, and those found in (Rose 1995) ranged from .65 to .90 for four coders segmenting sentences.
    Carletta cautions, however, that &amp;quot;... coding discourse and dialogue phenomena, and especially coding segme