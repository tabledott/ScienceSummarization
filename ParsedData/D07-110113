gher-order model for each language, using the averaged perceptron.
			In the experiments presented above we observed that the algorithmdoes not over-fit, and that after two or three train ing epochs only small variations in accuracy occur.
			Based on this fact, we designed a criterion to train models: we ran the training algorithm for up to three training test sent./min.
			mem.
			UAS LAS Arabic 1.21 1.8GB 81.48 70.20 Basque 33.15 1.2GB 81.08 75.73 Catalan 5.50 1.7GB 92.46 87.60 Chinese 1461.66 60MB 86.20 80.86 Czech 18.19 1.8GB 85.16 78.60 English 15.57 1.0GB 90.63 89.61 Greek 8.10 250MB 81.37 73.56 Hungarian 5.65 1.6GB 79.92 75.42 Italian 12.44 900MB 87.19 83.46 Turkish 116.55 600MB 82.41 75.85 Average - - 84.79 79.09 Table 3: Performance of the higher-order projective models on the multilingual track of the CoNLL-2007 task.
			The first twocolumns report the speed (in sentences per minute) and mem ory requirements of the training algorithm?these evaluationswere made on the first 1,000 training sentences 