ary tasks.
    These auxiliary tasks are sometimes specific to the supervised task, and sometimes general language modeling tasks like &#8220;predict the missing word&#8221;.
    Suzuki and Isozaki (2008) present a semisupervised extension of CRFs.
    (In Suzuki et al. (2009), they extend their semi-supervised approach to more general conditional models.)
    One of the advantages of the semi-supervised learning approach that we use is that it is simpler and more general than that of Ando and Zhang (2005) and Suzuki and Isozaki (2008).
    Their methods dictate a particular choice of model and training regime and could not, for instance, be used with an NLP system based upon an SVM classifier.
    Lin and Wu (2009) present a K-means-like non-hierarchical clustering algorithm for phrases, which uses MapReduce.
    Since they can scale to millions of phrases, and they train over 800B unlabeled words, they achieve state-of-the-art accuracy on NER using their phrase clusters.
    This suggests that extending wor