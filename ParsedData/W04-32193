nyama et al. (2002) identify dependency paths in two collections of newspaper articles.
    In each case, however, the information extracted is limited to a small set of patterns.
    Barzilay &amp; Lee (2003) exploit the metainformation implicit in dual collections of newswire articles, but focus on learning sentence-level patterns that provide a basis for generation.
    Multisequence alignment (MSA) is used to identify sentences that share formal (and presumably semantic) properties.
    This yields a set of clusters, each characterized by a word lattice that captures n-grambased structural similarities between sentences.
    Lattices are in turn mapped to templates that can be used to produce novel transforms of input sentences.
    Their methodology provides striking results within a limited domain characterized by a high frequency of stereotypical sentence types.
    However, as we show below, the approach may be of limited generality, even within the training domain.
  
  
    Our training corpus, like