 reflects an objective reality and can be mapped unambiguously to an operational procedure for marking text units as similar or not.
    At the same time, it would also validate the judgments between text units that we use for our experiments (see Section 5.1).
    In this task, judges were given the opportunity to provide reasons for claiming similarity or dissimilarity, and comments on the task were logged for future analysis.
    The three additional judges agreed with the manually marked and standardized corpus on 97.6% of the comparisons.
    Unfortunately, approximately 97% (depending on the specific experiment) of the comparisons in both our model and the subsequent validation experiment receive the value &amp;quot;not similar&amp;quot;.
    This large percentage is due to our finegrained notion of similarity, and is parallel to what happens in randomly sampled IR collections, since in that case most documents will not be relevant to any given query.
    Nevertheless, we can account for the high probab