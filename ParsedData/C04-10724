factor in the 95% confidence intervals.
			For example, even PER is as good as BLEU12 in adequacy.
			One reason for this might be due to data sparseness since only 8 systems are available.
			The other potential problem for correlation analysis of human vs. automatic framework is that high corpus-level correlation might not translate to high sentence-level correlation.
			However, high sentence-level correlation is often an important property that machine translation researchers look for.
			For example, candidate translations shorter than 12 words would have zero BLEU12 score but BLEU12 has the best correlation with human judgment in fluency as shown in Table 1.
			In order to evaluate the ever increasing number of automatic evaluation metrics for machine translation objectively, efficiently, and reliably, we introduce a new evaluation method: ORANGE.
			We describe ORANGE in details in Section 2 and briefly introduce three new automatic metrics that will be used in comparisons in Section 3.
			The results 