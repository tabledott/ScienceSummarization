om models that represent word pairs in terms of patterns linking them.
    In turn, both these models differ from those used to explore concept properties or argument alternations.
    The typical approach in the field has been a local one, in which each semantic task (or set of closely related tasks) is treated as a separate problem, that requires its own corpus-derived model and algorithm, both optimized to achieve the best performance in a given task, but lacking generality, since they resort to task-specific distributional representations, often complemented by additional taskspecific resources.
    As a consequence, the landscape of DSMs looks more like a jigsaw puzzle in which different parts have been completed and the whole figure starts to emerge from the fragments, but it is not clear yet how to put everything together and compose a coherent picture.
    We argue that the &#8220;one semantic task, one distributional model&#8221; approach represents a great limit of the current state of the art.
    