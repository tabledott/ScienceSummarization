w Ip) = 1 for the entity w with which it is paired.
    Thus this metric must be tempered to take into account the quantity of data that supports its conclusion.
    To put this another way, we want to pick (w, p) pairs that have two properties, p(w p) is high and I to, p is large.
    We need a metric that combines these two desiderata in a natural way.
    We tried two such metrics.
    The first is Dunning's [10] log-likelihood metric which measures how &amp;quot;surprised&amp;quot; one would be to observe the data counts w,P -'w,P I, I to,-' p I and I -'w,-19 I if one assumes that p(w = p(w).
    Intuitively this will be high when the observed p(w I p) &gt;&gt; p(w) and when the counts supporting this calculation are large.
    The second metric is proposed by Johnson (personal communication).
    He suggests asking the question: how far apart can we be sure the distributions p(w j p)and p(w) are if we require a particular significance level, say .05 or .01.
    We call this new test the &amp;quot;signifi