
  Bleu: A Method For Automatic Evaluation Of Machine Translation
  
    Human evaluations of machine translation are extensive but expensive.
    Human evaluations can take months to finish and involve human labor that can not be reused.
    We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.
    We present this method as an automated understudy to skilled human judges which substitutes for them when there is for quick or frequent
  
  
    Human evaluations of machine translation (MT) weigh many aspects of translation, including adequacy, fidelity , and fluency of the translation (Hovy, 1999; White and O&#8217;Connell, 1994).
    A comprehensive catalog of MT evaluation techniques and their rich literature is given by Reeder (2001).
    For the most part, these various human evaluation approaches are quite expensive (Hovy, 1999).
    Moreover, they can ta