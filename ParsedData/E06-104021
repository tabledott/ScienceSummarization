 is not effective at evaluating texts which are as good as (or better than) the reference texts.
    This is not a problem for MT, because the output of current (wide-coverage) MT systems is generally worse than human translations.
    But it is an issue for NLG, where systems are domain-specific and can generate texts that are judged better by humans than human-written texts (as seen in Tables 4 and 2).
    Although the automatic evaluation metrics generally replicated human judgments fairly well when comparing different statistical NLG systems, there was a discrepancy in the ranking of pCRUroulette (ranked high by humans, low by several of the automatic metrics). pCRU-roulette differs from the other statistical generators because it does not always try to make the most common choice (maximise the likelihood of the corpus), instead it tries to vary choices.
    In particular, if there are several competing words and phrases with similar probabilities, pCRU-roulette will tend to use different words and phrase