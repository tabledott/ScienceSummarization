thm allows us to do exactly that.2 Given a sentence w and its unannotated tree T, consider a nonterminal A spanning (r, t) and its children B and C spanning (r, s) and (s, t).
    Let Ax be a subsymbol of A, By of B, and Cz of C. Then the inside and outside probabilities PIN(r, t, Ax) def = P(wT:t|Ax) and POUT(r, t, Ax) def = P(w1:TAxwt:n) can be computed reencourages sparsity) suggest a large reduction.
    2Other techniques are also possible; Henderson (2004) uses neural networks to induce latent left-corner parser states.
    Although we show only the binary component here, of course there are both binary and unary productions that are included.
    In the Expectation step, one computes the posterior probability of each annotated rule and position in each training set tree T: In the Maximization step, one uses the above probabilities as weighted observations to update the rule probabilities: Note that, because there is no uncertainty about the location of the brackets, this formulation of the insideoutside