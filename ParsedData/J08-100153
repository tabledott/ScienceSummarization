elated to the way it is currently represented.
    Addition of this knowledge source to our grid representation, doubles the number of features that serve as input to the learning algorithm.
    In other words, salienceaware models need to learn twice as many parameters as salience-free models, while having access to the same amount of training data.
    Achieving any improvement in these conditions is challenging.
    Comparison with State-of-the-Art Methods.
    We next discuss the performance of the HMMbased content models (Barzilay and Lee 2004) and LSA (Foltz, Kintsch, and Landauer 1998) in comparison to our model (Coreference+Syntax+Salience+).
    First, note that the entity-grid model significantly outperforms LSA on both domains (p &lt; .01 using a Sign test, see Table 5).
    In contrast to our model, LSA is neither entity-based nor unlexicalized: It measures the degree of semantic overlap across successive sentences, without handling discourse entities in a special way (all content words in a sente