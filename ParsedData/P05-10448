for a given (x, y), but Z may be much harder to compute.
    All the objective functions in this paper take the form where Ai C Bi (for each i).
    For log-linear models this is simply So there is no need to compute Z(0), but we do need to compute sums over A and B. Tab.
    2 summarizes some concrete examples; see also &#167;3.1&#8211;3.2.
    We would prefer to choose an objective function such that these sums are easy.
    CE focuses on choosing appropriate small contrast sets Bi, both for efficiency and to guide the learner.
    The natural choice for Ai (which is usually easier to sum over) is the set of (x, y) that are consistent with what was observed (partially or completely) about the ith training example, i.e., the numerator E(x,y)EAi p(x, y  |&#65533;B) is designed to find p(observation i |0).
    The idea is to focus the probability mass within Bi on the subset Ai where the i the training example is known to be.
    It is possible to build log-linear models where each xi is a sequence.2 In this p