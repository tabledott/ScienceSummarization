ur tasks and was a constrained winner each time.
  
  
    Last year one of the long papers published at WMT criticized our method for compiling the overall ranking for systems in the translation task (Bojar et al., 2011).
    This year another paper shows some additional potential inconsistencies in the rankings (Lopez, 2012).
    In this section we delve into a detailed analysis of a variety of methods that use the human evaluation to create an overall ranking of systems.
    In the human evaluation, we collect ranking judgments for output from five systems at a time.
    We in5 2 4) pairwise judgments over terpret them as 10 &#8226; ( systems and use these to analyze how each system faired compared against each of the others.
    Not all pairwise comparisons detect statistical significantly superior quality of either system, and we note this accordingly.
    It is desirable to additionally produce an overall ranking.
    In the past evaluation campaigns, we used two different methods to obtain such a ranki