   For instance, he claims that finding associations between two variables that both rely on coding schemes with K &lt; .7 is often impossible, and says that content analysis researchers generally think of K &gt; .8 as good reliability, with .67 &lt; K &lt; .8 allowing tentative conclusions to be drawn.
    We would add two further caveats.
    First, although kappa addresses many of the problems we have been struggling with as a field, in order to compare K across studies, the underlying assumptions governing the calculation of chance expected agreement still require the units over which coding is performed to be chosen sensibly and comparably.
    (To see this, compare, for instance, what would happen to the statistic if the same discourse boundary agreement data were calculated variously over a base of clause boundaries, transcribed word boundaries, and transcribed phoneme boundaries.)
    Where no sensible choice of unit is available pretheoretically, measure (1) may still be preferred.
    Secondly, codi