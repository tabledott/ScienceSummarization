iciency gain of a factor of 2,600 for the new algorithm over the obvious implementation of the boosting approach.
    Efficiency issues are important, because the parsing task is a fairly large problem, involving around one million parse trees and over 500,000 features.
    The improved algorithm can perform 100,000 rounds of feature selection on our task in a few hours with current processing speeds.
    The 100,000 rounds of feature selection require computation equivalent to around 40 passes over the entire training set (as opposed to 100,000 passes for the &#8220;naive&#8221;implementation).
    The problems with history-based models and the desire to be able to specify features as arbitrary predicates of the entire tree have been noted before.
    In particular, previous work (Ratnaparkhi, Roukos, and Ward 1994; Abney 1997; Della Pietra, Della Pietra, and Lafferty 1997; Johnson et al. 1999; Riezler et al.
    2002) has investigated the use of Markov random fields (MRFs) or log-linear models as probabilis