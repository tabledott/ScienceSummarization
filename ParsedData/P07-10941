MLE.
    We find improvements both when training from data alone, and using a tagging dictionary.
  
  
    Unsupervised learning of linguistic structure is a difficult problem.
    Recently, several new model-based approaches have improved performance on a variety of tasks (Klein and Manning, 2002; Smith and Eisner, 2005).
    Nearly all of these approaches have one aspect in common: the goal of learning is to identify the set of model parameters that maximizes some objective function.
    Values for the hidden variables in the model are then chosen based on the learned parameterization.
    Here, we propose a different approach based on Bayesian statistical principles: rather than searching for an optimal set of parameter values, we seek to directly maximize the probability of the hidden variables given the observed data, integrating over all possible parameter values.
    Using part-of-speech (POS) tagging as an example application, we show that the Bayesian approach provides large performance improvements