sifier to Rater 2: agreement was high and kappa was low.
    In addition, for both raters, precision was much higher than recall.
    As noted earlier, the table does not include the cases that the classifier skipped due to misspelling, antonymous prepositions, and benefactives.
    Both precision and recall are low in these comparisons to the human raters.
    We are particularly concerned about precision because the feedback that students receive from an automated writing analysis system should, above all, avoid false positives, i.e., marking correct usage as incorrect.
    We tried to improve precision by adding to the system a naive Bayesian classifier that uses the same features found in Table 1.
    As expected, its performance is not as good as the ME model (e.g., precision = 0.57 and recall = 0.29 compared to Rater 1 as the gold standard), but when the Bayesian classifier was given a veto over the decision of the ME classifier, overall precision did increase substantially (to 0.88), though with a redu