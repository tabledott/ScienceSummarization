tate of machine translation for different language pairs.
  
  
    In addition to allowing us to analyze the translation quality of different systems, the data gathered during the manual evaluation is useful for validating the automatic evaluation metrics.
    Last year, NIST began running a similar &#8220;Metrics for MAchine TRanslation&#8221; challenge (MetricsMATR), and presented their findings at a workshop at AMTA (Przybocki et al., 2008).
    In this year&#8217;s shared task we evaluated a number of different automatic metrics: for non-identical items.
    To find a maximum weight matching that matches each system item to at most one reference item, the items are then modeled as nodes in a bipartite graph.
    &#8226; wcd6p4er (Leusch and Ney, 2008)&#8212;a measure based on cder with word-based substitution costs.
    Leusch and Ney (2008) also submitted two contrastive metrics: bleusp4114, a modified version of BLEU-S (Lin and Och, 2004), with tuned n-gram weights, and bleusp, with constant weights. w