ors and good translations receive the same penalty as poor translations.
  
  
    We performed our experiments on the Large-Data Track of the NIST Chinese-to-English MT task (NIST, 2003).
    The goal of this task is the translation of news stories from Chinese to English.
    The test set has a total of 1791 sentences, consisting of 993 sentences from the NIST 2001 MT-eval set and 878 sentences from the NIST 2002 MT-eval set.
    Each Chinese sentence in this set has four reference translations.
    The performance of the baseline and the MBR decoders under the different loss functions was measured with respect to the four reference translations provided for the test set.
    Four evaluation metrics were used.
    These were multi-reference Word Error Rate (mWER) (Och, 2002), multi-reference Position-independent word Error Rate (mPER) (Och, 2002) , BLEU and multi-reference BiTree Error Rate.
    Among these evaluation metrics, the BLEU score directly takes into account multiple reference translations (Papin