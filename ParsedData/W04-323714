d be quite large since the cardinality of Fbackground is typically several orders of magnitude larger than that of Fadapt and our approach also updates the weights corresponding to features in Fbackground \ Fadapt.
    Further experiments are needed to compare the performance of the two approaches.
  
  
    The baseline 1-gram and the background MEMM capitalizer were trained on various amounts of WSJ (Paul and Baker, 1992) data from 1987 &#8212; files WS87_{001-126}.
    The in-domain test data used was file WS94_000 (8.7kwds).
    As for the adaptation experiments, two different sets of BN data were used, whose sizes are summarized in Table 1: We have proceeded building both 1-gram and MEMM capitalizers using various amounts of background training data.
    The model sizes for the 1gram and MEMM capitalizer are presented in Table 2.
    Count cut-off feature selection has been used for the MEMM capitalizer with the threshold set at 5, so the MEMM model size is a function of the training data.
    The 1-gram