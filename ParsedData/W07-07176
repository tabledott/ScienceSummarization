, 1993).
  
  
    Our approach to mixture-model adaptation can be summarized by the following general algorithm: from several different domains.
    Set mixture weights as a function of the distances from corpus components to the current source text.
    4.
    Combine weighted component models into a single global model, and use it to translate as described in the previous section.
    We now describe each aspect of this algorithm in more detail.
    We partition the corpus into different genres, defined as being roughly identical to corpus source.
    This is the simplest way to exploit heterogeneous training material for adaptation.
    An alternative, which we have not explored, would be to cluster the corpus automatically according to topic.
    We adapt both language and translation model features within the overall loglinear combination (1).
    To train translation models on each corpus component, we used a global IBM2 model for word alignment (in order to avoid degradation in alignment quality due t