 in the IBM model one cannot link a Target word to more than a Source word, the training procedure train the IBM-4 model, we used Giza (Al-Onaizan et al., 1999).
    IBM&#8722;4 T&#8722;Table IBM&#8722;4 Intuitive Joint Joint T&#8722;Table p(y  |a) = 1 p(x  |c) = 1 p(z  |b) = 0.98 p(x  |b) = 0.02 S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c S1: a b c T1: x y S2: b c p(x y, a b c) = 0.32 p(x, b c) = 0.34 p(y, a) = 0.01 p(z, b) = 0.33 Corresponding Conditional Table T2: x T2: x T2: x p(x y  |a b c ) = 1 p(x  |b c) = 1 p(y  |a) = 1 p(z  |b) = 1 S3: b S3: b S3: b T3: z T3: z T3: z a) b) c) d) e) Figure 1: Alignments and probability distributions in IBM Model 4 and our joint phrase-based model. yields unintuitive translation probabilities.
    (Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).)
    In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word leve