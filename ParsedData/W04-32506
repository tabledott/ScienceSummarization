y of having multiple translations of the same source material.
    Fortunately, it has not been shown so far that having only a single reference translation causes serious problems.
    While BLEU has become the most popular metric for machine translation evaluation, some of its short-comings have become apparent: It does not work on single sentences, since 4-gram precision is often 0.
    It is also hard to interpret.
    What a BLEU score of 28.9% means is not intuitive and depends, e.g., on the number of reference translation used.
    Some researchers have recently used relative human BLEU scores, by comparing machine BLEU scores with high quality human translation scores.
    However, the resulting numbers are unrealistically high.
  
  
    In this section, we describe the experimental framework of our work.
    We also report on a number of preliminary experiments that give us some intuition on variance of test scores on different test sets.
    We carry out experiments using a phrase-based statistical