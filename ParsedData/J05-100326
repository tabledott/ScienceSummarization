o 20,000 examples, 16 features per example, and 500 rounds of feature selection for the PP attachment task in Ratnaparkhi (1998).
    As an estimate, assuming that computational complexity scales linearly in these factors,6 our task is 1,000,000 as large as the PP attachment task.
    These figures suggest that the maximum-entropy feature selection approach may be infeasible for large-scale tasks such as the one in this article.
    The fact that the boosting approach does not update the entire model at each round of feature selection may be a disadvantage in terms of the number of features or the test data accuracy of the final model.
    There is reason for concern that Step 2 will at some iterations mistakenly choose features which are apparently useful in reducing the loss function, but which would have little utility if the entire model had been optimized at the previous iteration of Step 3.
    However, previous empirical results for boosting have shown that it is a highly effective learning method, sug