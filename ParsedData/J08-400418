 joint probability of each coder making this assignment independently.
    For &#954; this joint probability is &#710;P(k|c1) &#183; &#710;P(k|c2); expected agreement is then the sum of this joint probability over all the categories k &#8712; K. It is easy to show that for any set of coding data, A&#960;e &#8805; A&#954;e and therefore &#960; &#8804; &#954;, with the limiting case (equality) obtaining when the observed distributions of the two coders are identical.
    The relationship between &#954; and S is not fixed.
    In corpus annotation practice, measuring reliability with only two coders is seldom considered enough, except for small-scale studies.
    Sometimes researchers run reliability studies with more than two coders, measure agreement separately for each pair of coders, and report the average.
    However, a better practice is to use generalized versions of the coefficients.
    A generalization of Scott&#8217;s &#960; is proposed in Fleiss (1971), and a generalization of Cohen&#8217;s &#954; i