behind the development of Bleu (Papineni et al., 2002) is that human evaluation of machine translation can be time consuming and expensive.
    An automatic evaluation metric, on the other hand, can be used for frequent tasks like monitoring incremental system changes during development, which are seemingly infeasible in a manual evaluation setting.
    The way that Bleu and other automatic evaluation metrics work is to compare the output of a machine translation system against reference human translations.
    Machine translation evaluation metrics differ from other metrics that use a reference, like the word error rate metric that is used in speech recognition, because translations have a degree of variation in terms of word choice and in terms of variant ordering of some phrases.
    Bleu attempts to capture allowable variation in word choice through the use of multiple reference translations (as proposed in Thompson (1991)).
    In order to overcome the problem of variation in phrase order, Bleu uses modi