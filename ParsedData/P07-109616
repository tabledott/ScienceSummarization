ic approximation of gradient descent that minimizes the squared errors of the misclassified samples (Widrow and Hoff, 1960).
    What is special with our learning algorithm is the strategy used to select samples for training.
    In general, this novel learning framework lies between supervised learning and reinforcement learning.
    Guided learning is more difficult than supervised learning, because we do not know the order of inference.
    The order is learned automatically, and partial output is in turn used to train the local classifier.
    Therefore, the order of inference and the local classification are dynamically incorporated in the learning phase.
    Guided learning is not as hard as reinforcement learning.
    At each local step in learning, we always know the undesirable labeling actions according to the gold standard, although we do not know which is the most desirable.
    In this approach, we can easily collect the automatically generated negative samples, and use them in learning.
    Thes