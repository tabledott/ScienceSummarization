 to do this, a PCFG would need productions like the following one, which encodes the fact that &#8220;want&#8221; is a Word.
    Word &#8594; w a n t Adaptor grammars can be viewed as a way of formalizing this idea.
    Adaptor grammars learn the probabilities of entire subtrees, much as in tree substitution grammar (Joshi, 2003) and DOP (Bod, 1998).
    (For computational efficiency reasons adaptor grammars require these subtrees to expand to terminals).
    The set of possible adapted tree fragments is the set of all subtrees generated by the CFG whose root label is a member of the set of adapted nonterminals A (adapted nonterminals are indicated by underlining in this paper).
    For example, in the unigram adaptor grammar A = {Word}, which means that the adaptor grammar inference procedure learns the probability of each possible Word subtree.
    Thus adaptor grammars are simple models of structure learning in which adapted subtrees are the units of generalization.
    One might try to reduce adaptor gram