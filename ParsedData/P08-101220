R drops first and increases again as the number of iterations varies from 1 to 10.
    The lowest AER using EM is achieved after the second iteration, which is .40.
    At iteration 10, AER for EM increases to .42.
    On the other hand, using VB, AER decreases monotonically over the 10 iterations and stabilizes at iteration 10.
    When &#945;C is 1e &#8722; 9, VB gets AER close to .35 at iteration 10.
    As we increase the bias toward sparsity, the AER decreases, following a long slow plateau.
    Although the magnitude of improvement is not large, the trend is encouraging.
    These experiments also indicate that a very sparse prior is needed for machine translation tasks.
    Unlike Johnson (2007), who found optimal performance when &#945; was approximately 10&#8722;4, we observed monotonic increases in performance as &#945; dropped.
    The dimensionality of this MT problem is significantly larger than that of the sequence problem, though, therefore it may take a stronger push from the prior to achieve 