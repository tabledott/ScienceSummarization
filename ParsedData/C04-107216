			In the next section, we present the evaluations of BLEU, NIST, PER, WER, ROUGE-L, ROUGE-W, and ROUGE-S using the ORANGE evaluation method described in Section 2.
			2 Combinations: C(4,2) = 4!/(2!*2!) = 6..
	
	
			Comparing automatic evaluation metrics using the ORANGE evaluation method is straightforward.
			To simulate real world scenario, we use n-best lists from ISI?s state-of-the-art statistical machine translation system, AlTemp (Och 2003), and the 2002 NIST Chinese-English evaluation corpus as.
			the test corpus.
			There are 878 source sentences in Chinese and 4 sets of reference translations provided by LDC3.
			For exploration study, we generate 1024-best list using AlTemp for 872 source sentences.
			AlTemp generates less than 1024 alternative translations for 6 out of the 878 source 3 Linguistic Data Consortium prepared these manual.
			translations as part of the DARPA?s TIDES project.
			sentences.
			These 6 source sentences are excluded from the 1024-best set.
			In order to compute BLEU a