.
    For example, the models that perform best in the Windows domain achieve nearly perfect word alignment scores.
    To further assess the contribution of the instruction text, we train a variant of our model without access to text features.
    This is possible in the game domain, where all of the puzzles share a single goal state that is independent of the instructions.
    This variant solves 34% of the puzzles, suggesting that access to the instructions significantly improves performance.
  
  
    In this paper, we presented a reinforcement learning approach for inducing a mapping between instructions and actions.
    This approach is able to use environment-based rewards, such as task completion, to learn to analyze text.
    We showed that having access to a suitable reward function can significantly reduce the need for annotations.
  
  
    The authors acknowledge the support of the NSF (CAREER grant IIS-0448168, grant IIS-0835445, grant IIS-0835652, and a Graduate Research Fellowship) and the ONR