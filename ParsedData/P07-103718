r experiments with large data sets.
    The n-gram target language model was built using 250M words from the English GigaWord Corpus using the SRILM toolkit.4 Taking 10% of the English GigaWord Corpus used for building our target language model, the supertag-based target language models were built from 25M words that were supertagged.
    For the LTAG supertags experiments, we used the LTAG English supertagger5 (Bangalore &amp; Joshi, 1999) to tag the English part of the parallel data and the supertag language model data.
    For the CCG supertag experiments, we used the CCG supertagger of (Clark &amp; Curran, 2004) and the Edinburgh CCG tools6 to tag the English part of the parallel corpus as well as the CCG supertag language model data.
    The NIST MT03 test set is used for development, particularly for optimizing the interpolation weights using Minimum Error Rate training (Och, 2003).
    Baseline System The baseline system is a stateof-the-art PBSMT system as described in section 3.
    We built two base