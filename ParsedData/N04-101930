ments that satisfy the compression constraints.
    For multidocument summarization compression rates are high, so even sentences with the highest relevance judgments are potentially not used.
    Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives.
    In their studies they found that multiple model summaries lead to more stable evaluation results.
    We believe a flaw in their work is that they calibrate the method to the erratic DUC scores.
    When applied to per set ranking of summaries, no correlation was seen with pyramid scores.
  
  
    There are many open questions about how to parameterize a summary for specific goals, making evaluation in itself a significant research question (Jing et al., 1998).
    Instead of attempting to develop a method to elicit reliable judgments from humans, we chose to calibrate our method to human summarization behavior