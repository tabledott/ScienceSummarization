 parser (Collins, 2003).
    Similar to their work, we use the idea of letting discrete categories reduce the search space during inference.
    We compare to fully tied RNNs in which the same weights are used at every node.
    Our syntactically untied RNNs outperform them by a significant margin.
    The idea of untying has also been successfully used in deep learning applied to vision (Le et al., 2010).
    This paper uses several ideas of (Socher et al., 2011b).
    The main differences are (i) the dual representation of nodes as discrete categories and vectors, (ii) the combination with a PCFG, and (iii) the syntactic untying of weights based on child categories.
    We directly compare models with fully tied and untied weights.
    Another work that represents phrases with a dual discrete-continuous representation is (Kartsaklis et al., 2012).
  
  
    This section introduces Compositional Vector Grammars (CVGs), a model to jointly find syntactic structure and capture compositional semantic information