ees on unlabeled data with a second classifier based on a different &#8220;view&#8221; of the data.
    This addresses one of the shortcomings of the original co-training paper: it gives a proof that justifies searching for classifiers that agree on unlabeled data.
    I extend this work in two ways.
    First, (Dasgupta et al., 2001) assume the same conditional independence assumption as proposed by Blum and Mitchell.
    I show that that independence assumption is remarkably powerful, and violated in the data; however, I show that a weaker assumption suffices.
    Second, I give an algorithm that finds classifiers that agree on unlabeled data, and I report on an implementation and empirical results.
    Finally, I consider the question of the relation between the co-training algorithm and the Yarowsky algorithm.
    I suggest that the Yarowsky algorithm is actually based on a different independence assumption, and I show that, if the independence assumption holds, the Yarowsky algorithm is effective at find