pothesized by a recognizer are expressed probabilistically in the recognizer language model (LM).
    It provides the prior distribution P(W) for finding the a posteriori most probable hypothesized words for an utterance, given the acoustic evidence A, (Bahl, Jelinek, and Mercer 1983):7 The likelihoods P(A,I W,) are estimated by the recognizer's acoustic model.
    In a standard recognizer the language model P(W) is the same for all utterances; the idea here is to obtain better-quality LMs by conditioning on the DA type U&#8222; since presumably the word distributions differ depending on DA type.
    As before in the DA classification model, we tacitly assume that the words Wi depend only on the DA of the current utterance, and also that the acoustics are independent of the DA type if the words are fixed.
    The DA-conditioned language models P(W,IU,) are readily trained from DA-specific training data, much as we did for DA classification from words.'
    The problem with applying Equation 11, of course, is 