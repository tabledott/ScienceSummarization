and use the (imperfect) result as further training data.
    This can be considered a variant of the EM algorithm, although we use the single most likely hypothesis for the unannotated data, rather than calculating the expectation over all hypotheses.
    Only one iteration of training on the unannotated data was performed.
    The unannotated data used consisted of 156,590 sentences containing the target words of our corpus, increasing the total amount of data available to roughly six times the 36,995 annotated training sentences.
    Table 12 shows results on noun phrases for the bootstrapping method.
    The accuracy of a system trained only on data from the automatic labeling (Panto) is 81.0%, reasonably close to the 87.0% for the system trained only on annotated data (Ptrai,,,).
    Combining the annotated and automatically labeled data increases coverage from 41.6% to 54.7% and performance to 44.5%.
    Because the automatically labeled data are not as accurate as the annotated data, we can do slightly 