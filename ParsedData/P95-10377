s to get a good estimate of the probability of the tagging decision.
    The decision tree described in this paragraph is shown in Figure 1.
    Each question asked by the decision tree is represented by a tree node (an oval in the figure) and the possible answers to this question are associated with branches emanating from the node.
    Each node defines a probability distribution on the space of possible decisions.
    A node at which the decision tree stops asking questions is a leaf node.
    The leaf nodes represent the unique states in the decision-making problem, i.e. all contexts which lead to the same leaf node have the same probability distribution for the decision.
    A decision-tree model is not really very different from an interpolated n-gram model.
    In fact, they are equivalent in representational power.
    The main differences between the two modeling techniques are how the models are parameterized and how the parameters are estimated.
    First, let's be very clear on what we mean by an 