tion, and language model score.We experiment with two language pairs: Chinese to-English (C-E) and Arabic-to-English (A-E).
			For C-E, we trained translation models using a subset of the Chinese-English parallel data released by LDC (mostly news, in particular FBIS and Xinhua News).
			This subset comprises 12.2M English words, and 11M Chinese words.
			Chinese words are segmented with a conditional random field (CRF) classifier that conforms to the Chinese Treebank (CTB) standard.
			The training set for our A-E systems also includes mostly news parallel data released by LDC, and contains 19.5M English words, and 18.7M Arabic tokens that have been segmented using the Arabic Treebank (ATB) (Maamouri et al, 2004) standard.6 For our language model, we trained a 5-gram model using the Xinhua and AFP sections of the Gigaword corpus (LDC2007T40), in addition to the target side of the parallel data.
			For both C-E and A-E, we manually removed documents of Gigaword that were released during periods that overlap wi