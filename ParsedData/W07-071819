hey assigned identical scores to the same items.
    For the ranking tasks we calculated P(A) by examining all pairs of systems which had been judged by two or more judges, and calculated the proportion of time that they agreed that A &gt; B, A = B, or A &lt; B.
    For intra-annotator agreement we did similarly, but gathered items that were annotated on multiple occasions by a single annotator.
    Table 5 gives K values for inter-annotator agreement, and Table 6 gives K values for intra-annoator agreement.
    These give an indication of how often different judges agree, and how often single judges are consistent for repeated judgments, respectively.
    The interpretation of Kappa varies, but according to Landis and Koch (1977) 0 &#8722; &#8722;.2 is slight, .21&#8722; &#8722;.4 is fair, .41&#8722;&#8722;.6 is moderate, .61&#8722;&#8722;.8 is substantial and the rest almost perfect.
    The K values for fluency and adequacy should give us pause about using these metrics in the future.
    When we analyzed 