  Nasr (1997) reported that the translation lexicon that our model induced from this tiny bitext accounted for 30% of the word types with precision between 84% and 90%.
    Recall drops when there is less training data, because the model refuses to make predictions that it cannot make with confidence.
    For many applications, this is the desired behavior.
    The most detailed evaluation of link tokens to date was performed by (Macklovitch &amp; Hannan, 1996), who trained Brown et al. 's Model 2 on 74 million words of the Canadian Hansards.
    These authors kindly provided us with the links generated by that model in 51 aligned sentences from a heldout test set.
    We generated links in the same 51 sentences using our two-class word-to-word model, and manually evaluated the content-word links from both models.
    The IBM models are directional; i.e. they posit the English words that gave rise to each French word, but ignore the distribution of the English words.
    Therefore, we ignored English words th