nce(s) that best answered the question, even where the sentence also contained additional (unnecessary) information.
    For AutSent, an automated routine replaced the human annotator, examining the texts and choosing the sentences, this time based on which one had the highest recall compared against the published answer key.
    For P&amp;R we note that in Figure 2, there are two content words in the answer key (library and congress) and sentence 1 matches both of them, for 2/2 = 100% recall.
    There are seven content words in sentence 1, so it scores 2/7 = 29% precision.
    Sentence 2 scores 1/2=50% recall and 1/6=17% precision.
    The human preparing the list of acceptable sentences for HumSent has a problem.
    Sentence 2 responds to the question, but requires pronoun coreference to give the full answer (the antecedent of it).
    Sentence 1 contains the words of the answer, but the sentence as a whole doesn't really answer the question.
    In this and other difficult cases, we have chosen to list n