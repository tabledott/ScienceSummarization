d 5 show the cue-level performances, i.e. the F-measure of cue phrase matching where true positives were strict matches.
    Note that it was optional to submit cue annotations for Task1 (if participants submitted systems for both Task2 and Task1B with cue tagging, only the better score of the two was considered).
    It is interesting to see that Morante et al. (2010) who obtained the best results on Task2 achieved a medium-ranked F-measure on the cue-level (e.g.
    their result on the cue-level is lower by 4% compared to Zhou et al. (2010), while on the scopelevel the difference is 13% in the reverse direction), which indicates that the real strength of the system of Morante et al.
    (2010) is the accurate detection of scope boundaries.
    The approaches to Task1 fall into two major categories.
    There were six systems which handled the task as a classical sentence classification problem and employed essentially a bag-of-words feature representation (they are marked as BoW in Table 6).
    The remaini