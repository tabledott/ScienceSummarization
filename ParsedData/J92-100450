ikely assumption.
    If the grammar's estimates reflect reality, the estimated probabilities will result in a reduction in the total perplexity An average perplexity for the 167 test sentences that were parsable was computed for the two conditions, without (Case 1) and with (Case 2) the estimated probabilities.
    The result was a perplexity of 368 for Case 1, but only 41.5 for Case 2, as summarized in Table 1.
    This is with a total vocabulary size of 985 words, and with a grammar that included some semantically restricted classes such as [ship-name] and [readinesscategory].
    The incorporation of arc probabilities reduced the perplexity by a factor of nine, a clear indicator that a proper mechanism for utilizing probabilities in a grammar can help significantly.
    An even lower perplexity could be realized within this domain by increasing the number of semantic nodes.
    In fact, this is a trend that we have increasingly adopted as we move to new domains.
    We didn't look at the test sentences wh