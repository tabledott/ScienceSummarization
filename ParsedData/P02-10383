ing this is maximum entropy (Berger et al., 1996).
    In this framework, we have a set of M feature functions hm(ei, fJ1 ), m = 1, ... , M. For each feature function, there exists a model parameter am, m = 1, ... , M. The direct translation probability is given the following two feature functions: This approach has been suggested by (Papineni et al., 1997; Papineni et al., 1998) for a natural language understanding task.
    We obtain the following decision rule: Hence, the time-consuming renormalization in Eq.
    8 is not needed in search.
    The overall architecture of the direct maximum entropy models is summarized in Figure 2.
    Interestingly, this framework contains as special case the source channel approach (Eq.
    5) if we use and set A1 = A2 = 1.
    Optimizing the corresponding parameters A1 and A2 of the model in Eq.
    8 is equivalent to the optimization of model scaling factors, which is a standard approach in other areas such as speech recognition or pattern recognition.
    The use of an