  For comparison, we also included two additional models: the &amp;quot;1-best LM&amp;quot; refers to always using the DAspecific LM corresponding to the most probable DA type for each utterance.
    It is thus an approximation to both mixture approaches where only the top DA is considered.
    Second, we included an &amp;quot;oracle LM,&amp;quot; i.e., always using the LM that corresponds to the hand-labeled DA for each utterance.
    The purpose of this experiment was to give us an upper bound on the effectiveness of the mixture approaches, by assuming perfect DA recognition.
    It was somewhat disappointing that the word error rate (WER) improvement in the oracle experiment was small (2.2% relative), even though statistically highly significant (p &lt; .0001, one-tailed, according to a Sign test on matched utterance pairs).
    The WER reduction achieved with the mixture-of-LMs approach did not achieve statistical significance (0.25 &gt; p &gt; 0.20).
    The 1-best DA and the two mixture models also did 