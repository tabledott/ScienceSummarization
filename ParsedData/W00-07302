lines as margin.
    SVMs take a simple strategy that finds the separating hyperplane which maximizes its margin.
    Precisely, two dashed lines and margin (d) can be written as: SVMs can be regarded as an optimization problem; finding w and b which minimize liwil under the constraints: yiRw xi) &#177; &gt; 1.
    Furthermore, SVMs have potential to cope with the linearly unseparable training data.
    We leave the details to (Vapnik, 1995), the optimization problems can be rewritten into a dual form, where all feature vectors appear in their dot product.
    By simply substituting every dot product of xi and xi in dual form with any Kernel function K(xi, xi), SVMs can handle nonlinear hypotheses.
    Among the many kinds of Kernel functions available, we will focus on the d-th polynomial kernel: K(xi, xi) = (xi &#8226; xi + 1)d Use of d-th polynomial kernel function allows us to build an optimal separating hyperplane which takes into account all combination of features up to d. We believe SVMs have advantag