
  Comparing Automatic And Human Evaluation Of NLG Systems
  
    We consider the evaluation problem in Language Generation and results for evaluating several systems with similar functionality, including a knowledge-based generator and several statistical systems.
    We compare evaluation results for these systems by human domain experts, human non-experts, and several automatic evaluation metrics, inand We that correlate best with human judgments, but that all automatic metrics we examined are biased in favour of generators that select on the basis of frequency alone.
    We conthat automatic evaluation of systems has considerable potential, in particular where high-quality reference texts and only a small number of human evaluators are available.
    However, in general it is probably best for automatic evaluations to be supported by human-based evaluations, or at least by studies that demonstrate that a particular metric correlates well with human judgments in a given domain.
  
  
    Evaluation is beco