ed bigrams form the set of candidate features presented to the decision tree learner.
    If there are ties in the top 100 rankings then there may be more than 100 features, and if the there were fewer than 100 bigrams that occurred more than 5 times then all such bigrams are included in the feature set. suring the association between two words, while the decision tree seeks bigrams that partition instances of the ambiguous word into into distinct senses.
    In particular, the decision tree learner makes decisions as to what bigram to include as nodes in the tree using the gain ratio, a measure based on the overall Mutual Information between the bigram and a particular word sense.
    Finally, note that the smallest decision trees are functionally equivalent to our benchmark methods.
    A decision tree with 1 leaf node and no internal nodes (1/1) acts as a majority classifier.
    A decision tree with 2 leaf nodes and 1 internal node (2/3) has the structure of a decision stump.
  
  
    One of our long-ter