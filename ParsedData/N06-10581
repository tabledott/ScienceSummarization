s the quality of a machine-generated output by considering its similarity to a reference text written by a human.
    Ideally, the similarity would reflect the semantic proximity between the two.
    In practice, this comparison breaks down to n-gram overlap between the reference and the machine output. machine translation from the NIST 2004 MT evaluation.
    Consider the human-written translation and the machine translation of the same Chinese sentence shown in Table 1.
    While the two translations convey the same meaning, they share only auxiliary words.
    Clearly, any measure based on word overlap will penalize a system for generating such a sentence.
    The question is whether such cases are common phenomena or infrequent exceptions.
    Empirical evidence supports the former.
    Analyzing 10,728 reference translation pairs1 used in the NIST 2004 machine translation evaluation, we found that only 21 (less than 0.2%) of them are identical.
    Moreover, 60% of the pairs differ in at least 11 words.
