 of 60 source segments, instead of being sampled from the larger pool of 1,000 source segments designated for the ranking task.7 The larger pool was used to choose source segments for nine other screens (also 45%).
    As for the remaining two screens (10%), they were chosen randomly from the set of eighteen screens already chosen.
    Furthermore, in the two &#8220;local repeat&#8221; screens, the system choices were also preserved.
    Heavily sampling from a small pool of source segments ensured we had enough data to measure inter-annotator agreement, while purposely making 10% of each annotator&#8217;s screens repeats of previously seen sets in the same batch ensured we
  
  
    the sentence ranking task.
    In this task, P(E) is 0.333. had enough data to measure intra-annotator agreement.
    We measured pairwise agreement among annotators using the kappa coefficient (K), which is defined as where P(A) is the proportion of times that the annotators agree, and P(E) is the proportion of time that they wo