ning section of the BROWN corpus, with parameters tuned against its held-out section.
    Despite seeing no in-domain data, the WSJ based parser is able to match the BROWN based parser.
    For the remainder of this paper, we will refer to the model trained on WSJ+2,500k sentences of NANC as our &#8220;best WSJ+NANC&#8221; model.
    We also note that this &#8220;best&#8221; parser is different from the &#8220;best&#8221; parser for parsing WSJ, which was trained on WSJ with a relative weight4 of 5 and 1,750k sentences from NANC.
    For parsing BROWN, the difference between these two parsers is not large, though.
    Increasing the relative weight of WSJ sentences versus NANC sentences when testing on BROWN development does not appear to have a significant effect.
    While (McClosky et al., 2006) showed that this technique was effective when testing on WSJ, the true distribution was closer to WSJ so it made sense to emphasize it.
    Up to this point, we have only considered the situation where we have no i