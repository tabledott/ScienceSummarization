ges could click on &#8220;Yes&#8221;, &#8220;No&#8221;, or &#8220;Not Sure.&#8221; The number of times people clicked on &#8220;Not Sure&#8221; varied by language pair and task.
    It was selected as few as 5% of the time for the EnglishSpanish News task to as many as 12.5% for the Czech-English News task.
    We collected judgments using a web-based tool that presented judges with batches of each type of evaluation.
    We presented them with five screens of sentence rankings, ten screens of constituent rankings, and ten screen of yes/no judgments.
    The order of the types of evaluation were randomized.
    In order to measure intra-annotator agreement 10% of the items were repeated and evaluated twice by each judge.
    In order to measure inter-annotator agreement 40% of the items were randomly drawn from a common pool that was shared across all annotators so that we would have items that were judged by multiple annotators.
    Judges were allowed to select whichever data set they wanted, and to evaluat