from the implicit assumption in other models that each word has only one sense.
    Each word is assigned the same unit of probability mass, which the model distributes over all candidate translations.
    The correct translations of a word that has several correct translations will be assigned a lower probability than the correct translation of a word that has only one correct translation.
    This imbalance foils thresholding strategies, clever as they might be (Gale &amp; Church, 1991; Wu &amp; Xia, 1994; Chen, 1996).
    The likelihoods in the word-to-word model remain unnormalized, so they do not compete.
    The word-to-word model maintains high precision even given much less training data.
    Resnik &amp; Melamed (1997) report that the model produced translation lexicons with 94% precision and 30% recall, when trained on French/English software manuals totaling about 400,000 words.
    The model was also used to induce a translation lexicon from a 6200-word corpus of French/English weather reports.
  