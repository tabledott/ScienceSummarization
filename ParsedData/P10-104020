ons performed better on the supervised task when they were induced on the clean unlabeled data, both embeddings and Brown clusters.
    This is the case even though the cleaning process was very aggressive, and discarded more than half of the sentences.
    According to the evidence and arguments presented in Bengio et al. (2009), the non-convex optimization process for Collobert and Weston (2008) embeddings might be adversely affected by noise and the statistical sparsity issues regarding rare words, especially at the beginning of training.
    For this reason, we hypothesize that learning representations over the most frequent words first and gradually increasing the vocabulary&#8212;a curriculum training strategy (Elman, 1993; Bengio et al., 2009; Spitkovsky et al., 2010)&#8212;would provide better results than cleaning.
    After cleaning, there are 37 million words (58% of the original) in 1.3 million sentences (41% of the original).
    The cleaned RCV1 corpus has 269K word types.
    This is the vocabu