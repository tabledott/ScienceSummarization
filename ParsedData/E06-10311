y as well as semantical correctness should be the evaluation criterion.
    But as human evaluation is tedious and cost-intensive, automatic evaluation measures are used in most MT research tasks.
    A high correlation between these automatic evaluation measures and human evaluation is thus desirable.
    State-of-the-art measures such as BLEU (Papineni et al., 2002) or NIST (Doddington, 2002) aim at measuring the translation quality rather on the document level1 than on the level of single sentences.
    They are thus not well-suited for sentence-level evaluation.
    The introduction of smoothing (Lin and Och, 2004) solves this problem only partially.
    In this paper, we will present a new automatic error measure for MT &#8211; the CDER &#8211; which is designed for assessing MT quality on the sentence level.
    It is based on edit distance &#8211; such as the well-known word error rate (WER) &#8211; but allows for reordering of blocks.
    Nevertheless, by defining reordering costs, the ordering of the