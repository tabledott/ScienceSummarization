paper?
    Teufel and Moens Summarizing Scientific Articles We use the kappa coefficient K (Siegel and Castellan 1988) to measure stability and reproducibility, following Carletta (1996).
    The kappa coefficient is defined as follows: where P(A) is pairwise agreement and P(E) random agreement.
    K varies between 1 when agreement is perfect and &#8722;1 when there is a perfect negative correlation.
    K = 0 is defined as the level of agreement that would be reached by random annotation using the same distribution of categories as the actual annotators did.
    The main advantage of kappa as an annotation measure is that it factors out random agreement by numbers of categories and by their distribution.
    As kappa also abstracts over the number of annotators considered, it allows us to compare the agreement numerically among a group of human annotators with the agreement between the system and one or more annotators (section 5), which we use as one of the performance measures of the system.
    3.2.2 Res