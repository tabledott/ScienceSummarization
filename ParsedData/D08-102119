stituted into, with a total of 8,422 judgements collected.
    Note that 7 different paraphrases were judged on average for every instance.
    This is because annotators judged paraphrases for eight conditions, and because we collected judgments for the 5-best paraphrases for many of the conditions.
    We measured inter-annotator agreement with the Kappa statistic (Carletta, 1996) using the 1,391 items that two annotators scored in common.
    The two annotators assigned the same absolute score 47% of the time.
    If we consider chance agreement to be 20% for 5-point scales, then K = 0.33, which is commonly interpreted as &#8220;fair&#8221; (Landis and Koch, 1977).
    If we instead measure agreement in terms of how often the annotators both judged an item to be above or below the thresholds that we set, then their rate of agreement was 80%.
    In this case chance agreement would be 50%, so K = 0.61, which is &#8220;substantial&#8221;.
    In order to allow other researchers to recreate our results or ext