also trained on sections 2?21 to demon strate that our methods can scale up and achievebroadly comparable results to existing state-of-the art parsers.
			When using a truncation level of K = 16, the standard PCFG with smoothing obtains an F1 score of 88.36 using 706157 effective rules whilethe HDP-PCFG-GR obtains an F1 score of 87.08 us ing 428375 effective rules.
			We expect to see greaterbenefits from the HDP-PCFG with a larger trunca tion level.
	
	
			The question of how to select the appropriate gram mar complexity has been studied in earlier work.It is well known that more complex models nec essarily have higher likelihood and thus a penaltymust be imposed for more complex grammars.
			Examples of such penalized likelihood procedures in clude Stolcke and Omohundro (1994), which used an asymptotic Bayesian model selection criterion and Petrov et al (2006), which used a split-merge algorithm which procedurally determines when to switch between grammars of various complexities.
			These techniques are mo