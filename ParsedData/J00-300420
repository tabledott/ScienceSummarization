of escape, this time to order 0, would have occurred (probability 3/13), and the x would be encoded with a probability of 1/256 (assuming that the alphabet has 256 characters) for a total of just over 10 bits.
    It is clear from Table 1 that, in the context tobeornottobe, if the next character is o it will be encoded by the order 2 model.
    Hence if an escape occurs down to order 1, the next character cannot be o.
    This makes it unnecessary to reserve probability space for the occurrence of o in the order 1 (or order 0 or order &#8212;1) models.
    This idea, which is called exclusion, can be exploited to improve compression.
    A character that occurs at one level is excluded from all lower-order predictions, allowing a greater share of the probability space to be allocated to the other characters in these lower-order models (Bell, Cleary, and Witten 1990).
    For example, if the character b were to follow tobeornottobe it would be encoded with probabilities (1/2,1/2,3/26), without exclusion, leadi