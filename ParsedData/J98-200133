way, we used the Kappa statistic (Siegel and Castellan 1988), recently proposed by Carletta as a measure of agreement for discourse analysis (Carletta 1996).
    We also used a measure of per-class agreement that we introduced ourselves.
    We discuss these results below, after reviewing briefly how K is computed.
    3.3.2 The Kappa Statistic.
    Kappa is a test suitable for cases when the subjects have to assign items to one of a set of nonordered classes.
    The test computes a coefficient K of agreement among coders, which takes into account the possibility of chance agreement.
    It is dependent on the number of coders, number of items being classified, and number of choices of classes to be ascribed to items.
    The kappa coefficient of agreement between c annotators is defined as: where P(A) is the proportion of times the annotators agree and P(E) is the proportion of times that we would expect the annotators to agree by chance.
    When there is complete agreement among the raters, K = 1; if ther