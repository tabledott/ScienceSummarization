
	Scalable Inference And Training Of Context-Rich Syntactic Translation Models
		Statistical MT has made great progress in the last few years, but current translation models are weakon re-ordering and target language fluency.
		Syn tactic approaches seek to remedy these problems.In this paper, we take the framework for acquiring multi-level syntactic translation rules of (Gal ley et al, 2004) from aligned tree-string pairs, and present two main extensions of their approach: first, instead of merely computing a single derivation that minimally explains a sentence pair, we constructa large number of derivations that include contextually richer rules, and account for multiple interpretations of unaligned words.
		Second, we pro pose probability estimates and a training procedure for weighting these rules.
		We contrast differentapproaches on real examples, show that our esti mates based on multiple derivations favor phrasal re-orderings that are linguistically better motivated, and establish that our larger rule