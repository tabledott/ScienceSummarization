task-specific training is also needed in PairClass, anyway).
    Similar considerations apply to space: Compressed, our source corpora take about 21 GB, our best DM tensor (TypeDM) 1.1 GB (and optimized sparse tensor representations could bring this quantity down drastically, if the need arises).
    Perhaps more importantly, extracting features from the corpus requires a considerable amount of NLP know-how (to pre-process the corpus appropriately, to navigate a dependency tree, etc.
    ), whereas the DM representation of distributional data as weighted triples is more akin to other standard knowledge representation formats based on typed relations, which are familiar to most computer and cognitive scientists.
    Thus, a trained DM can become a general-purpose resource and be used by researchers beyond the realms of the NLP community, whereas applying PairClass requires a good understanding of various aspects of computational linguistics.
    This severely limits its interdisciplinary appeal.
    At a more 