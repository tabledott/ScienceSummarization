ing data into smaller units of knowledge can improve a model&#8217;s ability to generalize (Zhang et al., 2006).
    In the limit, to maximize the chances of covering arbitrary new data, a model should decompose the training data into the smallest possible units, and learn from them.1 For phrasebased models, this stipulation implies phrases of length one.
    If the model is a synchronous rewriting system, then it should be able to generate every training sentence pair as the yield of a binary1Many popular models learn from larger units at the same time, but the size of the smallest learnable unit is what&#8217;s important for our purposes. branching synchronous derivation tree, where every word-to-word link is generated by a different derivation step.
    For example, a model that uses production rules could generate the previous example using the synchronous productions and (V, V) &#8212;* (left, left).
    A problem arises when this kind of decomposition is attempted for the alignment in Figure 1(a).
    I