0), Katz (1987), and Church and Gale (1991).
    We carry out experiments over many training data sizes on varied corpora using both bigram and trigram models.
    We demonstrate that the relative performance of techniques depends greatly on training data size and n-gram order.
    For example, for bigram models produced from large training sets Church-Gale smoothing has superior performance, while Katz smoothing performs best on bigram models produced from smaller data.
    For the methods with parameters that can be tuned to improve performance, we perform an automated search for optimal values and show that sub-optimal parameter selection can significantly decrease performance.
    To our knowledge, this is the first smoothing work that systematically investigates any of these issues.
    In addition, we introduce two novel smoothing techniques: the first belonging to the class of smoothing models described by Jelinek and Mercer, the second a very simple linear interpolation method.
    While being relativ