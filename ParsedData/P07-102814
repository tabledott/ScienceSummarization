els is still surprising.
    After all, the generalization corpus of the similarity-based models is far larger than the corpus used for clustering.
    Given the learning curve in Figure 1 it is unlikely that the reason for the lower coverage is data sparseness.
    However, EM-based clustering is a soft clustering method, which relates every predicate and every headword to every cluster, if only with a very low probability.
    In similarity-based models, on the other hand, two words that have never been seen in the same argument slot in the generalization corpus will have zero similarity.
    That is, a similarity-based model can assign a level of preference for an argument rp and word wo only if R(wo) n R(Seen(rp)) is nonempty.
    Since the flexibility of similarity-based models extends to the vector space for computing similarities, one obvious remedy to the coverage problem would be the use of a less sparse vector space.
    Given the low error rates of similarity-based models, it may even be advisable 