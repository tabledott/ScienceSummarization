Y e&#8212;z as e&#8212;z Y 0.
    For negative z, the two functions behave quite differently. f (z) = e&#8212;z shows an exponentially growing cost function as z Y &#8212; oo.
    In contrast, as z Y &#8212;oo it can be seen that log(1 + e&#8212;z) Y log(e&#8212;z) = &#8212;z, so this function shows asymptotically linear growth for negative z.
    As a final remark, note that both f (z) = e&#8212;z and f (z) = log(1 + e&#8212;z) are convex in z, with the result that LogLoss(&#175;&#65533;) and ExpLoss(&#175;&#65533;) are convex in the parameters &#175;&#65533;.
    This means that there are no problems with local minima when optimizing these two loss functions.
    In this article we concentrate on feature selection methods: algorithms which aim to make progress in minimizing the loss functions LogLoss(&#175;&#65533;) and ExpLoss(&#175;&#65533;) while using a small number of features (equivalently, ensuring that most parameter values in Potential functions underlying ExpLoss, LogLoss, and Error.
    The graph