rue competition, we show that the most common result is for different segmentations to be recruited for different examples, overfitting the training data and overly determinizing the phrase translation estimates.
    In this work, we first define a novel (but not radical) generative phrase-based model analogous to IBM Model 3.
    While its exact training is intractable, we describe a training regime which uses wordlevel alignments to constrain the space of feasible segmentations down to a manageable number.
    We demonstrate that the phrase analogue of the Dice coefficient is superior to our generative model (a result also echoing previous work).
    In the primary contribution of the paper, we present a series of experiments designed to elucidate what re-estimation learns in this context.
    We show that estimates are overly determinized because segmentations are used in unintuitive ways for the sake of data likelihood.
    We comment on both the beneficial instances of segment competition (idioms) as wel