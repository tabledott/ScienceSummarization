 of smoothing techniques in language modeling of such scope: no other study has used multiple training data sizes, corpora, or has performed parameter optimization.
    We show that in order to completely 3To implement the baseline method, we just used the interp-held-out code as it is a special case.
    Written anew, it probably would have been about 50 lines. characterize the relative performance of two techniques, it is necessary to consider multiple training set sizes and to try both bigram and trigram models.
    Multiple runs should be performed whenever possible to discover whether any calculated differences are statistically significant.
    Furthermore, we show that sub-optimal parameter selection can also significantly affect relative performance.
    We find that the two most widely used techniques, Katz smoothing and Jelinek-Mercer smoothing, perform consistently well across training set sizes for both bigram and trigram models, with Katz smoothing performing better on trigram models produced fro