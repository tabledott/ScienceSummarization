mputationally viable.
    In Section 4 we show that co-training outperforms self-training, and that co-training is most beneficial when the seed set of manually created parses is small.
    Section 4.4 shows that co-training is possible even when the set of initially labelled data is drawn from a different distribution to either the unlabelled training material or the test set; that is, we show that co-training can help in porting a parser from one genre to another.
    Finally, section 5 reports summary results of our experiments.
  
  
    Co-training can be informally described in the following manner (Blum and Mitchell, 1998): Effectively, by picking confidently labelled data from each model to add to the training data, one model is labelling data for the other.
    This is in contrast to self-training, in which a model is retrained only on the labelled examples that it produces (Nigam and Ghani, 2000).
    Blum and Mitchell prove that, when the two views are conditionally independent given the label, and