2 gives a description of the HMM and its application in NER: HMM-based chunk tagger.
    Section 3 explains the word feature used to capture both the internal and external evidences.
    Section 4 describes the back-off schemes used to tackle the sparseness problem.
    Section 5 gives the experimental results of our system.
    Section 6 contains our remarks and possible extensions of the proposed work.
  
  
    Given a token sequence G1n = g1g2 g , the goal The second item in (2-1) is the mutual information between T1n and n simplify the computation of this item, we assume mutual information independence: The basic premise of this model is to consider the raw text, encountered when decoding, as though it had passed through a noisy channel, where it had been originally marked with NE tags.
    The job of our generative model is to directly generate the original NE tags from the output words of the noisy channel.
    It is obvious that our generative model is reverse to the generative model of traditional HM