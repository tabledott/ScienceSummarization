d edge weights represent the pairwise sentence similarity.
    In this framework, text segmentation corresponds to a graph partitioning that optimizes the normalized-cut criterion (Shi and Malik, 2000).
    This criterion measures both the similarity within each partition and the dissimilarity across different partitions.
    Thus, our approach moves beyond localized comparisons and takes into account long-range changes in lexical distribution.
    Our key hypothesis is that global analysis yields more accurate segmentation results than local models.
    We tested our algorithm on a corpus of spoken lectures.
    Segmentation in this domain is challenging in several respects.
    Being less structured than written text, lecture material exhibits digressions, disfluencies, and other artifacts of spontaneous communication.
    In addition, the output of speech recognizers is fraught with high word error rates due to specialized technical vocabulary and lack of in-domain spoken data for training.
    Finally, pe