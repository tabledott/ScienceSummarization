(Viterbi tag sequence).
    The table below shows tag assignments (and their counts in parentheses) for a few word types which occur frequently in the test corpus.
    We see how the rare tag labels (like FW, SYM, etc.) are abused by EM.
    As a result, many word tokens which occur very frequently in the corpus are incorrectly tagged with rare tags in the EM tagging output.
    We also look at things more globally.
    We investigate the Viterbi tag sequence generated by EM training and count how many distinct tag bigrams there are in that sequence.
    We call this the observed grammar size, and it is 915.
    That is, in tagging the 24,115 test tokens, EM uses 915 of the available 45 x 45 = 2025 tag bigrams.2 The advantage of the observed grammar size is that we sequence.
    Here, we show a sample word sequence and the corresponding IP network generated for that sequence. can compare it with the gold tagging&#8217;s observed grammar size, which is 760.
    So we can safely say that EM is learning a gramma