he test set.
    While better taggers exist, we believe that the simpler HMM tagger overfits less, and is more representative of the tagging performance on nonWSJ corpus texts.
    Parsers We evaluate our parser against the transition-based MALT parser and the graph-based MST parser.
    We use version 1.2 of MALT parser8, with the settings used for parsing English in the CoNLL 2007 shared task.
    For the MST parser9, we use the default first-order, projective parser settings, which provide state-of-the-art results for English.
    All parsers are trained and tested on the same data.
    Our parser is trained for 20 iterations.
    Evaluation Measures We evaluate the parsers using three common measures: (unlabeled) Accuracy: percentage of tokens which got assigned their correct parent.
    Root: The percentage of sentences in which the ROOT attachment is correct.
    Complete: the percentage of sentences in which all tokens were assigned their correct parent.
    Unlike most previous work on English depende