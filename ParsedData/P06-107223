omatic, efficient methods for model selection.
    For annealing, inspiration may be drawn from continuation methods; see, e.g., Elidan and Friedman (2005).
    Ideally one would like to select values simultaneously for many hyperparameters, perhaps using a small annotated corpus (as done here), extrinsic figures of merit on successful learning trajectories, or plausibility criteria (Eisner and Karakos, 2005).
    Grammar induction serves as a tidy example for structural annealing.
    In future work, we envision that other kinds of structural bias and annealing will be useful in other difficult learning problems where hidden structure is required, including machine translation, where the structure can consist of word correspondences or phrasal or recursive syntax with correspondences.
    The technique bears some similarity to the estimation methods described by Brown et al. (1993), which started by estimating simple models, using each model to seed the next.
  
  
    We have presented a new unsupervised pa