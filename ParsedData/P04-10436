 the parse-tree.
    This prevents the learning algorithm to generalize well on unseen data.
    In order to address this problem, the next section describes a novel kernel space for predicate argument classification.
    Given a vector space in Rn and a set of positive and negative points, SVMs classify vectors according to a separating hyperplane, H(x) = w&#65533;x x&#65533;+ b = 0, where w&#65533; E Rn and b E Rare learned by applying the Structural Risk Minimization principle (Vapnik, 1995).
    To apply the SVM algorithm to Predicate Argument Classification, we need a function O :F &#8212; Rn to map our features space F = {f1, .., f|F|} and our predicate/argument pair representation,( Fp ,a =(( Fz, into Rn, such that: Fz &#8212; O(Fz) = (01(Fz), .., On(Fz)) From the kernel theory we have that: where, Fi Vi E {1, .., l} are the training instances and the product K(Fi, Fz) =&lt;O(Fi) &#183; O(Fz)&gt; is the kernel function associated with the mapping O.
    The simplest mapping that we can apply is O(Fz) =