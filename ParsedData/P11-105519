l. (2010) as well as our new model (MULTIR).
    We also compare with SOLOR, a reimplementation of their algorithm, which we built in Factorie (McCallum et al., 2009), and will use later to evaluate sentential extraction.
    MULTIR achieves competitive or higher precision over all ranges of recall, with the exception of the very low recall range of approximately 01%.
    It also significantly extends the highest recall achieved, from 20% to 25%, with little loss in precision.
    To investigate the low precision in the 0-1% recall range, we manually checked the ten highest confidence extractions produced by MULTIR that were marked wrong.
    We found that all ten were true facts that were simply missing from Freebase.
    A manual evaluation, as we perform next for sentential extraction, would remove this dip.
    Although their model includes variables to model sentential extraction, Riedel et al. (2010) did not report sentence level performance.
    To generate the precision / recall curve we used the join