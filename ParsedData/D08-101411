followed by a projection of the resulting labels back into the target language.
    Unlike the previous three experiments, in this experiment we only generate subjectivity-annotated resources, and we do not build and evaluate a standalone subjectivity analysis tool for the target language.
    Further training of a machine learning algorithm, as in experiments two and three, is required in order to build a subjectivity analysis tool.
    Thus, this fourth experiment is an evaluation of the resources generated in the target language, which represents an upper bound on the performance of any machine learning algorithm that would be trained on these resources.
    Figure 4 illustrates this experiment.
  
  
    Our initial evaluations are carried out on Romanian.
    The performance of each of the three methods is evaluated using a dataset manually annotated for subjectivity.
    To evaluate our methods, we generate a Romanian training corpus annotated for subjectivity on which we train a subjectivity classifier