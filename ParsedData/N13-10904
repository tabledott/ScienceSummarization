models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).
    This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models.
    This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).
    Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations.
  
  
    The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1.
    This architecture consists of an input layer, a hidden layer with