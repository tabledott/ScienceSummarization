o q(x).
    The overall divergence between p and q is the average divergence, where the averaging is over tree (tokens) in the corpus; i.e., point divergences 1n03(x)/q(x)) are weighted by /5(x) and summed.
    For example, let qi be, as before, the distribution determined by model M1.
    Table 1 shows qi, 17, the ratio qi (x)/13(x), and the weighted point divergence /3(x) ln(f 9(x) /q1(x)).
    The sum of the fourth column is the KL divergence D(Pliqi) between /3 and qi.
    The third column contains qi (x)/(x) rather than 17(x)/qi(x) so that one can see at a glance whether qi (x) is too large (&gt; 1) or too small (&lt; 1).
    The total divergence D(7'311(11) = 0.32.
    One set of weights is better than another if its divergence from the empirical distribution is less.
    For example, let us consider a different set of weights for grammar G1.
    Let M' be G1 with weights (1/2, 1/2, 1/2, 1/2, 1/2, 1/2), and let q' be the probability distribution determined by M'.
    Then the computation of the KL diver