.
  At each unit j ,  the weighted inlmt activations aiwij are summed and a bias pa- rameter Oj is added.
  net i = ~ aiwlj + Oj (1) t The resulting network input ,telj is then l)~uqsed through a sigmoid fimction (the logistic funclion) in order to restrict the value range of the resulting acti- vation aj to the interval [0,i].
  ~, (:~) The network learns by adapting the weights of the connections between units, tmtil the correct output is t~rocluced.
  One widely used method is the backl.o p- ~gation algorithm which performs a gradient descent search on the error surface, The weight update ~XlOij , i.e.
  the difference between the old and the new value of weight wij, is here defined ,~s: AWij - -  rlapi6pj, where { ,,pj(1 --,,,)(t,,j - "p J ) , if j is an output unit a,,~ = ,,vj(l _avs)~vk,oik, (a) k if j is a hidden unit Ilere, Zp is the target output vector which the network lnnst learn t .
  "Daining the MLP-network with the backpropagao tion rule guarantees that a local minimum of the er- ror surface i