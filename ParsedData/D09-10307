her, assigning scores from best to worst and allowing ties.
    We evaluate non-expert Turker judges by measuring their inter-annotator agreement with the WMT08 expert judges, and by comparing the correlation coefficient across the rankings of the machine translation systems produced by the two sets of judges. equal.
    The quality of their works varies.
    Figure 2 shows the agreement of individual Turkers with expert annotators, plotted against the number of HITs they completed.
    The figure shows that their agreement varies considerably, and that Turker who completed the most judgments was among the worst performing.
    To avoid letting careless annotators drag down results, we experimented with weighted voting.
    We weighted votes in two ways: Turker agreed with the rest of the Turkers over the whole data set.
    This does not require any gold standard calibration data.
    It goes beyond simple voting, because it looks at a Turker&#8217;s performance over the entire set, rather than on an item-by