.
    In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation.
    The major difference between our approach and theirs is that we only penalize deviation from the source parameters for the weights v of projected features, while they work with the weights of the original features only.
    For our small amount of labeled target data, attempting to penalize w using ws performed no better than our baseline.
    Because we only need to learn to ignore projections that misalign features, we can make much better use of our labeled data by adapting only 50 parameters, rather than 200,000.
    Table 3 summarizes the results of sections 4 and 5.
    Structural correspondence learning reduces the error due to transfer by 21%.
    Choosing pivots by mutual information allows us to further reduce the error to 36%.
    Finally, by adding 50 instances of target domain data and using this to correct the misaligned projections, we achieve an average relative reduction 