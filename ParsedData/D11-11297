6;beer&#8217; and &#8216;sherry&#8217; occur within the 1E.g. words which appear in the same sentence or n-word window, or words which hold particular grammatical or dependency relations to the word being learned. context of identifying words such as &#8216;drink&#8217;, &#8216;alcoholic&#8217; and &#8216;hangover&#8217; more frequently than they occur with other content words.
    Such context distributions can be encoded as vectors in a high dimensional space with contexts as &#8722;&#8722;&#8594; basis vectors.
    For any word vector word, the scalar weight cword iassociated with each context basis vector &#8594;&#8722;ni is a function of the number of times the word has appeared in that context.
    Semantic vectors (Ci ord, c2 ord , cnord) are also denoted by sums lof such weight/basis vector pairs: Learning a semantic vector is just learning its basis weights from the corpus.
    This setting offers geometric means to reason about semantic similarity (e.g. via cosine measure or k-means clustering), as 