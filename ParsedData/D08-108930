at orientation predictionsof the hierarchical model are consistent across train ing and testing, which is not the case for the otherordering models discussed in this paper (see Sec tion 4).
			Overall, hierarchical models are the most effective on the two sets: their best performances on MT06 and MT05 are respectively 45.64 and 56.07.
			The best non-hierarchical models obtain only 45.01 and 55.52 respectively for the same sets.
			All thesedifferences (i.e., .63 and .55) are statistically signifi cant at the .05 level.
	
	
			In this paper, we presented a lexicalized orientation model that enables phrase movements that are more complex than swaps between adjacent phrases.
			This model relies on a hierarchical structure that is builtas a by-product of left-to-right phrase-based decod ing without increase of asymptotic running time.
			Weshow that this model provides statistically signifi cant improvements for five NIST evaluation sets and for two language pairs.
			In future work, we plan to extend the param