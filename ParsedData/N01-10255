rplane by introducing a soft margin parameter , which trades off between the training error and the magnitude of the margin.
    Furthermore, SVMs have a potential to carry out the non-linear classification.
    Though we leave the details to (Vapnik, 1998), the optimization problem can be rewritten into a dual form, where all feature vectors appear in their dot products.
    By simply substituting every dot product of and in dual form with a certain Kernel function , SVMs can handle non-linear hypotheses.
    Among many kinds of Kernel functions available, we will focus on the -th polynomial kernel: .
    Use of-th polynomial kernel functions allows us to build an optimal separating hyperplane which takes into account all combinations of features up to.
    Statistical Learning Theory(Vapnik, 1998) states that training error (empirical risk) and test error (risk) hold the following theorem.
    Theorem 1 (Vapnik) If is the VC dimension ofthe class functions implemented by some machine learning algorithms, th