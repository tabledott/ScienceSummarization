 these hypothesis pairs are not i.i.d. data samples.
    Therefore, GIZA++ training on such a data set may be unreliable.
    Bangalore et al. (2001) used a multiple stringmatching algorithm based on Levenshtein edit distance, and later Sim et al.
    (2007) and Rosti et al. (2007) extended it to a TER-based method for hypothesis alignment.
    TER (Snover et al., 2006) measures the minimum number of edits, including substitution, insertion, deletion, and shift of blocks of words, that are needed to modify a hypothesis so that it exactly matches the other hypothesis.
    The best alignment is the one that gives the minimum number of translation edits.
    TER-based confusion network construction and system combination has demonstrated superior performance on various large-scale MT tasks (Rosti. et al, 2007).
    However, when searching for the optimal alignment, the TER-based method uses a strict surface hard match for counting edits.
    Therefore, it is not able to handle synonym matching well.
    Moreover