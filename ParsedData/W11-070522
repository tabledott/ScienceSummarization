model that uses about 10,000 features.
    We also experiment with combination of models.
    Combining unigrams with Senti-features outperforms the combination of kernels with Senti-features by 0.78%.
    This is our best performing system for the positive versus negative task, gaining about 4.04% absolute gain over a hard unigram baseline.
    Table 6 presents classifier accuracy and F1measure when features are added incrementally.
    We start with our baseline unigram model and subsequently add various sets of features.
    First, we add all non-polar features (rows f5, f6, f7, f10, f11 in Table 4) and observe no improvement in the performance.
    Next, we add all part-of-speech based features (rows f1, f8) and observe a gain of 3.49% over the unigram baseline.
    We see an additional increase in accuracy by 0.55% when we add other prior polarity features (rows f2, f3, f4, f9 in Table 4).
    From these experiments we conclude that the most important features in Senti-features are those that involve pri