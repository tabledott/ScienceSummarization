l non-experts and average expert judgment is only 0.496, implying that an arbitrary non-expert is not very likely to correlate well with average expert judgments.
    Experts are better predictors for each other&#8217;s judgments (0.799) than non-experts (0.609).
    Interestingly, it turns out that an arbitrary NIST-5 run is a better predictor (0.822) of average expert opinion than an arbitrary single expert (0.799).
    The number of forecasts we were able to use in our human experiments was small, and to back up the results presented in Table 2 we report NIST-5, BLEU-4, ROUGE-4 and SE scores averaged across the five test sets from the pCRU validation runs, in Table 4.
    The picture is similar to results for the smaller data set: the rankings assigned by all metrics are the same, except that NIST-5 and SE have swapped the ranks of SUMTIME-Hybrid and pCRU-roulette.
    Pair-wise AR tests showed all differences to be significant with p &lt; 0.05, except for the differences in BLEU, NIST and ROUGE scores for