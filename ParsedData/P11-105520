t model assignment score for each of the sentences that contributed to the aggregate extraction decision.
    Figure 4 shows approximate precision / recall curves for MULTIR and SOLOR computed against manually generated sentence labels, as defined in Section 6.3.
    MULTIR achieves significantly higher recall with a consistently high level of precision.
    At the highest recall point, MULTIR reaches 72.4% precision and 51.9% recall, for an F1 score of 60.5%.
    Since the data contains an unbalanced number of instances of each relation, we also report precision and recall for each of the ten most frequent relations.
    Let SM be the sentences where MULTIR extracted an instance of relation r E R, and let Sr be the sentences that match the arguments of a fact about relation r in A.
    For each r, we sample 100 sentences from both SM and Sr and manually check accuracy.
    To estimate precision Pr we compute the ratio of true relation mentions in SM , and to estimate recall Rr we take the ratio of true relat