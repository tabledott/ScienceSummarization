t is likely that it would yield good performance on names if lists of names were provided as supplementary training text.
    This has not yet been tested.
  
  
    Statistical language models are well developed in the field of text compression.
    Compression methods are usually divided into symbolwise and dictionary schemes (Bell, Cleary, and Witten, 1990).
    Symbolwise methods, which generally make use of adaptively generated statistics, give excellent compression&#8212;in fact, they include the best known methods.
    Although dictionary methods such as the Ziv-Lempel schemes perform less well, they are used in practical compression utilities like Unix compress and gzip because they are fast.
    In our work we use the prediction by partial matching (PPM) symbolwise compression scheme (Cleary and Witten 1984), which has become a benchmark in the compression community.
    It generates &amp;quot;predictions&amp;quot; for each input symbol in turn.
    Each prediction takes the form of a probability dis