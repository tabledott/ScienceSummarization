Niemann 1999).
    Training neural networks directly with posterior probability (Ries 1999a) seems to be a more principled approach and it also offers much easier integration with other knowledge sources.
    Prosodic features, for example, can simply be added to the lexical features, allowing the model to capture dependencies and redundancies across knowledge sources.
    Keyword-based techniques from the field of message classification should also be applicable here (Rose, Chang, and Lippmann 1991).
    Eventually, it is desirable to integrate dialogue grammar, lexical, and prosodic cues into a single model, e.g., one that predicts the next DA based on DA history and all the local evidence.
    The study of automatically extracted prosodic features for DA modeling is likewise only in its infancy.
    Our preliminary experiments with neural networks have shown that small gains are obtainable with improved statistical modeling techniques.
    However, we believe that more progress can be made by improving the