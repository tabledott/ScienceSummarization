g fewer spurious off-diagonal alignments, than the heuristic (see Figure 5), and thus produce far more valid phrases/rules.
    We now test our model&#8217;s performance on a larger corpus, representing a realistic SMT experiment with millions of words and long sentences.
    The Chinese-English training data consists of the FBIS corpus (LDC2003E14) and the first 100k sentences from the Sinorama corpus (LDC2005E47).
    The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c &gt; 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02).
    The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008).
    The Arabic text was preprocessed according to the D2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size.
    The parameters of the NIST systems were tuned using Och&#8217;s algorithm to max