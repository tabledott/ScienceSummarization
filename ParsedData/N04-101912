ighest weighting factor).
    Then the pyramid score P is the ratio of D to Max.
    Because P compares the actual distribution of SCUs to an empirically determined weighting, it provides a direct correlate of the way human summarizers select information from source texts.
    We aimed for an annotation method requiring relatively little training, and with sufficient interannotator reliability to produce a stable pyramid score.
    Here we present results indicating good interannotator reliability, and pyramid scores that are robust across annotations.
    SCU annotation involves two types of choices: extracting a contributor from a sentence, and assigning it to an SCU.
    In a set of four summaries about the Philippine Airlines (PAL), two coders (C1 and C2; the co-authors) differed on the extent of the following contributor: {Crafter 1C2the ground crew union turned down a settlement}Cr which}C2.
    Our approach is to separate syntactic from semantic agreement, as in (Klavans et al., 2003).
    Because cons