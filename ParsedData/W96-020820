ds are slowest, C4.5 and perceptron intermediate, and naive Bayes the fastest.
    Since it just stores examples, training time for Nearest Neighbor is always zero.
    In general, connectionist methods are much slower to train than alternative techniques (Shavlik et al., 1991); however, in this case a simple perceptron converges quite rapidly.
    With respect to testing time, the symbolic induction methods are fastest and almost indistinguishable from zero in Figure 3 since they only need to test a small subset of the features.
    4 All visible differences in the graph are significant.
    Naive Bayes is the slowest; both it and perceptron have the constant overhead of computing a weighted function over all of the almost 3,000 features.
    Nearest neighbor grows linearly with the number of training instances as expected; more sophisticated indexing methods can reduce this to logarithmic expected time (Friedman, Bentley, &amp; Finkel, 1977).5
  
  
    Naive Bayes and perceptron are similar in that they bo