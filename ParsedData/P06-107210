ent &#948;0 values are plotted in Fig.
    2.
    Generally speaking, &#948;0 &#171; 0 performs better.
    There is consistently an early increase in performance as &#948; increases, but the stopping &#948;f matters tremendously.
    Selected annealed-&#948; models surpass EM in all six languages; see the third column of Table 3.
    Note that structural annealing does not always outperform fixed-&#948; training (English and Portuguese).
    This is because we only tested a few values of &#948;0, since annealing requires longer runtime.
  
  
    A related way to focus on local structure early in learning is to broaden the set of hypotheses to include partial parse structures.
    If x = (x1, x2, ..., xn), the standard approach assumes that x corresponds to the vertices of a single dependency tree.
    Instead, we entertain every hypothesis in which x is a sequence of yields from separate, independently-generated trees.
    For example, (x1, x2, x3) is the yield of one tree, (x4, x5) is the with structural a