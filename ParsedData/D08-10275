ne vision tasks and report speed and costs similar to our findings; their summaries of worker behavior also corroborate with what we have found.
    In general, volunteer-supplied or AMT-supplied data is more plentiful but noisier than expert data.
    It is powerful because independent annotations can be aggregated to achieve high reliability.
    Sheng et al. (2008) explore several methods for using many noisy labels to create labeled data, how to choose which examples should get more labels, and how to include labels&#8217; uncertainty information when training classifiers.
    Since we focus on empirically validating AMT as a data source, we tend to stick to simple aggregation methods.
  
  
    In this section we describe Amazon Mechanical Turk and the general design of our experiments.
    We employ the Amazon Mechanical Turk system in order to elicit annotations from non-expert labelers.
    AMT is an online labor market where workers are paid small amounts of money to complete small tasks.
    The des