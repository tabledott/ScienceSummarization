put ing kappa.
			Despite the rigorous training regimen, kappa ranged from 0.411 to 0.786, with an overall combined value of 0.630.
			Of the prepositions that Rater 1 judged to be errors, Rater 2 judged 30.2% to be acceptable.
			Conversely, of the prepositions Rater 2 judged to be erroneous, Rater 1 found 38.1% acceptable.
			The kappa of 0.630 shows the difficulty of this task and also shows how two highly trained raters can produce very different judgments.
			Details on our annotation and human judgment experiments can be found in (Tetreault and Chodorow, 2008).
			Variability in raters?
			judgments translates to variability of system evaluation.
			For instance, in our previous work (Chodorow et al, 2007), wefound that when our system?s output was com pared to judgments of two different raters, therewas a 10% difference in precision and a 5% differ ence in recall.
			These differences are problematicwhen evaluating a system, as they highlight the potential to substantially over- or under-estimate per f