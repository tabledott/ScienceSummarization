n.
    In section 4.3, we present a manual evaluation of several paraphrasing methods and show a close connection between intrinsic and extrinsic assessments of these methods.
    We begin by describing relevant background information, including the BLEU evaluation method, the test data set, and the alternative paraphrasing methods considered in our experiments.
  
  
    BLEU is the basic evaluation measure that we use in our experiments.
    It is the geometric average of the n-gram precisions of candidate sentences with respect to the corresponding reference sentences, times a brevity penalty.
    The BLEU score is computed as follows: where pn is the n-gram precision, c is the cardinality of the set of candidate sentences and r is the size of the smallest set of reference sentences.
    To augment BLEU evaluation with paraphrasing information, we substitute each reference with the corresponding synthetic reference.
  
  
    We use the Chinese portion of the 2004 NIST MT dataset.
    This portion contains