 We used GIZA++ package (Och and Ney, 2003) to train IBM translation models.
    The training scheme is 15H535, which means that Model 1 are trained for five iterations, HMM model for five iterations and finally Model 3 for five iterations.
    Except for changing the iterations for each model, we use default configuration of GIZA++.
    After that, we used three types of methods for performing a symmetrization of IBM models: intersection, union, and refined methods (Och and Ney , 2003).
    The base feature of our log-linear models, IBM Model 3, takes the parameters generated by GIZA++ as parameters for itself.
    In other words, our loglinear models share GIZA++ with the same parameters apart from POS transition probability table and bilingual dictionary.
    Table 2 compares the results of our log-linear models with IBM Model 3.
    From row 3 to row 7 are results obtained by IBM Model 3.
    From row 8 to row 12 are results obtained by log-linear models.
    As shown in Table 2, our log-linear models ach