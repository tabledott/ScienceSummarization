eoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008).
    Socher et al. (2011a) and Socher et al.
    (2011b) present a framework based on recursive neural networks that learns vector space representations for multi-word phrases and sentences.
    The network is given a list of word vectors as input and a binary tree representing their syntactic structure.
    Then, it computes an n-dimensional representation p of two n-dimensional children and the process is repeated at every parent node until a representation for a full tree is constructed.
    Parent representations are computed essentially by concatenating the representations of their children.
    During training, the model tries to minimize the reconstruction errors between the n-dimensional parent vectors and those representing their children.
    This model can also compute co