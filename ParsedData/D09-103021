ions, ranging from 3&#8211;6 questions per article.
    25 Turkers answered questions about each translated article.
    To avoid them answering the questions multiple times, we randomly selected which system&#8217;s translation was shown to them.
    Each system&#8217;s translation was displayed an average of 5 reference 0.94 google.fr-en 0.85 google.de-en 0.80 rbmt5.de-en 0.77 geneva.de-en 0.63 jhu-tromble.de-en 0.50 times per article.
    As a control, we had three Turkers answer the reading comprehension questions using the reference translation.
    Table 3 gives the percent of questions that were correctly answered using each of the different systems&#8217; outputs and using the reference translation.
    The ranking is exactly what we would expect, based on the HTER scores and on the human evaluation of the systems in WMT09.
    This again helps to validate that the reading comprehension methodology.
    The scores are more interpretable than Blue scores and than the WMT09 relative rankings, since it g