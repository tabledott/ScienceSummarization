develop and evaluate novel solutions for textual entailment.
  
  
    Crowdsourcing services, such as Amazon Mechanical Turk6 (MTurk) and CrowdFlower7, have been recently used with success for a variety of NLP applications (Callison-Burch and Dredze, 2010).
    The idea is that the acquisition and annotation of large amounts of data needed to train and evaluate NLP tools can be carried out in a cost-effective manner by defining simple Human Intelligence Tasks (HITs) routed to a crowd of non-expert workers (aka &#8220;Turkers&#8221;) hired through on-line marketplaces.
    As regards textual entailment, the first work exploring the use of crowdsourcing services for data annotation is described in (Snow et al., 2008), which shows high agreement between non-expert annotations of the RTE-1 dataset and existing gold standard labels assigned by expert labellers.
    Focusing on the actual generation of monolingual entailment pairs, (Wang and Callison-Burch, 2010) experiments the use of MTurk to collect facts and c