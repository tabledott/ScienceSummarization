aining data tokens selected are displayed in Figure 1.
    (Note that the training data token counts are displayed on a logarithmic scale.)
    The test set perplexity for the language model trained on the full Gigaword corpus is 135.
    As we might expect, reducing training data by random sampling always increases perplexity.
    Selecting Gigaword sentences by their cross-entropy according to the Europarl-trained model is effective in reducing both test set perplexity and training corpus size, with an optimum perplexity of 124, obtained with a model built from 36% of the Gigaword corpus.
    Klakow&#8217;s method is even more effective, with an optimum perplexity of 111, obtained with a model built from 21% of the Gigaword corpus.
    The cross-entropy difference selection method, however, is yet more effective, with an optimum perplexity of 101, obtained with a model built from less than 7% of the Gigaword corpus.
    The comparisons implied by Figure 1, however, are only approximate, because each perplex