a maximum entropy model which is a common choice for incorporating various types of features for classification problems in natural language processing (Berger et al, 1996).
			Regularization is important in maximum entropy modeling to avoid overfitting to the training data.
			For this purpose, we use the maximum entropy modeling with inequality constraints (Kazama andTsujii, 2003).
			The model gives equally good per formance as the maximum entropy modeling with Gaussian priors (Chen and Rosenfeld, 1999), and the size of the resulting model is much smaller thanthat of Gaussian priors because most of the param eters become zero.
			This characteristic enables us to easily handle the model data and carry out quick decoding, which is convenient when we repetitivelyperform experiments.
			This modeling has one param eter to tune, which is called the width factor.
			We tuned this parameter using the development data in each type of experiments.
			470 Current word wi &amp; ti Previous word wi?1 &amp; ti Next wo