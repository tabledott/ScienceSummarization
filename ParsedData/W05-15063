 are often cascades of mod ules, where we want to optimize the modules?
			objectivefunctions jointly.
			However, often a module is incompati ble with the packed representation of the previous module due to factors like non-local dependencies.
			So we might want to postpone some disambiguation by propagating k-best lists to subsequent phases, as in joint parsing and semantic role labeling (Gildea and Jurafsky, 2002; Suttonand McCallum, 2005), information extraction and coreference resolution (Wellner et al, 2004), and formal se mantics of TAG (Joshi and Vijay-Shanker, 1999).Moreover, much recent work on discriminative training uses k-best lists; they are sometimes used to approximate the normalization constant or partition func tion (which would otherwise be intractable), or to train a model by optimizing some metric incompatible with the packed representation.
			For example, Och (2003) showshow to train a log-linear translation model not by max imizing the likelihood of training data, but maximizing the B