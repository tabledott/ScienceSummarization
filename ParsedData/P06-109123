ne in Table 3. ing word-based features shown in line and line training takes less than days.
    Finally, the training for the most complex model in line takes about days.
    Figure 2 shows the BLEU performance for the model corresponding to line in Table 3 as a function of the number of training iterations.
    By adding top scoring alternatives in the training algorithm in Table 2, the BLEU performance on the training data improves from about for the initial model to about for the best model after iterations.
    After each training iteration the test data is decoded as well.
    Here, the BLEU performance improves from for the initial model to about for the final model (we do not include the test data block sequences in the training).
    Table 3 shows a typical learning curve for the experiments in Table 3: the training BLEU score is much higher than the test set BLEU score despite the fact that the test set uses reference translations.
  
  
    The work in this paper substantially differs from previous