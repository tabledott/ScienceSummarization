xpressed as follows w E aiyi xi b = w &#8226; xi &#8212; yi.
    /;xiEsvs The elements of the set SVs are the Support Vectors that lie on the separating hyperplanes.
    Finally, the decision function f : &#8212;&gt; {&#177;1} can be written as: high accuracy without falling into over-fitting even with a large number of words taken as the features (Joachims, 1998; Taira and Haruno, 1999).
    In this paper, we propose an application of SVMs to Japanese dependency structure analysis.
    We use the features that have been studied in conventional statistical dependency analysis with a little modification on them.
  
  
    Let us define the training data which belong either to positive or negative class as follows. xi is a feature vector of i-th sample, which is represented by an n dimensional vector (xi = (f1, fn) E Rn). yi is a scalar value that specifies the class (positive(+1) or negative(1) class) of i-th data.
    Formally, we can define the pattern recognition problem as a learning and building process o