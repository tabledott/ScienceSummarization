eded to store the additional negative features.
			Accordingly, the algorithm first extracts the features for each training instance, then maps the features to indexes for the weight vector with the hash function and calculates the weight arrays.
			Algorithm 3: Training ? Hash Kernel for n?
			1 to E // iteration over the training epochs for i?
			1 to I // iteration over the training exmaples k ?
			(n? 1) ? I + i ? ?
			E ? I ? k + 2 // passive-aggressive weight tse,k; A?
			extr.-features-&amp;-calc-arrays(i,??w ) ; tee,k tsp,k; yp?
			predicte-projective-parse-tree(A);tep,k tsa,k; ya?
			non-projective-approx.(yp ,A); tea,k update ??w , ??v according to ?(yp, yi) and ? w = v/(E ? I) // average For different j, the hash function h(j) might generate the same value k. This means that the hash function maps more than one feature to thesame weight.
			We call such cases collisions.
			Col lisions can reduce the accuracy, since the weights are changed arbitrarily.
			This procedure is similar to randomization 