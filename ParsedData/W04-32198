hodology, two annotators each created an initial annotation for each dataset, subcategorizing alignments as either SURE (necessary) or POSSIBLE (allowed, but not required).
    Differences were highlighted and the annotators were asked to review their choices on these differences.
    Finally we combined the two annotations into a single gold standard: if both annotators agreed that an alignment should be SURE, then the alignment was marked as SURE in the gold-standard; otherwise the alignment was marked as POSSIBLE.
    To compute Precision, Recall, and Alignment Error Rate (AER) for the twin datasets, we used exactly the formulae listed in Och &amp; Ney (2003).
    Let A be the set of alignments in the comparison, S be the set of SURE alignments in the gold standard, and P be the union of the SURE and POSSIBLE alignments in the gold standard.
    Then we have: Measured in terms of AER4, final interrater agreement between the two annotators on the 250 sentences was 93.1%.
    Table 1 shows the results of eva