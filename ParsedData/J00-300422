single character that has occurred before reappears is greater than the 1 &#8212; 1/(2n) implied by the above estimator.
    Consequently, in this case the probability is increased in an ad hoc manner to 1 &#8212; 1/(6n).
    Inserting spaces into text can be viewed as a hidden Markov modeling problem.
    Being entirely adaptive, the method works regardless of what language it is used with.
    For pedagogical purposes, we will explain it with English text.
    Between every pair of characters lies a potential space.
    Figure 4(a) illustrates the model for the fragment tobeornottobe.
    It contains one node for each letter in the text and one for each possible intercharacter space (represented as dots &#8226; in the figure).
    Any given assignment of word boundaries to this text fragment will correspond to a path through the model from beginning (at the left) to end (at the right).
    Of all possible paths, we seek the one that gives the best compression according to the PPM text compression method, su