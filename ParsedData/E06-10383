ntended meaning.
    The paper is organized as follows: Section 2 discusses previous approaches to sentence compression.
    In particular, we discuss the advantages and disadvantages of the models of Knight and Marcu (2000).
    In Section 3 we present our discriminative large-margin model for sentence compression, including the learning framework and an efficient decoding algorithm for searching the space of compressions.
    We also show how to extract a rich feature set that includes surfacelevel bigram features of the compressed sentence, dropped words and phrases from the original sentence, and features over noisy dependency and phrase-structure trees for the original sentence.
    We argue that this rich feature set allows the model to learn which words and phrases should be dropped and which should remain in the compression.
    Section 4 presents an experimental evaluation of our model compared to the models of Knight and Marcu (2000) and finally Section 5 discusses some areas of future work.
  
  
 