ately, they could keep the soft clustering, with the representation for a particular word token being the posterior probability distribution over the states.)
    However, the CRF chunker in Huang and Yates (2009), which uses their HMM word clusters as extra features, achieves F1 lower than a baseline CRF chunker (Sha &amp; Pereira, 2003).
    Goldberg et al. (2009) use an HMM to assign POS tags to words, which in turns improves the accuracy of the PCFG-based Hebrew parser.
    Deschacht and Moens (2009) use a latent-variable language model to improve semantic role labeling.
  
  
    Another approach to word representation is to learn a distributed representation.
    (Not to be confused with distributional representations.)
    A distributed representation is dense, lowdimensional, and real-valued.
    Distributed word representations are called word embeddings.
    Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties.
    A di