  The first three rows provide baselines, as mentioned in the previous section.
    Our primary baseline is MONOLINGUAL, which is the monolingual counterpart to our model and only uses the language-specific distributions E or F. The next three rows shows the performance of various bilingual models that don&#8217;t use character-tocharacter phonetic correspondences to capture cognate information.
    We find that with the exception of the HEBREW(+ARAMAIC) pair, the bilingual models show marked improvement over MONOLINGUAL.
    We notice that in general, adding English &#8211; which has comparatively little morphological ambiguity &#8211; is about as useful as adding a more closely related Semitic language.
    However, once characterto-character phonetic correspondences are added as an abstract morpheme prior (final two rows), we find the performance of related language pairs outstrips English, reducing relative error over MONOLINGUAL by 10% and 24% for the Hebrew/Arabic pair.
  
  
    We started out by posin