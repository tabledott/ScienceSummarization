as trained on the English part of the bilingual training corpus and additional monolingual English data from the GigaWord corpus.
    The total amount of language model training data was about 600M running words.
    We use a fourgram language model with modified Kneser-Ney smoothing as implemented in the SRILM toolkit (Stolcke, 2002).
    For the four English reference translations of the evaluation sets, the accumulated statistics are presented.
    In this section, we present the classification results for the three language pairs.
    In Table 4, we present the classification results for two orientation classes.
    As baseline we always choose the most frequent orientation class.
    For Arabic-English, the baseline is with 6.3% already very low.
    This means that the word order in Arabic is very similar to the word order in English.
    For Chinese-English, the baseline is with 12.7% about twice as large.
    The most differences in word order occur for Japanese-English.
    This seems to be reasonabl