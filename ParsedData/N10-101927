e speakers?
    2.
    How much error-annotated data is sufficient to produce positive results using that approach?
    We have shown that a meta-classifier approach outperforms using a language model or a classifier alone.
    An interesting side result is that the language model solidly outperforms the contextual classifier for both article and preposition correction, contrary to current practice in the field.
    Training data requirements for the meta-classifier vary significantly between article and preposition error detection.
    The article meta-classifier can be trained with as few as 600 annotated errors, but the preposition meta-classifier requires more annotated data by an order of magnitude.
    Still, the overall amount of expensive error-annotated data is relatively small, and the meta-classification approach makes it possible to leverage large amounts of wellformed text in the primary models, tuning to the non-native domain in the meta-classifier.
    We believe that the logical next step is t