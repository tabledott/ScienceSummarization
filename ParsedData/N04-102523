 curve of our classifier as a function of the mean model training set size in tokens.
    The lowest mean RMS error of 1.92 was achieved at the maximum training set size threshold of 32,000 tokens per grade model.
    We fit a monotonically decreasing power-law function to the data points (Duda et al. 2001, p. 492).
    This gave extrapolated estimates for mean RMS error of about 1.79 at 64,000 tokens per model, 1.71 at 128,000 tokens per model, and 1.50 at 1,000,000 tokens per model.
    While doubling the current mean training set size to 64,000 tokens per model would give a useful reduction in RMS error (about 6.7%), each further reduction of tion error for Grade 4 documents, comparing Smoothed Unigram to the UNK semantic variable.
    Error bars show 95% confidence interval.
    The grey vertical lines mark logarithmic length. that magnitude would require a corresponding doubling of the training set size.
    This is the trade-off that must be considered between overall RMS accuracy and the cost of gather