ment between a flattened answer parse tree and a question.
    For each test question, we automatically generate a web query and use the top 300 answer sentences returned by our IR engine to look for an answer.
    For each question Q and for each answer sentence Si, we use the algorithm described in Section 3.2 to exhaustively generate all Q- Si,Ai,j pairs.
    Hence we We evaluate the results by generating automatically the mean reciprocal rank (MRR) using the TREC 2002 patterns and QuizZone original answers when testing on TREC 2002 and QuizZone test sets respectively.
    Our baseline is a state of the art QA system, QA-base, which was ranked from second to seventh in the last 3 years at TREC.
    To ensure a fair comparison, we use the same Web-based IR system in all experiments with no answer retrofitting.
    For the same reason, we use the QA-base system with the post-processing module disabled.
    (This module re-ranks the answers produced by QA-base on the basis of their redundancy, frequency on th