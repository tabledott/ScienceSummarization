ks may keep information for their movements/reorderings.
    To test this hypothesis, we calculate the information gain ratio (IGR) for boundary words as well as the whole blocks against the order on the reordering examples extracted by the algorithm described above.
    The IGR is the measure used in the decision tree learning to select features (Quinlan, 1993).
    It represents how precisely the feature predicate the class.
    For feature f and class c, the IGR(f, c) where En(&#183;) is the entropy and En(&#183;|&#183;) is the conditional entropy.
    To our surprise, the IGR for the four boundary words (IGR((b1.s1, b2.s1, b1.t1, b2.t1), order) = 0.2637) is very close to that for the two blocks together (IGR((b1, b2), order) = 0.2655).
    Although our reordering examples do not cover all reordering events in the training data, this result shows that boundary words do provide some clues for predicating reorderings.
  
  
    We carried out experiments to compare against various reordering models and syste