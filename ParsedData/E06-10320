
  Re-Evaluation The Role Of Bleu In Machine Translation Research
  
    We argue that the machine translation community is overly reliant on the Bleu machine translation evaluation metric.
    We show that an improved Bleu score is neither necessary nor sufficient for achieving an actual improvement in translation quality, and give two significant counterexamples to Bleu&#8217;s correlation with human judgments of quality.
    This offers new potential for research which was previously deemed unpromising by an inability to improve upon Bleu scores.
  
  
    Over the past five years progress in machine translation, and to a lesser extent progress in natural language generation tasks such as summarization, has been driven by optimizing against n-grambased evaluation metrics such as Bleu (Papineni et al., 2002).
    The statistical machine translation community relies on the Bleu metric for the purposes of evaluating incremental system changes and optimizing systems through minimum error rate training (Och, 20