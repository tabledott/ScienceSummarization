d that a symbol stream should not be delimited even though subcomponents of the stream have been seen elsewhere.
    In such cases, these larger units may be MWUs.
    The principal work on segmentation has focused either on identifying words in phonetic streams (Saffran, et. al, 1996; Brent, 1996; de Marcken, 1996) or on tokenizing Asian and Indian languages that do not normally include word delimiters in their orthography (Sproat, et al, 1996; Ponte and Croft 1996; Shimohata, 1997; Teahan, et al., 2000; and many others).
    Such efforts have employed various strategies for segmentation, including the use of hidden Markov models, minimum description length, dictionary-based approaches, probabilistic automata, transformation-based learning, and text compression.
    Some of these approaches require significant sources of human knowledge, though others, especially those that follow data compression or HMM schemes, do not.
    These approaches could be applied to languages where word delimiters exist (such as 