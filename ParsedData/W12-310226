ata gathered during the manual evaluation is useful for validating automatic evaluation metrics.
    Table 6 lists the participants in this task, along with their metrics.
    A total of 12 metrics and their variants were submitted to the metrics task by 8 research groups.
    We provided BLEU and TER scores as baselines.
    We asked metrics developers to score the outputs of the machine translation systems and system combinations at the system-level and at the segmentlevel.
    The system-level metrics scores are given in the Appendix in Tables 29&#8211;36.
    The main goal of the metrics shared task is not to score the systems, but instead to validate the use of automatic metrics by measuring how strongly they correlate with human judgments.
    We used the human judgments collected during the manual evaluation for the translation task and the system combination task to calculate how well metrics correlate at system-level and at the segment-level.
    We measured the correlation of the automatic metrics w