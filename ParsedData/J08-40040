
  Survey Article: Inter-Coder Agreement for Computational Linguistics
  
    This article is a survey of methods for measuring agreement among corpus annotators.
    It exposes the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff&#8217;s alpha as well as Scott&#8217;s pi and Cohen&#8217;s kappa; discusses the use of coefficients in several annotation tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappalike measures in computational linguistics, may be more appropriate for many corpus annotation tasks&#8212;but that their use makes the interpretation of the value of the coefficient even harder.
  
  
    Since the mid 1990s, increasing effort has gone into putting semantics and discourse research on the same empirical footing as other areas of computational linguistics (CL).
    This soon led to worries about the subjectivity of the judgments required to create annotated resources, much greater for semantics and pragmatics than for