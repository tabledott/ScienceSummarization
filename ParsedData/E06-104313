 as: The syntactic behaviour of the target verb&#8211;noun pair is defined as the posterior probability distribution over the patterns, given the particular pair.
    The posterior probability of an individual pattern is estimated as: The degree of syntactic fixedness of the target verb&#8211;noun pair is estimated as the divergence of its syntactic behaviour (the posterior distribution 2We collapse some patterns since with a larger pattern set the measure may require larger corpora to perform reliably. over the patterns), from the typical syntactic behaviour (the prior distribution).
    The divergence of the two probability distributions is calculated using a standard information-theoretic measure, the Kullback Leibler (KL-)divergence: KL-divergence is always non-negative and is zero if and only if the two distributions are exactly the same.
    Thus, .
    KL-divergence is argued to be problematic because it is not a symmetric measure.
    Nonetheless, it has proven useful in many NLP applications (Resnik,