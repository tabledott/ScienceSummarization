ts.
			One or more feature func tions may be of the form h(e, f) = h(e), in which case it is referred to as a language model.
			We focus on n-gram language models, which are trained on unlabeled monolingual text.
			As a generalrule, more data tends to yield better language mod els.
			Questions that arise in this context include: (1) How might one build a language model that allows scaling to very large amounts of training data?
			(2) How much does translation performance improve as the size of the language model increases?
			(3) Is there a point of diminishing returns in performance as a function of language model size?
			This paper proposes one possible answer to the first question, explores the second by providinglearning curves in the context of a particular statis tical machine translation system, and hints that thethird may yet be some time in answering.
			In particu lar, it proposes a distributed language model training and deployment infrastructure, which allows direct and efficient integration 