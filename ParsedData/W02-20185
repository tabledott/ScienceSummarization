first derivatives with respect to the parameter &#952; is: Since the likelihood function (2) is concave over the parameter space, it has a global maximum where the gradient is zero.
    Unfortunately, simply setting G(&#952;) = 0 and solving for &#952; does not yield a closed form solution, so we proceed iteratively.
    At each step, we adjust an estimate of the parameters &#952;(k) to a new estimate &#952;(k+1) based on the divergence between the estimated probability distribution q(k) and the empirical distribution p. We continue until successive improvements fail to yield a sufficiently large decrease in the divergence.
    While all parameter estimation algorithms we will consider take the same general form, the method for computing the updates &#948;(k) at each search step differs substantially.
    As we shall see, this difference can have a dramatic impact on the number of updates required to reach convergence.
    One popular method for iteratively refining the model parameters is Generalized Iterati