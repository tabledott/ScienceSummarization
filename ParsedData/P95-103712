 small.
    On the other hand, the decision-tree learning algorithm increases the size of a model only as the training data allows.
    Thus, it can consider very large history spaces, i.e. n-gram models with very large n. Regardless of the value of n, the number of parameters in the resulting model will remain relatively constant, depending mostly on the number of training examples.
    The leaf distributions in decision trees are empirical estimates, i.e. relative-frequency counts from the training data.
    Unfortunately, they assign probability zero to events which can possibly occur.
    Therefore, just as it is necessary to smooth empirical ngram models, it is also necessary to smooth empirical decision-tree models.
    The decision-tree learning algorithms used in this work were developed over the past 15 years by the IBM Speech Recognition group (Bahl et al., 1989).
    The growing algorithm is an adaptation of the CART algorithm in (Breiman et al., 1984).
    For detailed descriptions and discussions