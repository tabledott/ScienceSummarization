 With a little algebraic manipulation, we have a quantity that is at its maximum when ni = ci and at its minimum when ni = 1, that is, when every future observed in training was unique.
    This latter case represents when the model is most &#8220;uncertain,&#8221; in that the transition distribution from Oi(B) is uniform and poorly trained (one observation per possible transition).
    Because these smoothing weights measure, in some sense, the closeness of the observed distribution to uniform, they can be viewed as proxies for the entropy of the distribution p(&#183;  |Oi(B)).
    Back-off levels for PLw/PRw, the modifier headword generation parameter classes. wLiand tLi are, respectively, the headword and its part of speech of the nonterminal Li.
    This table is basically a reproduction of the last column of Table 7.1 in Collins&#8217; thesis.
    Our new parameter class for the generation of headwords of modifying nonterminals. when ft = 0.
    In such situations, making &#955;i = 0 throws all remaining