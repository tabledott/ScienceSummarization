e, and G is a grammar constant.
    The average sentence length in the data set we use (the Penn WSJ treebank) is over 23 words; the grammar constant G can easily take a value of 1000 or greater.
    These factors make exact inference algorithms virtually intractable for training or decoding GLMs for full syntactic parsing.
    As a result, in spite of the potential advantages of these methods, there has been very little previous work on applying GLMs for full parsing without the use of fairly severe restrictions or approximations.
    For example, the model in (Taskar et al., 2004) is trained on only sentences of 15 words or less; reranking models (Collins, 2000; Charniak and Johnson, 2005) restrict Y(x) to be a small set of parses from a first-pass parser; see section 1.1 for discussion of other related work.
    The following ideas are central to our approach:
  
  
    describe a novel, TAG-based parsing formalism that allows full constituent-based trees to be recovered.
    A driving motivation for our a