g to smoothing distribution P(arg|dir, val) in EVG smoothed-skip-head model with lowest free energy.
  
  
    We present a smoothing technique for unsupervised PCFG estimation which allows us to explore more sophisticated dependency grammars.
    Our method combines linear interpolation with a Bayesian prior that ensures the backoff distribution receives probability mass.
    Estimating the smoothed model requires running the standard Variational Bayes on an extended PCFG.
    We used this technique to estimate a series of dependency grammars which extend DMV with additional valence and lexical information.
    We found that both were helpful in learning English dependency grammars.
    Our L-EVG model gives the best reported accuracy to date on the WSJ10 corpus.
    Future work includes using lexical information more deeply in the model by conditioning argument words and valence on the lexical head.
    We suspect that successfully doing so will require using much larger datasets.
    We would also like to 