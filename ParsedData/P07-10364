 One is using the prior knowledge to accurately tailor the generative model so that it captures the domain structure.
    For example, (Grenager et al., 2005) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels.
    This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF.
    However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol.
    A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning.
    This was used, for example, by (Thelen and Riloff, 2002; Collins and Singer, 1999) in information extraction, and by (Smith and Eisner, 2005) in POS tagging.
    (Haghighi and Klein, 2006) extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual wor