g too optimistic in deciding for statistical significant difference between systems.
    We are therefore applying a different method, which has been used at the 2005 DARPA/NIST evaluation.
    We divide up each test set into blocks of 20 sentences (100 blocks for the in-domain test set, 53 blocks for the out-of-domain test set), check for each block, if one system has a higher BLEU score than the other, and then use the sign test.
    The sign test checks, how likely a sample of better and worse BLEU scores would have been generated by two systems of equal performance.
    Let say, if we find one system doing better on 20 of the blocks, and worse on 80 of the blocks, is it significantly worse?
    We check, how likely only up to k = 20 better scores out of n = 100 would have been generated by two equal systems, using the binomial distribution: If p(0..k; n, p) &lt; 0.05, or p(0..k; n, p) &gt; 0.95 then we have a statistically significant difference between the systems.
  
  
    While automatic measures are 