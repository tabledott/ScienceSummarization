xtending it is challenging.
    The main advantage of Dirichlet processes is that they are exchangeable, allowing parameters to be integrated out, but Haghighi and Klein forgo this when they introduce salience.
    Their model thus requires Gibbs sampling over both assignments and parameters, which can be very expensive.
    Haghighi and Klein circumvent this by making approximations that potentially hurt accuracy.
    At the same time, the Dirichlet process prior favors skewed cluster sizes and a number of clusters that grows logarithmically with the number of data points, neither of which seems generally appropriate for coreference resolution.
    Further, deterministic or strong non-deterministic dependencies cause Gibbs sampling to break down (Poon &amp; Domingos, 2006), making it difficult to leverage many linguistic regularities.
    For example, apposition (as in &#8220;Bill Gates, the chairman of Microsoft&#8221;) suggests coreference, and thus the two mentions it relates should always be placed in th