resented in Table 2 resemble the topics one would obtain through models likeLDA (Blei et al, 2003), they are not identical.
			Be cause of the lengthy process of Gibbs sampling, we initially thought that using LDA assignments as aninitial state would converge faster than a random initial assignment.
			While this was the case, it con verged to a state that less probable than the randomlyinitialized state and no better at sense disambigua tion (and sometimes worse).
			The topics presented in 2 represent words both that co-occur together in a corpus and co-occur on paths through WORDNET.
			Because topics created through LDA only have the first property, they usually do worse in terms of both total probability and disambiguation accuracy (see Figure 3).
			Another interesting property of topics in LDAWN is that, with higher levels of smoothing, words that don?t appear in a corpus (or appear rarely) but are in similar parts of WORDNET might have relatively high probability in a topic.
			For example, ?maturity?