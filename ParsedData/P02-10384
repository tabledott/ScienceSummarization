 &#8216;inverted&#8217; translation model in the unconventional decision rule of Eq.
    6 results if we use the feature function log Pr(eI1|fJ1 ) instead of log Pr(fJ1 |eI1).
    In this framework, this feature can be as good as log Pr(fJ1 |eI1).
    It has to be empirically verified, which of the two features yields better results.
    We even can use both features log Pr(eI1|fJ1 ) and log Pr(fJ1 |eI1), obtaining a more symmetric translation model.
    As training criterion, we use the maximum class posterior probability criterion: This corresponds to maximizing the equivocation or maximizing the likelihood of the direct translation model.
    This direct optimization of the posterior probability in Bayes decision rule is referred to as discriminative training (Ney, 1995) because we directly take into account the overlap in the probability distributions.
    The optimization problem has one global optimum and the optimization criterion is convex.
    Typically, the probability Pr(fJ1 |eI1) is decomposed via