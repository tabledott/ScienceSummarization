 average in order to decode a message.
    Cross entropy is a useful yardstick for measuring the ability of a language model to predict a source of data.
    If the language model is very good at predicting the future output of the source, then the cross entropy will be small.
    No matter how good the language model is, though, the cross entropy cannot be reduced below a lower bound, known as the entropy of the source, the cross entropy of the source with itself.
    One can also think of the cross entropy between a language model and a probabilistic source as the number of bits that will be needed on average to encode a symbol from the source when it is assumed, albeit mistakenly, that the language model is a perfect probabilistic characterization of the source.
    Thus, there is a close connection between a language model and a coding scheme.
    Table 5 below lists a number of coding schemes along with estimates of their cross entropies with English text.
    The standard ASCII code requires 8 bits per 