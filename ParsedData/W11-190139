e evaluation criteria used.
    Unlike for propositions, word sense and named entities, where it is simply a matter of counting the correct answers, or for parsing, where there are several established metrics, evaluating the accuracy of coreference continues to be contentious.
    Various al16OntoNotes is deeply grateful to the Linguistic Data Consortium for making the source data freely available to the task participants. ternative metrics have been proposed, as mentioned below, which weight different features of a proposed coreference pattern differently.
    The choice is not clear in part because the value of a particular set of coreference predictions is integrally tied to the consuming application.
    A further issue in defining a coreference metric concerns the granularity of the mentions, and how closely the predicted mentions are required to match those in the gold standard for a coreference prediction to be counted as correct.
    Our evaluation criterion was in part driven by the OntoNotes data st