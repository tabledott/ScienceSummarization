ster models as before.
    Each model was used to decide which of two verbs v and v' are more likely to appear with a noun n where the (v, n) data was deleted from the training set, and the decisions were compared with the corresponding ones derived from the original event frequencies in the initial data set.
    The error rate for each model is simply the proportion of disagreements for the selected (v, n, v') triples.
    Figure 4 shows the error rates for each model for all the selected (v, n, v') (all) and for just those exceptional triples in which the conditional ratio p(n, v) p(n, v') is on the opposite side of 1 from the marginal ratio p(v)I p(v').
    In other words, the exceptional cases are those in which predictions based just on the marginal frequencies, which the initial one-cluster model represents, would be consistently wrong.
    Here too we see some overtraining for the largest models considered, although not for the exceptional verbs.
  
  
    We have demonstrated that a general divisive c