s, and that is why TAGs are used here.
    Figure 2 shows the combined model&#8217;s dependency structure for the repair of Figure 1.
    Interestingly, if we trace the temporal word string through this dependency structure, aligning words next to the words they are dependent on, we obtain a &#8220;helical&#8221; type of structure familiar from genome models, and in fact TAGs are being used to model genomes for very similar reasons.
    The noisy channel model described here involves two components.
    A language model defines a probability distribution P(X) over the source sentences X, which do not contain repairs.
    The channel model defines a conditional probability distribution P(YIX) of surface sentences Y , which may contain repairs, given source sentences.
    In the work reported here, X is a word string and Y is a speech transcription not containing punctuation or partial words.
    We use two language models here: a bigram language model, which is used in the search process, and a syntactic parse