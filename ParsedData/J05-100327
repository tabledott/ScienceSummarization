gesting that this is not in fact a problem for the approach.
    Given the previous strong results for the boosting approach, and for reasons of computational efficiency, we pursue the boosting approach to feature selection in this article.
    Minimization of LogLoss is most often justified as a parametric, maximum-likelihood (ML) approach to estimation.
    Thus this approach benefits from the usual guarantees for ML estimation: If the distribution generating examples is within the class of distributions specified by the log-linear form, then in the limit as the sample size goes to infinity, the model will be optimal in the sense of convergence to the true underlying distribution generating examples.
    As far as we are aware, behavior of the models for finite sample sizes is less well understood.
    In particular, while feature selection methods have often been proposed for maximum-entropy models, little theoretical justification (in terms of guarantees about generalization) has been given for them.
    