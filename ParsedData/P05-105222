nificant. different kernel setups.
    Types are ordered in decreasing order of frequency of occurrence in the ACE corpus.
    In SVM training, the same parameters were used for all 7 types.
    Table 3 shows the performance of SVM and KNN (k Nearest Neighbors) on different kernel setups.
    For KNN, k was set to 3.
    In the first setup of KNN, the two kernels which seem to contain most of the important information are used.
    It performs quite well when compared with the SVM result.
    The other two tests are based on the full kernel setup.
    For the two KNN experiments, adding more kernels (features) does not help.
    The reason might be that all kernels (features) were weighted equally in the composite kernel &#934;2 and this may not be optimal for KNN.
    Another reason is that the polynomial extension of kernels does not have any benefit in KNN because it is a monotonic transformation of similarity values.
    So the results of KNN on kernel (&#936;1+&#936;3) and &#934;1 would be exactly the sa