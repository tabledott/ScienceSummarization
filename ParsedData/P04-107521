the beginning iterations (F-measure &lt; 60), the three methods performed similarly.
    But with the larger training set, the efficiencies of Stratety1 and Strategy2 begin to be evident.
    Table 4 highlights the final result of the three methods.
    In order to reach the performance of supervised learning, Strategy1 (40K words) and Strategyy2 (31K words) require about 80% and 60% of the data that Info_Min (51.9K) does.
    So we believe the effective combinations of informativeness, representativeness and diversity will help to learn the NER model more quickly and cost less in annotation.
  
  
    Since there is no study on active learning for NER task previously, we only introduce general active learning methods here.
    Many existing active learning methods are to select the most uncertain examples using various measures (Thompson et al. 1999; Schohn and Cohn 2000; Tong and Koller 2000; Engelson and Dagan 1999; Ngai and Yarowsky 2000).
    Our informativeness-based measure is similar to these works.
 