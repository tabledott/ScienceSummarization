ton generalised distribution for the transition weights, and using absolute discounting with backing off for the membership/output function.
    (Ney et al., 1994; Martin et al., 1998) We trained the model on sections 00-09 of the Penn Treebank, ( 518769 tokens including sentence boundaries and punctuation) and tested it on sections 10&#8212; l 9 (537639 tokens).
    We used the full vocabulary of the training and test sets together which was 45679, of which 14576 had frequency zero in the training data and thus had to be categorised based solely on their morphology and frequency.
    We did not reduce the vocabulary or change the capitalization in any way.
    We compared different models with varying numbers of clusters: 32 64 and 128.
    Table 4 shows the results of the perplexity evaluation on the WSJ data.
    As can be seen the models incorporating morphological information have slightly lower perplexity on the test data than the D5 model.
    Note that this is a global evaluation over all the words in