t.
    If none of the five edits was deemed to be acceptable, then we used the edit distance between the MT and the reference.
    Setup We evaluated five machine translation systems using HTER.
    These systems were selected from WMT09 (Callison-Burch et al., 2009).
    We wanted a spread in quality, so we took the top two and bottom two systems from the GermanEnglish task, and the top system from the FrenchEnglish task (which significantly outperformed everything else).
    Based on the results of the WMT09 evaluation we would expect the see the following ranking from the least edits to the most edits: google.fr-en, google.de-en, rbmt5.de-en, geneva.de-en and tromble.de-en.
    Results Table 2 gives the HTER scores for the five systems.
    Their ranking is as predicted, indicating that the editing is working as expected.
    The table reports averaged scores when the five annotators are subsampled.
    This gives a sense of how much each additional editor is able to minimize the score for each system.
   