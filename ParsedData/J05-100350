that C/T will be small for most iterations.
    In section 5.4.3 we show that this is the case for our experiments.
    4.6 Feature Selection for LogLoss We now describe an approach that was implemented for LogLoss.
    At the first iteration, a0 is set to one.
    Feature selection then searches for values of the remaining parameters, a1, ... , am.
    We now describe how to calculate the optimal update for a feature k with the LogLoss function.
    First we recap the definition of the probability of a particular parse xi,q given parameter settings &#175;a: Unfortunately, unlike the case of ExpLoss, in general an analytic solution for BestWt does not exist.
    However, we can define an iterative solution using techniques from iterative scaling (Della Pietra, Della Pietra, and Lafferty 1997).
    We first define &#732; number of times that feature k is seen in the best parse, and &#732;pk&#240;&#175;a&#222;, the expected number of times under the model that feature k is seen: Given this method for calculatin