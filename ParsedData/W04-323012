e difference between the inner product (or score) of the correct path A &#183; F(yj, xj) and those of all other candidates A &#183; F(y, xj), y &#8712; Y(xj).
    CRFs is thus trained to discriminate the correct path from all other candidates, which reduces the influences of the label and length bias in encoding.
    At the optimal point, the first-derivative of the log-likelihood becomes 0, thus, where Ok = Ej Fk(yj, xj) is the count of feature k observed in the training data T, and Ek = Ej EP(y|xj)[Fk(y, xj)] is the expectation of feature k over the model distribution P(y|x) and T. The expectation can efficiently be calculated using a variant of the forward-backward algorithm. where fk is an abbreviation for fk(hw', t'i, hw, ti), B(x) is a set of all bi-gram sequences observed in the lattice for x, and &#945;(w,t) and &#946;(w,t) are the forward-backward costs given by the following recursive definitions: which can be found with the Viterbi algorithm.
    An interesting note is that the decoding process of 