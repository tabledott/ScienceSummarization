onal N. The algorithm also learns language-specific constructions such as the Japanese case markers &#8220;no&#8221; and &#8220;wa&#8221;, which are treated as modifiers that do not add semantic content.
    Language-specific word order is also encoded, using the slash directions of the CCG categories.
    For example, &#8220;what&#8221; and &#8220;que&#8221; take their arguments to the right in the wh-initial English and Spanish.
    However, the Turkish wh-word &#8220;nelerdir&#8221; and the Japanese question marker &#8220;nan desu ka&#8221; are sentence final, and therefore take their arguments to the left.
    Learning regularities of this type allows UBL to generalize well to unseen data.
    There is less variation and complexity in the learned lexical items for the variable-free representation.
    The fact that the meaning representation is deeply nested influences the form of the induced grammar.
    For example, recall that the sentence &#8220;what states border texas&#8221; would be paired with the