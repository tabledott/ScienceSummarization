side-outside algorithm on bilingual data, assuming every phrase pair that can appear as a leaf in a parse tree of the grammar a valid candidate.
    However, it is easy to show that the maximum likelihood training will lead to the saturated solution where PC = 1 &#8212; each sentence pair is generated by a single phrase spanning the whole sentence.
    From the computational point of view, the full EM algorithm runs in O(n6) where n is the average length of the two input sentences, which is too slow in practice.
    The key is to control the number of parameters, and therefore the size of the set of candidate phrases.
    We deal with this problem in two directions.
    First we change the objective function by incorporating a prior over the phrasal parameters.
    This has the effect of preferring parameter vectors in &#952;C with fewer non-zero values.
    Our second approach was to constrain the search space using simpler alignment models, which has the further benefit of significantly speeding up training