 its components, which have similar individual performance.
    Finally, the best performing model on the BNC also combines two layers capturing wider (10w) and more local (5w) contextual information (see Table 3, right side).
    Comparison to State-of-the-Art Table 4 compares our model against the two best performing sense induction systems that participated in the Semeval-2007 competition.
    IR2 (Niu et al., 2007) performed sense induction using the Information Bottleneck algorithm, whereas UMND2 (Pedersen, 2007) used k-means to cluster second order co-occurrence vectors associated with the target word.
    These models and our own model significantly outperform the most-frequent-sense baseline (p &lt; 0.01 using a x2 test).
    Our best system (10w+5w on WSJ) is significantly better than UMND2 (p &lt; 0.01) and quantitatively better than IR2, although the difference is not statistically significant.
  
  
    This paper presents a novel Bayesian approach to sense induction.
    We formulated sense induc