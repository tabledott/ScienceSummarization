h the indomain set, followed by examining the subcorpora to see whether they are actually in-domain, out-ofdomain, or something in between.
    Based on this, we compare translation model combination methods.
    Finally, we show that these tiny translation models for model combination can improve system performance even further over the current standard way of producing a domain-adapted MT system.
    The resulting process is lightweight, simple, and effective.
  
  
    An underlying assumption in domain adaptation is that a general-domain corpus, if sufficiently broad, likely includes some sentences that could fall within the target domain and thus should be used for training.
    Equally, the general-domain corpus likely includes sentences that are so unlike the domain of the task that using them to train the model is probably more harmful than beneficial.
    One mechanism for domain adaptation is thus to select only a portion of the general-domain corpus, and use only that subset to train a complete sys