probability of each of these translations to their empirical probabilities.
    The next few constraints selected by the algorithm are shown in Table 5.
    The first column gives the identity of the feature whose expected value is constrained; the second column gives AL(S, f ), the approximate increase in the model's log-likelihood on the data as a result of imposing this constraint; the third column gives L(p), the log-likelihood after adjoining the feature and recomputing the model.
    Let us consider the fifth row in the table.
    This constraint requires that the model's expected probability of dans, if one of the three words to the right of in is the word speech, is equal to that in the empirical sample.
    Before imposing this constraint on the model during the iterative model-growing process, the log-likelihood of the current model on the empirical sample was &#8212;2.8703 bits.
    The feature selection algorithm described in Section 4 calculated that if this constraint were imposed on the model, 