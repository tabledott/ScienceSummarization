K, the output from the algorithm is chosen to be the y(k) for k &lt; K that maximizes the objective function in Eq.
    11.
    The graphs show that values of K less than 50 produce almost identical performance to K = 50, but with fewer cases giving certificates of optimality (with K = 10, the f-score of the method is 90.69%; with K = 5 it is 90.63%).
    In a second experiment, we used dual decomposition to integrate the Model 1 parser with the Stanford max-ent trigram POS tagger (Toutanova and Manning, 2000), using a very similar algorithm to that described in section 4.1.
    We use the same training/dev/test split as in section 7.1.
    The two models were again trained separately.
    We ran the algorithm with a limit of K = 50 iterations.
    Out of 2416 test examples, the algorithm found an exact solution in 98.9% of the cases.
    Table 1 gives statistics showing the speed of convergence for different examples: over 94% of the examples converge to an exact solution in 10 iterations or fewer.
    In te