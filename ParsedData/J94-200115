e kind of text used in this experiment).
    These results call for some comments.
    ML training is a theoretically sound procedure, and one that is routinely and successfully used in speech recognition to estimate the parameters of hidden Markov models that describe the relations between sequences of phonemes and the speech signal.
    Although ML training is guaranteed to improve perplexity, perplexity is not necessarily related to tagging accuracy, and it is possible to improve one while degrading the other.
    Also, in the case of tagging, ML training from various initial points (top line corresponds to N=100, bottom line to N=a11). the relations between words and tags are much more precise than the relations between phonemes and speech signals (where the correct correspondence is harder to define precisely).
    Some characteristics of ML training, such as the effect of smoothing probabilities, are probably more suited to speech than to tagging.
    For this experiment we considered the initial model 