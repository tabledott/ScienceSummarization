ges quickly and all of [&#65533;&#163;]n1 can be loaded into memory at once.
    We have reviewed three tuning methods and introduced three tuning methods.
    All six methods employ sentence-level loss functions, which in turn employ sentence-level BLEU approximations.
    Except for online MIRA, all methods plug nicely into the existing MERT architecture.
    These methods can be split into two groups: MIRA variants (online, batch k-best, batch lattice), and direct optimizers (PRO, MR and SVM).
    The MIRA variants use pseudocorpus BLEU in place of smoothed BLEU, and provide access to richer hypothesis spaces through the use of online training or lattices.5 The direct optimizers have access to a tunable regularization parameter &#955;, and do not require special purpose code for hope and fear lattice decoding.
    Batch k-best MIRA straddles the two groups, benefiting from pseudo-corpus BLEU and easy implementation, while being restricted to a k-best list.
  
  
    We evaluated the six tuning strategies d