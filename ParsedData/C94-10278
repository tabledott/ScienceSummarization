ownall numher of times the word occurs.
  This method is known as the Ma.vimum Likelihood Principle.
  IZ ~ For tile preceding words, there is more information available, because they have already bccn tagged.
  The activation values of the output units at the time of processing are here used instead of the lexieal part-of- speech probabilitiesa: i , , ; /t)  = o, , t / t  + O, if ; &lt; 0 (6) Copying output activations of tile network into the input units introduces recurrence into the network.
  This complicates the training process, because the out- put of the network is not correct, when the training starts and therefore, it cannot be fed back directly, when the training starts.
  Instead a weighted average of the actual output and the target output is used.
  It resembles more the output of the trained network which is similar (or at least shouhl be similar) to the target output.
  At tile beginning of the training, the weighting of the target output is high.
  It fails to zero during the training.
  The