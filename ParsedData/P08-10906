rotagonist overlap to motivate narrative chain learning.
    Work on semantic similarity learning such as Chklovski and Pantel (2004) also automatically learns relations between verbs.
    We use similar distributional scoring metrics, but differ with our use of a protagonist as the indicator of relatedness.
    We also use typed dependencies and the entire space of events for similarity judgements, rather than only pairwise lexical decisions.
    Finally, Fujiki et al. (2003) investigated script acquisition by extracting the 41 most frequent pairs of events from the first paragraph of newswire articles, using the assumption that the paragraph&#8217;s textual order follows temporal order.
    Our model, by contrast, learns entire event chains, uses more sophisticated probabilistic measures, and uses temporal ordering models instead of relying on document order.
  
  
    Our model is inspired by Centering (Grosz et al., 1995) and other entity-based models of coherence (Barzilay and Lapata, 2005) in which an e