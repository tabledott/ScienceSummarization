xas.edu/users/ml/ml-progs.html.
    All systems were run on a Sun SPARCstation 5 with 40MB of main memory.
    The simplest algorithms tested were a naive Bayesian classifier which assumes conditional independence of features and a k nearest-neighbor classifier, which assigns a test example to the majority class of the 3 closest training examples (using Hamming distance to measure closeness) (Duda &amp; Hart, 1973; Kulikowski &amp; Weiss, 1991).
    Initial results indicated that k nearest neighbor with k=3 resulted in slightly better performance than k=1.
    Naive Bayes is intended as a simple representative of statistical methods and nearest neighbor as a simple representative of instancebased (case-based, exemplar) methods (Cover &amp; Hart, 1967; Aha, Kibler, &amp; Albert, 1991).
    Since the previous results of Leacock et al. (1993b) indicated that neural networks did not benefit from hidden units on the &amp;quot;line&amp;quot; disambiguation data, we employed a simple perceptron (Rosenblatt, 1962) as