gging: average pseconds per token tokenizer lexicon Viterbi total 604 388 233 1235 It can be seen from these figures that training on a new corpus may be accomplished in a matter of minutes, and that tens of megabytes of text may then be tagged per hour.
    When using a lexicon and tagset built from the tagged text of the Brown corpus [Francis and KuEera, 1982], training on one half of the corpus (about 500,000 words) and tagging the other, 96% of word instances were assigned the correct tag.
    Eight iterations of training were used.
    This level of accuracy is comparable to the best achieved by other taggers [Church, 1988, Merialdo, 1991].
    The Brown Corpus contains fragments and ungrammaticalities, thus providing a good demonstration of robustness.
    A tagger should be tunable, so that systematic tagging errors and anomalies can be addressed.
    Similarly, it is important that it be fast and easy to target the tagger to new genres and languages, and to experiment with different tagsets reflecting