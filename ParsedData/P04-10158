, the number of times that zi =6 yi for some (t, i) pair.
    We can then state the following theorem (see (Collins, 2002) for a proof): This theorem implies that if there is a parameter vector U which makes zero errors on the training set, then after a finite number of iterations the training algorithm will converge to parameter values with zero training error.
    A crucial point is that the number of mistakes is independent of the number of candidates for each example Inputs: Training examples (xi, yi) Algorithm: (i.e. the size of GEN(xi) for each i), depending only on the separation of the training data, where separation is defined above.
    This is important because in many NLP problems GEN(x) can be exponential in the size of the inputs.
    All of the convergence and generalization results in Collins (2002) depend on notions of separability rather than the size of GEN. Two questions come to mind.
    First, are there guarantees for the algorithm if the training data is not separable?
    Second, perfo