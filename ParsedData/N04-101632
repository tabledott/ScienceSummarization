rve that having access to linguistic information makes up for the lack of vast amounts of data.
    Our results therefore indicate that large data sets such as those obtained from the web are not the panacea that they are claimed to be (at least implicitly) by authors such as Grefenstette (1998) and Keller and Lapata (2003).
    Rather, in our opinion, web-based models should be used as a new baseline for NLP tasks.
    The web baseline indicates how much can be achieved with a simple, unsupervised model based on n-grams with access to a huge data set.
    This baseline is more realistic than baselines obtained from standard corpora; it is generally harder to beat, as our comparisons with the BNC baseline throughout this paper have shown.
    Note that for certain tasks, the performance of a web baseline model might actually be sufficient, so that the effort of constructing a sophisticated supervised model and annotating the necessary training data can be avoided.
    Another possibility that needs further in