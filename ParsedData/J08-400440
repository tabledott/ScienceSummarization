rved agreement, a higher difference in coder marginals implies a lower expected agreement and therefore a higher &#954; value, the conclusion that &#954; penalizes coders for having similar distributions is unwarranted.
    This is because Ao and Ae are not independent: Both are drawn from the same set of observations.
    What &#954; does is discount some of the disagreement resulting from different coder marginals by incorporating it into Ae.
    Whether this is desirable depends on the application for which the coefficient is used.
    The most common application of agreement measures in CL is to infer the reliability of a large-scale annotation, where typically each piece of data will be marked by just one coder, by measuring agreement on a small subset of the data which is annotated by multiple coders.
    In order to make this generalization, the measure must reflect the reliability of the annotation procedure, which is independent of the actual annotators used.
    Reliability, or reproducibility of th