ords per seen headword.
    The difference between similarity metrics, however, is striking.
    Differences between similarity metrics.
    As Table 2 shows, Lin and Jaccard worked best (though Lin has very low coverage), Dice and Hindle not as good, and Cosine showed the worst performance.
    To determine possible reasons for the difference, Table 5 explores properties of the five similarity measures.
    Given a set S = Seen(rp) of seen headwords for some role rp, each similarity metric produces a set like(S) of words that have nonzero similarity to S, that is, to at least one word in S. Line (a) shows the average frequency of words in like(S).
    The results confirm that the Lin and Cosine metrics tend to propose less frequent words as similar.
    Line (b) pursues the question of the frequency bias further, showing the percentage of headword/confounder pairs for which the more frequent of the two words &#8220;won&amp;quot; in the pseudodisambiguation task (using uniform weights).
    This it is an indi