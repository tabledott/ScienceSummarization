e models, such as structure models (Chelba, 1997; Chelba and Jelinek, 1998), or longer ngram models would lead to the system generating headlines that were more similar in phrasing to real headlines because longer range dependencies summary sentences, respectively, of the article.
    Using Part of Speech (POS) and information about a token&#8217;s location in the source document, in addition to the lexical information, helps improve performance on the Reuters&#8217; test set. could be taken into account.
    Table 1 shows the results of these term selection schemes.
    As can be seen, even with such an impoverished language model, the system does quite well: when the generated headlines are four words long almost one in every five has all of its words matched in the article s actual headline.
    This percentage drops, as is to be expected, as headlines get longer.
    Multiple Selection Models: POS and Position As we mentioned earlier, the zero-level model that we have discussed so far can be extended to t