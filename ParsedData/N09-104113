968;G to be drawn from BETA(5,2), preferring general content words, and every later sentence from BETA(1,2).20 emitting a topic specific content word, we must decide which of 0Cl, ... , 0CK to use.
    In order to ensure tight lexical cohesion amongst the specific topics, we assume that each sentence draws a single specific topic ZS used for every specific content word in that sentence.
    Reflecting intuition that adjacent sentences are likely to share specific content vocabulary, we utilize a &#8216;sticky&#8217; HMM as in Barzilay and Lee (2004) over the each sentences&#8217; ZS.
    Concretely, ZS for the first sentence in a document is drawn uniformly from 1, ... , K, and each subsequent sentence&#8217;s ZS will be identical to the previous sentence with probability Q, and with probability 1 &#8722; Q we select a successor topic from a learned transition distribution amongst 1, ... , K.21 Our intent is that the general content distribution 0C0 now prefers words which not only appear in many documents, b