
  Mixture-Model Adaptation for SMT
  
    We describe a mixture-model approach to adapting a Statistical Machine Translation System for new domains, using weights that depend on text distances to mixture components.
    We investigate a number of variants on this approach, including cross-domain versus dynamic adaptation; linear versus loglinear mixtures; language and translation model adaptation; different methods of assigning weights; and granularity of the source unit being adapted to.
    The best methods achieve gains of approximately one BLEU percentage point over a state-of-the art non-adapted baseline system.
  
  
    Language varies significantly across different genres, topics, styles, etc.
    This affects empirical models: a model trained on a corpus of car-repair manuals, for instance, will not be well suited to an application in the field of tourism.
    Ideally, models should be trained on text that is representative of the area in which they will be used, but such text is not always availabl