ta that is less well-matched to the translation application.
    In this paper, however, we show that for a data source that is not entirely in-domain, we can improve the match between the language model from that data source and the desired application output by intelligently selecting a subset of the available data as language model training data.
    This not only produces a language model better matched to the domain of interest (as measured in terms of perplexity on held-out in-domain data), but it reduces the computational resources needed to exploit a large amount of non-domain-specific data, since the resources needed to filter a large amount of data are much less (especially in terms of memory) than those required to build a language model from all the data.
  
  
    Our approach to the problem assumes that we have enough in-domain data to train a reasonable indomain language model, which we then use to help score text segments from other data sources, and we select segments based on a score cutoff 