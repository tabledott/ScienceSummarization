d to tagging all words).
    The construction of ensembles was based on bagging, selection of different subsets of features (e.g., context and lexical features) in decision tree construction, and selection of different splitting criteria in decision tree construction.
    In all experiments, simple voting was used to combine component tagger decisions.
    All combination approaches resulted in a better accuracy (an error reduction between 8% and 12% on average compared to the basic decision tree trained on the same data).
    But as these error reductions refer to only part of the tagging task (18 ambiguity classes), they are hard to compare with our own results.
    In Abney, Schapire, and Singer (1999), ADABOOST variants are used for tagging WSJ material.
    Component classifiers here are based on different information sources (subsets of features), e.g., capitalization of current word, and the triple &amp;quot;string, capitalization, and tag&amp;quot; of the word to the left of the current word are the b