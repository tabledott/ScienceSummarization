nd AS = 0.01.
    These parameters were minimally tuned (without reference to ROUGE results) in order to ensure that all topic distribution behaved as intended.
  
  
    We present formal experiments on the DUC 2007 data main summarization task, proposing a general summary of at most 250 words22 which will be evaluated automatically and manually in order to simulate as much as possible the DUC evaluation environment.23 DUC 2007 consists of 45 document sets, each consisting of 25 documents and 4 human reference summaries.
    We primarily evaluate the HIERSUM model, extracting a single summary from the general content distribution using the KLSUM criterion (see section 3.2).
    Although the differences in ROUGE between HIERSUM and TOPICSUM were minimal, we found HIERSUM summary quality to be stronger.
    In order to provide a reference for ROUGE and manual evaluation results, we compare against PYTHY, a state-of-the-art supervised sentence extraction summarization system.
    PYTHY uses humangenerated summa