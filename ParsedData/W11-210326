intra-annotator agreement across the various evaluation tasks.
    These give an indication of how often different judges agree, and how often single judges are consistent for repeated judgments, respectively.
    There are some general and expected trends that can be seen in this table.
    First of all, intra-annotator agreement is higher than inter-annotator agreement.
    Second, reference translations are noticeably better than other system outputs, which means that annotators have an artificially high level of agreement on pairwise comparisons that include a reference translation.
    For this reason, we also report the agreement levels when such comparisons are excluded.
    The exact interpretation of the kappa coefficient is difficult, but according to Landis and Koch (1977), 0 &#8722; 0.2 is slight, 0.2 &#8722; 0.4 is fair, 0.4 &#8722; 0.6 is moderate, 0.6 &#8722; 0.8 is substantial, and 0.8 &#8722; 1.0 is almost perfect.
    Based on these interpretations, the agreement for sentence-level ranking i