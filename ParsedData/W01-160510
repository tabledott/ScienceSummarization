nt, based on an initial set of tagging guidelines, and then meeting as a group to compare results.
    The initial focus was on resolving segmentation differences, but over time this shifted to addressing issues of relations and nuclearity.
    These exploratory sessions led to enhancements in the tagging guidelines.
    To reinforce new rules, annotators re-tagged the document.
    During this process, we regularly tracked interannotator agreement (see Section 4.2).
    In the final phase, the annotation team concentrated on ways to reduce differences by adopting some heuristics for handling higher levels of the discourse structure.
    Wiebe et al. (1999) present a method for automatically formulating a single best tag when multiple judges disagree on selecting between binary features.
    Because our annotators had to select among multiple choices at each stage of the discourse annotation process, and because decisions made at one stage influenced the decisions made during subsequent stages, we could not a