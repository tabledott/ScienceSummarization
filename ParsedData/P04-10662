arting point in which none of the initial parameter values is zero.
    If one equates optimum parameter estimation with finding the global maximum for the likelihood of the training data, then this result would seem to show no improvement is possible.
    However, in virtually every application of statistical techniques in natural-language processing, maximizing the likelihood of the training data causes overfitting, resulting in lower task performance than some other estimates for the model parameters.
    This is implicitly recognized in the widespread adoption of early stopping in estimating the parameters of Model 1.
    Brown et al. (1993a) stopped after only one iteration of EM in using Model 1 to initialize their Model 2, and Och and Ney (2003) stop after five iterations in using Model 1 to initialize the HMM word-alignment model.
    Both of these are far short of convergence to the maximum likelihood estimates for the model parameters.
    We have identified at least two ways in which the standard E