
  Distortion Models For Statistical Machine Translation
  
    In this paper, we argue that n-gram language models are not sufficient to address word reordering required for Machine Translation.
    We propose a new distortion model that can be used with existing phrase-based SMT decoders to address those n-gram language model limitations.
    We present empirical results in Arabic to English Machine Translation that show statistically significant improvements when our proposed model is used.
    We also propose a novel metric to measure word order similarity (or difference) between any pair of languages based on word alignments.
  
  
    A language model is a statistical model that gives a probability distribution over possible sequences of words.
    It computes the probability of producing a given word w1 given all the words that precede it in the sentence.
    An n-gram language model is an n-th order Markov model where the probability of generating a given word depends only on the last n &#8722; 1 word