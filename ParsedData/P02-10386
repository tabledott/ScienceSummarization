all possible target language sentences eI1 and all possible alignments aJ1 .
    Generalizing this approach to direct translation models, we extend the feature functions to include the dependence on the additional hidden variable.
    Using M feature functions of the form hm(eI1, fJ1 , aJ1), m = 1, ... , M, we obtain the following model: Obviously, we can perform the same step for translation models with an even richer structure of hidden variables than only the alignment aJ1 .
    To simplify the notation, we shall omit in the following the dependence on the hidden variables of the model.
    Hence, we obtain three different probability distributions: Pr(aK1 |eI1), Pr(zK1 |aK1 , eI1) and Pr(fJ1 |zK1 ,aK1 ,eI1).
    Here, we omit a detailed description of modeling, training and search, as this is not relevant for the subsequent exposition.
    For further details, see (Och et al., 1999).
    To use these three component models in a direct maximum entropy approach, we define three different feature functions f