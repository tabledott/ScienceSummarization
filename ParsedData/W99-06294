in memory.
    In recent work Daelemans et at.
    (1999b) have shown that for typical natural language processing tasks, this approach is at an advantage because it also &amp;quot;remembers&amp;quot; exceptional, low-frequency cases which are useful to extrapolate from.
    Moreover, automatic feature weighting in the similarity metric of an MB learner makes the approach well-suited for domains with large numbers of features from heterogeneous sources, as it embodies a smoothing-by-similarity method when data is sparse (Zavrel and Daelemans, 1997).
    We have used the following MBL algorithms': test item and each memory item is defined as the number of features for which they have a different value (overlap metric).
    IB1-IG : IB1 with information gain (an information-theoretic notion measuring the reduction of uncertainty about the class to be predicted when knowing the value of a feature) to weight the cost of a feature value mismatch during comparison.
    IGTree : In this variant, a decision tree is c