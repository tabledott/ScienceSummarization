the nouns being compared.
    However, we can make a further observation: with the exception of the confusion probability, all the functions we compared are symmetric, that is, f (q, r) = f (r, q).
    But the substitutability of one word for another need not symmetric.
    For instance, &amp;quot;fruit&amp;quot; may be the best possible approximation to &amp;quot;apple&amp;quot;, but the distribution of &amp;quot;apple&amp;quot; may not be a suitable proxy for the distribution of &amp;quot;fruit&amp;quot; .4 In accordance with this insight, we developed a novel asymmetric generalization of the KL divergence, the a-skew divergence: scr(q,r) = D (r Ia.q+(1 &#8212;a) r) for 0 &lt; a &lt; 1.
    It can easily be shown that sc, depends only on the verbs in Vqr.
    Note that at a = 1, the skew divergence is exactly the KL divergence, and s1/2 is twice one of the summands of JS (note that it is still asymmetric).
    40n a related note, an anonymous reviewer cited the 30 following example from the psychology liter