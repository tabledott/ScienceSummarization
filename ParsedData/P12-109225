se or mistakes in context clustering.By utilizing global context, our model outper forms C&amp;W?s vectors and the above baselines on this dataset.
			With multiple representations per word, we show that the multi-prototype approachcan improve over the single-prototype version with out using context (62.8 vs. 58.6).
			Moreover, using AvgSimC4 which takes contexts into account, the multi-prototype model obtains the best performance (65.7).
	
	
			Neural language models (Bengio et al, 2003; Mnih and Hinton, 2007; Collobert and Weston, 2008; Schwenk and Gauvain, 2002; Emami et al, 2003) have been shown to be very powerful at languagemodeling, a task where models are asked to ac curately predict the next word given previously seen words.
			By using distributed representations of 3We first tried movMF as in Reisinger and Mooney (2010b), but were unable to get decent results (only 31.5).
			4probability of being in a cluster is calculated as the inverse of the distance to the cluster centroid.
			879 words which 