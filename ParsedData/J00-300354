r ASR.
    The mixture-of-posteriors approach yields whereas the mixture-of-LMs approach gives We see that the second equation reduces to the first under the crude approximation P(A111.11) P(A,).
    In practice, the denominators are computed by summing the numerators over a finite number of word hypotheses W,, so this difference translates into normalizing either after or before summing over DAs.
    When the normalization takes place as the final step it can be omitted for score maximization purposes; this shows why the mixture-of-LMs approach is less computationally expensive.
    We tested both the mixture-of-posteriors and the mixture-of-LMs approaches on our Switchboard test set of 19 conversations.
    Instead of decoding the data from scratch using the modified models, we manipulated n-best lists consisting of up to 2,500 best hypotheses for each utterance.
    This approach is also convenient since both approaches require access to the full word string for hypothesis scoring; the overall model is no 