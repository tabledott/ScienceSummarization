 from the enhanced oracle translations.
    Finally, we compared our parallelization method against a simpler method in which all processors learn independently and their weight vectors are all averaged together (line 5).
    We see that sharing information among the processors makes a significant difference.
  
  
    In this paper, we have brought together two existing lines of work: the training method of Watanabe et al. (2007), and the models of Chiang (2005) and Marton and Resnik (2008).
    Watanabe et al.&#8217;s work showed that large-margin training with MIRA can be made feasible for state-of-the-art MT systems by using a manageable tuning set; we have demonstrated that parallel processing and exploiting more of the parse forest improves MIRA&#8217;s performance and that, even using the same set of features, MIRA&#8217;s performance compares favorably to MERT in terms of both translation quality and computational cost.
    Marton and Resnik (2008) showed that it is possible to improve translation in 