eers, and a small number of paid annotators.
    More than 120 people participated in the manual evaluation5, with 89 people putting in more than an hour&#8217;s worth of effort, and 29 putting in more than four hours.
    A collective total of 337 hours of labor was invested.6 We asked people to evaluate the systems&#8217; output in two different ways: The total number of judgments collected for the different modes of annotation is given in Table 3.
    In all cases, the output of the various translation systems were judged on equal footing; the output of system combinations was judged alongside that of the individual system, and the constrained and unconstrained systems were judged together.
    Ranking translations relative to each other is a reasonably intuitive task.
    We therefore kept the instructions simple: Rank translations from Best to Worst relative to the other choices (ties are allowed).
    5We excluded data from three errant annotators, identified as follows.
    We considered annotators com