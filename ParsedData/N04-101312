deep, non-context-free grammar, and linear parsing time is possible for sentences beyond that length.
    The Complete grammar achieved 100% coverage of section 23 as unseen unlabeled data: 79% as full parses, 21% FRAGMENT and/or SKIMMED parses.
    The stochastic disambiguation model we employ defines an exponential (a.k.a. log-linear or maximum-entropy) probability model over the parses of the LFG grammar.
    The advantage of this family of probability distributions is that it allows the user to encode arbitrary properties of the parse trees as feature-functions of the probability model, without the feature-functions needing to be independent and non-overlapping.
    The general form of conditional exponential models is as follows: where Z&#955;(y) = ExEX(y) e&#955;'f(x) is a normalizing constant over the set X(y) of parses for sentence y, X is a vector of log-parameters, f is a vector of featurevalues, and X &#183; f(x) is a vector dot product denoting the (log-)weight of parse x. Dynamic-programming algo