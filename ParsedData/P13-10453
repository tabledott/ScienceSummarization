    But, even with large corpora, many n-grams will never be seen during training, especially when n is large.
    In these cases, one cannot simply use distributional similarities to represent unseen phrases.
    In this work, we present a new solution to learn features and phrase representations even for very long, unseen n-grams.
    We introduce a Compositional Vector Grammar Parser (CVG) for structure prediction.
    Like the above work on parsing, the model addresses the problem of representing phrases and categories.
    Unlike them, it jointly learns how to parse and how to represent phrases as both discrete categories and continuous vectors as illustrated in Fig.
    1.
    CVGs combine the advantages of standard probabilistic context free grammars (PCFG) with those of recursive neural networks (RNNs).
    The former can capture the discrete categorization of phrases into NP or PP while the latter can capture fine-grained syntactic and compositional-semantic information on phrases and words.
    This