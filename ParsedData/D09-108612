 al., 2005).
    Instead, our models learn the necessary transformations that align and transform a source tree into a target tree.
    Other researchers have tackled the interesting task of learning parsers from unparsed bitext alone (Kuhn, 2004; Snyder et al., 2009); our methods take advantage of investments in high-resource languages such as English.
    In work most closely related to this paper, Ganchev et al. (2009) constrain the posterior distribution over target-language dependencies to align to source dependencies some &#8220;reasonable&#8221; proportion of the time (&#8776; 70%, cf.
    Table 2 in this paper).
    This approach performs well but cannot directly learn regular cross-language non-isomorphisms; for instance, some fixup rules for auxiliary verbs need to be introduced.
    Finally, Huang et al. (2009) use features, somewhat like QG configurations, on the shift-reduce actions in a monolingual, targetlanguage parser.
  
  
    As discussed in &#167;1, the adaptation scenario is a special ca