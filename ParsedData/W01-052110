rence statistics seem to be of no benefit when attempting to generalize to a new corpus.
  
  
    The relatively high performance of a parsing model with no lexical bigram statistics on the WSJ task led us to explore whether it might be possible to significantly reduce the size of the parsing model by selectively removing parameters without sacrificing performance.
    Such a technique reduces the parser's memory requirements as well as the overhead of loading and storing the model, which could be desirable for an application where limited computing resources are available.
    Significant effort has gone into developing techniques for pruning statistical language models for speech recognition, and we borrow from this work, using the weighted difference technique of Seymore and Rosenfeld (1996).
    This technique applies to any statistical model which estimates probabilities by backing off, that is, using probabilities from a less specific distribution when no data are available are available for the full d