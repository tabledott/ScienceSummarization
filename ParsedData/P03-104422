nce.
    Co-Training The type of learning described in this paper differs from the co-training method, covered, e.g., in (Blum and Mitchell, 1998).
    In co-training, learning centers on labeling a set of data-points in situations where these data-points have multiple disjoint and redundant views.8 Examples of spaces of such data-points are strings of text containing proper names, (Collins and Singer, 1999), or Web pages relevant to a query (Blum and Mitchell, 1998).
    Co-training iteratively trains, or refines, two or more n-way classifiers.9 Each classifier utilizes only one of the views on the data-points.
    The main idea is that the classifiers can start out weak, but will strengthen each other as a result of learning, by labeling a growing number of data-points based on the mutually independent sets of evidence that they provide to each other.
    In this paper the context is somewhat different.
    A data-point for each learner is a single document in the corpus.
    The learner assigns a binary la