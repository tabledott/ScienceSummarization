er the semantic information in the vectors of their context words, rather than over the context words themselves.
    Start with an r-dimensional vector space N with basis {&#8722;&#8594;n i}i, in which meaning vectors of atomic words, such as nouns, live.
    The basis vectors of N are in principle all the words from the corpus, however in practice and following Mitchell and Lapata (2008) we had to restrict these to a subset of the most occurring words.
    These basis vectors are not restricted to nouns: they can as well be verbs, adjectives, and adverbs, so that we can define the meaning of a noun in all possible contexts&#8212;as is usual in context-based models&#8212;and not only in the context of other nouns.
    Note that basis words with relational types are treated as pure lexical items rather than as semantic objects represented as matrices.
    In short, we count how many times a noun has occurred close to words of other syntactic types such as &#8216;elect&#8217; and &#8216;scientific&#8217;, rath