s, buyouts and jail in the financial news domain.
    In practice, we use this technique to find correspondences among all features, not just word features.
  
  
    Structural correspondence learning involves a source domain and a target domain.
    Both domains have ample unlabeled data, but only the source domain has labeled training data.
    We refer to the task for which we have labeled training data as the supervised task.
    In our experiments, the supervised task is part of speech tagging.
    We require that the input x in both domains be a vector of binary features from a finite feature space.
    The first step of SCL is to define a set of pivot features on the unlabeled data from both domains.
    We then use these pivot features to learn a mapping 0 from the original feature spaces of both domains to a shared, low-dimensional real-valued feature space.
    A high inner product in this new space indicates a high degree of correspondence.
    During supervised task training, we use both the tran