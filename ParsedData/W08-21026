 applications, or may improve dependency parsing accuracy, as is the case in our experiments.2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming.
    Taskar et al. (2004) describe a max-margin approach; however, in this work training sentences were limited to be of 15 words or less.
    Clark and Curran (2004) describe a log-linear GLM for CCG parsing, trained on the Penn treebank.
    This method makes use of parallelization across an 18 node cluster, together with up to 25GB of memory used for storage of dynamic programming structures for training data.
    Clark and Curran (2007) describe a perceptronbased approach for CCG parsing which is considerably more efficient, and makes use of a supertagging model to prune the search space of the full parsing model.
    Recent work (Petrov et al., 2007; Finkel et al., 2008) describes log-linear GLMs applied to PCFG representations, but does not make use of dependency features.
  
  
    This section 