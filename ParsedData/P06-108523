nique word types must be generated via P0, but in the bigram model there is an additional level of discounting (the unigram process) before reaching P0.
    Smaller values of &#945;0 lead to fewer word types with fewer characters on average.
    Table 3 compares the optimal results of the HDP model to the only previous model incorporating bigram dependencies, NGS.
    Due to search, the performance of the bigram NGS model is not much different from that of the unigram model.
    In Figure 5: Word (F) and lexicon (LF) F-score (a) as a function of &#945;0, with &#945;1 = 10 and (b) as a function of &#945;1, with &#945;0 = 1000. in bold.
    HDP results are with p# = .5, &#945;0 = 1000, and &#945;1 = 10. contrast, our HDP model performs far better than our DP model, leading to the highest published accuracy for this corpus on both tokens and lexical items.
    Overall, these results strongly support our hypothesis that modeling bigram dependencies is important for accurate word segmentation.
  
  
    In this pa