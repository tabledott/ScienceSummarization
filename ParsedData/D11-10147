toencoder models such as the recursive autoassociative memory (RAAM) model (Pollack, 1990; Voegtlin and Dominey, 2005) or recurrent neural networks (Elman, 1991) since sigmoid units are inherently continuous.
    Pollack circumvented this problem by having vocabularies with only a handful of words and by manually defining a threshold to binarize the resulting vectors.
    The goal of autoencoders is to learn a representation of their inputs.
    In this section we describe how to obtain a reduced dimensional vector representation for sentences.
    In the past autoencoders have only been used in setting where the tree structure was given a-priori.
    We review this setting before continuing with our model which does not require a given tree structure.
    Fig.
    2 shows an instance of a recursive autoencoder (RAE) applied to a given tree.
    Assume we are given a list of word vectors x = (x1,... , xm) as described in the previous section as well as a binary tree structure for this input in the form of bra