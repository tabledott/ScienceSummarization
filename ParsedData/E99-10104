lustering we use a maximum-likelihood approach as in the monolingual case.
    We maximize the joint probability of a bilingual training corpus To perform the maximization of Eq.
    (6) we have to model the monolingual a priori probability p(ef1E) and the translation probability p(f lel; e, F).
    For the first we use the class-based bigram probability from Eq.
    (1).
    To model p(fillef; E, .7) we assume the existence of an alignment af.
    We assume that every word fj is produced by the word ea, at position a3 in the training corpus with the probability P(filea,): The word alignment ail is trained automatically using statistical translation models as described in (Brown et al., 1993; Vogel et al., 1996).
    The idea is to introduce the unknown alignment a as hidden variable into a statistical model of the translation probability p(glef).
    By applying the EMalgorithm we obtain the model parameters.
    The alignment cif that we use is the Viterbi-Alignment of an HMM alignment model similar to (Vog