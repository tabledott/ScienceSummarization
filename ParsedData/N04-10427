ach shown in Figure 2.
    In this paper we provide an empirical study of these three priors.
    With a Gaussian prior, log-likelihood (2) is penalized as follows: This adjusted constraint (as well as the adjustments imposed by the other two priors) is intuitively understandable: rather than matching exact empirical feature frequencies, the model is tuned to match discounted feature frequencies.
    Chen and Rosenfeld (2000) discuss this in the context of other discounting procedures common in language modeling.
    We call the term subtracted from the empirical counts (in this case &#955;k/&#963;2) a discounted value.
    The variance can be feature dependent.
    However for simplicity, constant variance is often used for all features.
    In this paper, however, we experiment with several alternate versions of Gaussian prior in which the variance is feature dependent.
    Although Gaussian (and other) priors are gradually overcome by increasing amounts of training data, perhaps not at the right rate.
    