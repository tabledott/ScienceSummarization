ifferent sentences result in different PASs.
    For example, the sentence: &#8220;Antigens were originally defined as antibodies which bound specifically to non-self molecules&#8221;, uses the same words as (2) but has different meaning.
    Its PB annotation: (3)Antigens were originally defined as [ARG1 antibodies] [R&#8722;A1 which] [rel bound] [ARGM&#8722;MNR specifically] [ARG2 to non-self molecules], clearly differs from (2), as ARG2 is now nonself molecules; consequently, the PASs are also different.
    Once we have assumed that parse trees and PASs can improve on the simple BOW representation, we face the problem of representing tree structures in learning machines.
    Section 3 introduces a viable approach based on tree kernels.
  
  
    As mentioned above, encoding syntactic/semantic information represented by means of tree structures in the learning algorithm is problematic.
    A first solution is to use all its possible substructures as features.
    Given the combinatorial explosion of consid