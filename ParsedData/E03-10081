 problems such as word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998) and named-entity recognition (Collins and Singer, 1999).
    However, these tasks typically involved a small set of labels (around 2-3) and a relatively small parameter space.
    It is therefore instructive to consider co-training for more complex models.
    Compared to these earlier models, a statistical parser has a larger parameter space, and instead of class labels, it produces recursively built parse trees as output.
    Previous work in co-training statistical parsers (Sarkar, 2001) used two components of a single parsing framework (that is, a parser and a supertagger for that parser).
    In contrast, this paper considers co-training two diverse statistical parsers: the Collins lexicalized PCFG parser and a Lexicalized Tree Adjoining Grammar (LTAG) parser.
    Section 2 reviews co-training theory.
    Section 3 considers how co-training applied to training statistical parsers can be made co