t of patterns used for extraction to take advantage of appositions.
    Further, following Banko and Brill (2001), we increase our yield by increasing the amount of data used by an order of magnitude over previously published work.
    Finally, in order to address the Precision problem, we use machine learning techniques to filter the output of the part of speech patterns, thus purifying the extracted instances.
    Approximately 15GB of newspaper text was collected from: the TREC 9 corpus (~3.5GB), the TREC 2002 corpus (~3.5GB), Yahoo!
    News (.5GB), the AP newswire (~2GB), the Los Angeles Times (~.5GB), the New York Times (~2GB), Reuters (~.8GB), the Wall Street Journal (~1.2GB), and various online news websites (~.7GB).
    The text was cleaned of HTML (when necessary), word and sentence segmented, and part of speech tagged using Brill&#8217;s tagger (Brill, 1994).
    Part of speech patterns were generated to take advantage of two syntactic constructions that often indicate concept-instance relationship