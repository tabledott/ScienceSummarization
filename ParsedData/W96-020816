indirect comparisons to other methods.
    Learning curves were generated by splitting the preprocessed &amp;quot;line&amp;quot; corpus into 1,200 training examples and 894 test cases, training all methods on an increasingly larger subset of the training data and repeatedly testing them on the test set.
    Learning curves are fairly common in machine learning but not in corpus-based language research.
    We believe they are important since they reveal how algorithms perform with varying amounts of training data and how their performance improves with additional training.
    Results on a fixed-sized training set gives only one data point on the learning curve and leaves the possibility that differences between algorithms are hidden due to a ceiling effect, in which there are sufficient training examples for all methods to reach near Bayes-optimal performance.3 Learning 3Bayes-optimal performance is achieved by always picking the category with the maximum probability given all of its features.
    This requi