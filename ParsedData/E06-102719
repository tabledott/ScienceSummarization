 any neutrals.
			Figure 1: Accuracy of word sentiment tagging.
			884 were also found in GI-H4 and/or HM lists, which allowed us to evaluate STEP performance and HM-GI agreement on the subset of neutrals as well.
			The graph in Figure 1 shows the distributionof adjectives by Net Overlap scores and the aver age accuracy/agreement rate for each group.Figure 1 shows that the greater the Net Over lap Score, and hence, the greater the distance of the word from the neutral subcategory (i.e., from zero), the more accurate are STEP results and thegreater is the agreement between two teams of hu man annotators (HM and GI-H4).
			On average, for all categories, including neutrals, the accuracy of STEP vs. GI-H4 was 66.5%, human-annotated HM had 78.7% accuracy vs. GI-H4.
			For the words with Net Overlap of ?7 and greater, both STEPand HM had accuracy around 90%.
			The accu racy declined dramatically as Net Overlap scores approached zero (= Neutrals).
			In this category,human-annotated HM showed only 20% agree ment 