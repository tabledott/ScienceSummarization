
    These Chinese sentences were sorted according to their n-gram overlap (see Section 3.5) with the development corpus, and the top 5,000 Chinese sentences were used.
    The selection and scoring in Algorithm 1 were performed using confidence estimation with a threshold.
    Again, a new phrase table was trained on these data.
    As can be seen in Table 5, this system outperforms the baseline system on all test corpora.
    The error rates are significantly reduced in all three settings, and BLEU score increases in all cases.
    A comparison with Table 4 shows that transductive learning on the development set and test corpora, adapting the system to their domain and style, is more effective in improving the SMT system than the use of additional source language data.
    In all experiments on NIST, Algorithm 1 was run for one iteration.
    We also investigated the use of an iterative procedure here, but this did not yield any improvement in translation quality.
  
  
    Semi-supervised learning has been