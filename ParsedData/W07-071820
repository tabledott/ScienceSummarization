them as they are intended to be&#8212;scores classifying the translations of sentences into different types&#8212;the inter-annotator agreement was barely considered fair, and the intra-annotator agreement was only moderate.
    Even when we reassessed fluency and adequacy as relative ranks the agreements increased only minimally.
    The agreement on the other two types of manual evaluation that we introduced were considerably better.
    The both the sentence and constituent ranking had moderate inter-annotator agreement and substantial intra-annotator agreement.
    Because the constituent ranking examined the translations of short phrases, often times all systems produced the same translations.
    Since these trivially increased agreement (since they would always be equally ranked) we also evaluated the inter- and intra-annotator agreement when those items were excluded.
    The agreement remained very high for constituent-based evaluation.
    We used the web interface to collect timing information.
   