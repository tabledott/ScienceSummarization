her the Bleu score improvements translate into improvements that can be perceived by humans.
    To this end, we randomly selected 138 sentences of less than 20 words from our development corpus; we expected the translation quality of sentences of this size to be easier to assess than that of sentences that are very long.
    We prepared a web-based evaluation interface that showed for each input sentence: The evaluated `MT systems&#8221; were the six systems shown in Table 1 and one of the reference translations.
    The reference translation presented as automatically produced output was selected from the set of four reference translations provided by NIST so as to be representative of human translation quality.
    More precisely, we chose the second best reference translation in the NIST corpus according to its Bleu score against the other three reference translations.
    The seven outputs were randomly shufied and presented to three English speakers for assessment.
    The judges who participated in our