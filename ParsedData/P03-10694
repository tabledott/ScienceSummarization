se an algorithm that first identifies topically related groups of sentences and then orders them according to chronological information.
    In this paper we introduce an unsupervised probabilistic model for text structuring that learns ordering constraints from a large corpus.
    The model operates on sentences rather than facts in a knowledge base and is potentially useful for text-to-text generation applications.
    For example, it can be used to order the sentences obtained from a multidocument summarizer or a question answering system.
    Sentences are represented by a set of informative features (e.g., a verb and its subject, a noun and its modifier) that can be automatically extracted from the corpus without recourse to manual annotation.
    The model learns which sequences of features are likely to co-occur and makes predictions concerning preferred orderings.
    Local coherence is thus operationalized by sentence proximity in the training corpus.
    Global coherence is obtained by greedily sear