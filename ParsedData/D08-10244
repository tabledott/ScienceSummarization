n progressively refine it.
    Let e, by abuse of notation, stand for both output strings and their derivations.
    We represent the feature vector for derivation e as h(e).
    Initialize the feature weights w. Then, repeatedly: passes through the training data are made, we only average the weight vectors from the last pass.)
    The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training, and has been shown to work well in practice (Freund and Schapire, 1999; Collins, 2002).
    We follow McDonald et al. (2005) in applying this technique to MIRA.
    Note that the objective (1) is not the same as that used by Watanabe et al. ; rather, it is the same as that used by Crammer and Singer (2003) and related to that of Taskar et al.
    (2005).
    We solve this optimization problem using a variant of sequential minimal optimization (Platt, 1998): for each i, initialize &#945;ij = C for a single value of j such that e