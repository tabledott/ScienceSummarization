t when the penalty was set to &#8722;log(0.1), the accuracy of the model jumped to 80.6%.
  
  
    We proposed to use constraints as a way to guide semi-supervised learning.
    The framework developed is general both in terms of the representation and expressiveness of the constraints, and in terms of the underlying model being learned &#8211; HMM in the current implementation.
    Moreover, our framework is a useful tool when the domain knowledge cannot be expressed by the model.
    The results show that constraints improve not only the performance of the final inference stage but also propagate useful information during the semisupervised learning process and that training with the constraints is especially significant when the number of labeled training data is small.
    Acknowledgments: This work is supported by NSF SoDHCER-0613885 and by a grant from Boeing.
    Part of this work was done while Dan Roth visited the Technion, Israel, supported by a Lady Davis Fellowship.
  

