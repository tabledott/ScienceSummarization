CDER Test 1 SMT 42.21 7.888 65.40 63.24 40.45 45.58 37.80 40.09 SMT+WSD 42.38 7.902 65.73 63.64 39.98 45.30 37.60 39.91 Test 2 SMT 41.49 8.167 66.25 63.85 40.95 46.42 37.52 40.35 SMT+WSD 41.97 8.244 66.35 63.86 40.63 46.14 37.25 40.10 Test 3 SMT 49.91 9.016 73.36 70.70 35.60 40.60 32.30 35.46 SMT+WSD 51.05 9.142 74.13 71.44 34.68 39.75 31.71 34.58 Table 2: Evaluation results on the NIST test set: integrating the WSD translation predictions improves BLEU, NIST, METEOR, WER, PER, CDER and TER Exper.
			BLEU NIST METEOR METEOR (no syn) TER WER PER CDER SMT 20.41 7.155 60.21 56.15 76.76 88.26 61.71 70.32 SMT+WSD 20.92 7.468 60.30 56.79 71.34 83.87 57.29 67.38 5.1 Data set.
			Preliminary experiments are conducted using training and evaluation data drawn from the multilin gual BTEC corpus, which contains sentences used inconversations in the travel domain, and their transla tions in several languages.
			A subset of this data wasmade available for the IWSLT06 evaluation cam paign (Paul, 2006); the training set con