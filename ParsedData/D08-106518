
    We then train word alignment models (Och and Ney, 2003) using 6 Model-1 iterations and 6 HMM iterations.
    An additional 2 iterations of Model-4 are performed for zhen and enzh pairs.
    Word Alignments in both source-to-target and target-to-source directions are obtained using the Maximum A-Posteriori (MAP) framework (Matusov et al., 2004).
    An inventory of phrase-pairs up to length 5 is then extracted from the union of source-target and target-source alignments.
    Several feature functions are then computed over the phrasepairs.
    5-gram word language models are trained on the allowed monolingual corpora.
    Minimum Error Rate Training under BLEU is used for estimating approximately 20 feature function weights over the dev1 development set.
    Translation is performed using a standard dynamic programming beam-search decoder (Och and Ney, 2004) using two decoding passes.
    The first decoder pass generates either a lattice or an N-best list.
    MBR decoding is performed in the second pass.