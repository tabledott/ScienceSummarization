 the decoder, we use several thresholds to limit search beams for each span: It is worth noting that the decoder does not force a complete target parse tree to be generated.
    If no rules can be used to generate a complete target parse tree, the decoder just outputs whatever have phrase rules2.
    Finally, we investigate the impact of maximal sub-tree number and sub-tree depth in our model.
    All of the following discussions are held on the training and test data. been translated so far monotonically as one hypothesis.
  
  
    We conducted Chinese-to-English translation experiments.
    We trained the translation model on the FBIS corpus (7.2M+9.2M words) and trained a 4gram language model on the Xinhua portion of the English Gigaword corpus (181M words) using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing.
    We used sentences with less than 50 characters from the NIST MT-2002 test set as our development set and the NIST MT2005 test set as our test set.
    We used the Stanford