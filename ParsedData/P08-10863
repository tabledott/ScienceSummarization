present a distributed version of the modified algorithm, which makes it feasible to obtain word classifications using billions of tokens of training data.
    We then show that using partially class-based language models trained using the resulting classifications together with word-based language models in a state-of-the-art statistical machine translation system yields improvements despite the very large size of the word-based models used.
  
  
    By partitioning all N&#8222; words of the vocabulary into Nc sets, with c(w) mapping a word onto its equivalence class and c(wi) mapping a sequence of words onto the sequence of their respective equivalence classes, a typical class-based n-gram model approximates P(wi|wi&#8722;1 thus greatly reducing the number of parameters in the model, since usually Nc is much smaller than N&#8222;.
    In the following, we will call this type of model a two-sided class-based model, as both the history of each n-gram, the sequence of words conditioned on, as well as the predi