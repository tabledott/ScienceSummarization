 unlabeled data for many supervised learning applications in Natural Language Processing (NLP).
    Therefore, to improve performance, the development of an effective framework for semi-supervised learning (SSL) that uses both labeled and unlabeled data is attractive for both the machine learning and NLP communities.
    We expect that such SSL will replace most supervised learning in real world applications.
    In this paper, we focus on traditional and important NLP tasks, namely part-of-speech (POS) tagging, syntactic chunking, and named entity recognition (NER).
    These are also typical supervised learning applications in NLP, and are referred to as sequential labeling and segmentation problems.
    In some cases, these tasks have relatively large amounts of labeled training data.
    In this situation, supervised learning can provide competitive results, and it is difficult to improve them any further by using SSL.
    In fact, few papers have succeeded in showing significantly better results than sta