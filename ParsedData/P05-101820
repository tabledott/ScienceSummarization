aving seen the source texts.
    The ratings (approximately 23 per summary) given by our subjects were averaged to provide a rating between 1 and 7 for each summary.
    The reliability of the collected judgments is crucial for our analysis; we therefore performed several tests to validate the quality of the annotations.
    First, we measured how well humans agree in their coherence assessment.
    We employed leaveone-out resampling4 (Weiss and Kulikowski, 1991), by correlating the data obtained from each participant with the mean coherence ratings obtained from all other participants.
    The inter-subject agreement was r = .768.
    Second, we examined the effect of different types of summaries (human- vs. machine-generated.)
    An ANOVA revealed a reliable effect of summary type: F(1;15) = 20.38, p &lt; 0.01 indicating that human summaries are perceived as significantly more coherent than system-generated ones.
    Finally, the judgments of our participants exhibit a significant correlation with DUC eva