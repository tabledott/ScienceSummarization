n is dependent on the syntactic structure being instantiated in natural language.1 Assuming that each dimension in the starting vectors v1 and v2 is a candidate predictor, and that each dimension in the composed vector v3 is a dependent variable, vector-based semantic compositionality can be formulated as a problem of multivariate multiple regression.
    This is, in principle, a tractable problem that can be solved by standard machine learning techniques such as multilayer perceptrons or support vector machines.
    However, given that sequences of words tend to be of very low frequency (and thus difficult to represent in a DSM), suitable data sets will inevitably suffer the curse of dimensionality: we will often have many more variables (dimensions) than observations.
    Partial Least Squares Regression (PLSR) is a multivariate regression technique that has been designed specifically to tackle such situations with high dimensionality and limited data.
    PLSR is widely used in in unrelated fields such as 