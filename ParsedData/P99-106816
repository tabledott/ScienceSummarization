e two judges, between STRAND and each individual judge, and the agreement between STRAND and the intersection of the two judges' annotations &#8212; that is, STRAND evaluated against only those cases where the two judges agreed, which are therefore the items we can regard with the highest confidence.
    The table also shows Cohen's lc, an agreement measure that corrects for chance agreement (Carletta, 1996); the most important value in the table is the value of 0.7 for the two human judges, which can be interpreted as sufficiently high to indicate that the task is reasonably well defined.
    (As a rule of thumb, classification tasks with ic &lt; 0.6 are generally thought of as suspect in this regard.)
    The value of N is the number of pairs that were included, after excluding those for which the human judgement in the comparison was undecided.
    Since the cases where the two judges agreed can be considered the most reliable, these were used as the basis for the computation of recall and precision.
    F