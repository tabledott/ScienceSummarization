 labeled data is relatively large (&gt;200K), making it hard to benefit from unlabeled data.
    We show that our ASO-based semi-supervised learning method (hereafter, ASO-semi) can produce results appreciably better than all of the top systems, by using unlabeled data as the only additional resource.
    In particular, we do not use any gazetteer information, which was used in all other systems.
    The CoNLL corpora are annotated with four types of named entities: persons, organizations, locations, and miscellaneous names (e.g., &#8220;World Cup&#8221;).
    We use the official training/development/test splits.
    Our unlabeled data sets consist of 27 million words (English) and 35 million words (German), respectively.
    They were chosen from the same sources &#8211; Reuters and ECI Multilingual Text Corpus &#8211; as the provided corpora but disjoint from them.
    Our feature representation is a slight modification of a simpler configuration (without any gazetteer) in (Zhang and Johnson, 2003), as show