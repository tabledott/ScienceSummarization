.
    In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor and Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy.
    One of the most powerful representations for this is Markov logic, which is a probabilistic extension of first-order logic (Richardson and Domingos, 2006).
    Markov logic makes it possible to compactly specify probability distributions over complex relational domains, and has been successfully applied to unsupervised coreference resolution (Poon and Domingos, 2008) and other tasks.
    A Markov logic network (MLN) is a set of weighted first-order clauses.
    Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause.
    The weight of a feature is the weight of the first-order clause that originated it.
    The probability of a state x in such a network is given by the log-linear m