stributed representation is compact, in the sense that it can represent an exponential number of clusters in the number of dimensions.
    Word embeddings are typically induced using neural language models, which use neural networks as the underlying predictive model (Bengio, 2008).
    Historically, training and testing of neural language models has been slow, scaling as the size of the vocabulary for each model computation (Bengio et al., 2001; Bengio et al., 2003).
    However, many approaches have been proposed in recent years to eliminate that linear dependency on vocabulary size (Morin &amp; Bengio, 2005; Collobert &amp; Weston, 2008; Mnih &amp; Hinton, 2009) and allow scaling to very large training corpora.
    Collobert and Weston (2008) presented a neural language model that could be trained over billions of words, because the gradient of the loss was computed stochastically over a small sample of possible outputs, in a spirit similar to Bengio and S&#180;en&#180;ecal (2003).
    This neural model of