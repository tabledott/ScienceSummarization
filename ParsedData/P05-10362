i-supervised (our term for a combination of supervised and unsupervised) methods of sentence compression with inspiration from the K&amp;M model, and create additional constraints to improve the compressions.
    We conclude with the problems inherent in both models.
  
  
    The K&amp;M probabilistic model, adapted from machine translation to this task, is the noisy-channel model.
    In machine translation, one imagines that a string was originally in English, but that someone adds some noise to make it a foreign string.
    Analogously, in the sentence compression model, the short string is the original sentence and someone adds noise, resulting in the longer sentence.
    Using this framework, the end goal is, given a long sentence l, to determine the short sentence s that maximizes The probability of the long sentence, P(l) can be ignored when finding the maximum, because the long sentence is the same in every case.
    P(s) is the source model: the probability that s is the original sentence.
    P(l  