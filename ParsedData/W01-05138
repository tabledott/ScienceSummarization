ed or treated as words.
    Since we are equally interested in finding units like &#8220;Dr.&#8221; and &#8220;U.
    S.,&#8221; we opt to treat punctuation as words.
    Once we tokenize, we use Church&#8217;s (1995) suffix array approach to identify word n-grams that occur at least T times (for T=10).
    We then rank-order the n-gram list in accordance to each probabilistic algorithm.
    This task is non-trivial since most algorithms were originally suited for finding twoword collocations.
    We must therefore decide how to expand the algorithms to identify general n-grams (say, C=w1w2 ...wn).
    We can either generalize or approximate.
    Since generalizing requires exponential compute time and memory for several of the algorithms, approximation is an attractive alternative.
    One approximation redefines X and Y to be, respectively, the word sequences w1w2 ...wi and wi+1wi+2...wn, where i is chosen to maximize PXPY.
    This has a natural interpretation of being the expected probability of concatena