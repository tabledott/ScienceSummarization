 other judges noticeably improves.
    Because judge B's poor performance in the second tagging experiment is linked to a difference in procedure, judge B's tags are excluded from our subsequent analysis of the data gathered during the second tagging experiment.
    Table 2 shows the changes, from study 1 to study 2, in the Kappa values for pairwise agreement among the judges.
    The best results are clearly for the two who are not authors of this paper (D and M).
    The Kappa value for the agreement between D and M considering all certainty ratings reaches .76, which allows tentative conclusions on Krippendorf's scale (1980).
    If we exclude the sentences with certainty rating 0, the Kappa values for pairwise agreement between M and D and between J and M are both over .8, which allows definite conclusions on Krippendorf's scale.
    Finally, if we only consider sentences with certainty 2 or 3, the pairwise agreements among M, D, and J all have high Kappa values, 0.87 and over.
    We are aware of only on