ik, 1995b; Wu and Palmer, 1994).
    All of those similarity measures are defined directly by a formula.
    We use instead an information-theoretic definition of similarity that can be derived from the following assumptions: where cornmon(A, B) is a proposition that states the commonalities between A and B; /(s) is the amount of information contained in the proposition s. Assumption 2: The differences between A and B is measured by where describe(A, B) is a proposition that describes what A and B are.
    Assumption 3: The similarity between A and B, sim(A, B), is a function of their commonality and differences.
    That is, sim(A, B) = f (I (common(A, B)), I (describe(A, B))) The domain of f (x, y) is {(x , y)ix &gt; 0, y &gt; 0, y &gt; x}.
    Assumption 4: Similarity is independent of the unit used in the information measure.
    According to Information Theory (Cover and Thomas, 1991), /(s) = &#8212;log bP(s), where P(s) is the probability of s and b is the unit.
    When b = 2, /(s) is the number of bit