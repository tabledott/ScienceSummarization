istributional model, by earmarking a single cluster d for non-constituents.
    During the calculation of cluster assignments, only a non-crossing subset of the observed word sequences can be assigned to other, constituent clusters.
    This integrated approach is empirically successful.
    The CCM works as follows.
    Sentences are given as sequences s of word classes (parts-of-speech or otherwise).
    One imagines each sentence as a list of the O(n2) index pairs (i, j), each followed by the corresponding subspan isj and linear context That is, all spans guess their sequences and contexts given only a constituency decision b.7 This is a model P(s, B) over hidden bracketings and observed sentences, and it is estimated via EM to maximize the sentence likelihoods P(s) over the training corpus.
    Figure 6 shows the accuracy of the CCM model not only on English but for the Chinese and German corpora discussed above.8 Results are reported at convergence; for the English case, F1 is monotonic during training, 