bilities (12) require O(n) time to calculate for each of I samples.
    The hierarchical Pitman-Yor language model produces discounts that grow gradually as a function of n-gram counts.
    Notice that although each Pitman-Yor process Gu only has one discount parameter, the predictive probabilities (12) produce different discount values since tuw&#183; can take on different values for different words w. In fact tuw&#183; will on average be larger if cuw&#183; is larger; averaged over the posterior, the actual amount of discount will grow slowly as the count cuw&#183; grows.
    This is shown in Figure 2 (left), where we see that the growth of discounts is sublinear.
    The correspondence to interpolated Kneser-Ney is now straightforward.
    If we restrict tuw&#183; to be at most 1, that is, we will get the same discount value so long as cuw&#183; &gt; 0, i.e. absolute discounting.
    Further supposing that the strength parameters are all &#952;|u |= 0, the predictive probabilities (12) now directly reduces