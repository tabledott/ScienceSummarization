 as some more formbased labels.
    Furthermore, the tag set is structured so as to allow labelers to annotate a Switchboard conversation from transcripts alone (i.e., without listening) in about 30 minutes.
    Without these constraints the DA labels might have included some finer distinctions, but we felt that this drawback was balanced by the ability to cover a large amount of data.'
    Labeling was carried out in a three-month period in 1997 by eight linguistics graduate students at CU Boulder.
    Interlabeler agreement for the 42-label tag set used here was 84%, resulting in a Kappa statistic of 0.80.
    The Kappa statistic measures agreement normalized for chance (Siegel and Castellan, Jr. 1988).
    As argued in Carletta (1996), Kappa values of 0.8 or higher are desirable for detecting associations between several coded variables; we were thus satisfied with the level of agreement achieved.
    (Note that, even though only a single variable, DA type, was coded for the present study, our goal is, amo