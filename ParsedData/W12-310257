;Oracle HTER&#8221; DeltaAvg score of 0.77.
    We are not sure if this difference can be bridged completely, but having measured a quantitative difference between the current best-performance and a realistic upperbound is an important achievement of this shared-task.
    The results for the scoring task are presented in Table 13, sorted from best to worse by using the MAE metric scores (Equation 16) as primary key and the RMSE metric scores (Equation 17) as secondary key.
    The winning submission is SDLLW&#8217;s M5PbestDeltaAvg, with an MAE of 0.61 and an RMSE of 0.75 (the difference with respect to all the other submissions is statistically significant at p = 0.05, using pairwise bootstrap resampling (Koehn, 2004)).
    The strong, state-of-the-art quality-estimation baseline system is measured to have an MAE of 0.69 and RMSE of 0.82, with six other submissions having performances that are not different from the baseline at a statisticallysignificant level (p = 0.05), as shown by the gray area in the mid