 between number of tweets and accuracy, but accuracy (and lexical coverage) levels out after 750,000 tweets.
    We use the largest clustering (56 million tweets and 1,000 clusters) as the default for the released tagger. tagset plus several Twitter-specific tags, referred to in Table 1 as RITTERTW.
    Linguistic concerns notwithstanding (&#167;5.2), for a controlled comparison, we train and test our system on this data with the same 4-fold cross-validation setup they used, attaining 90.0% (f0.5%) accuracy.
    Ritter et al.&#8217;s CRFbased tagger had 85.3% accuracy, and their best tagger, trained on a concatenation of PTB, IRC, and Twitter, achieved 88.3% (Table 4).
    First, we compare to a tagger in the same setup as experiments on this data in Forsyth (2007), training on 90% of the data and testing on 10%; we average results across 10-fold cross-validation.25 The full tagger model achieved 93.4% (f0.3%) accuracy, significantly improving over the best result they report, 90.8% accuracy with a tagger tra