ll of the trees worked successfully with these programs.
    We tracked inter-annotator agreement during each phase of the project, using a method developed by Marcu et al. (1999) for computing kappa statistics over hierarchical structures.
    The kappa coefficient (Siegel and Castellan, 1988) has been used extensively in previous empirical studies of discourse (Carletta et al., 1997; Flammia and Zue, 1995; Passonneau and Litman, 1997).
    It measures pairwise agreement among a set of coders who make category judgments, correcting for chance expected agreement.
    The method described in Marcu et al. (1999) maps hierarchical structures into sets of units that are labeled with categorial judgments.
    The strengths and shortcomings of the approach are also discussed in detail there.
    Researchers in content analysis (Krippendorff, 1980) suggest that values of kappa &gt; 0.8 reflect very high agreement, while values between 0.6 and 0.8 reflect good agreement.
    Table 1 shows average kappa statistics ref