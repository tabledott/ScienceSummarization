e Figure 1.
    Following Och and Ney (2002), we depart from the traditional noisy-channel approach and use a more general log-linear model.
    The weight of each rule is: where the &#966;i are features defined on rules.
    For our experiments we used the following features, analogous to Pharaoh&#8217;s default feature set: model to learn a preference for longer or shorter derivations, analogous to Koehn&#8217;s phrase penalty (Koehn, 2003).
    The exceptions to the above are the two glue rules, (13), which has weight one, and (14), which has weight (16) w(S _&#65533; (S 1 X 2 , S 1 X 2 )) = exp(&#8212;&#955;g) the idea being that &#955;g controls the model&#8217;s preference for hierarchical phrases over serial combination of phrases.
    Let D be a derivation of the grammar, and let f(D) and e(D) be the French and English strings generated by D. Let us represent D as a set of triples (r, i, j), each of which stands for an application of a grammar rule r to rewrite a nonterminal that spans f(D)ji on the F