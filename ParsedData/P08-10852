.
    In recent work, researchers try to address these deficiencies by using dictionaries with unfiltered POS-tags, and testing the methods on &#8220;diluted dictionaries&#8221; &#8211; in which many of the lexical entries are missing (Smith and Eisner, 2005) (SE), (Goldwater and Griffiths, 2007) (GG), (Toutanova and Johnson, 2008) (TJ).
    All the work mentioned above focuses on unsupervised English POS tagging.
    The dictionaries are all derived from tagged English corpora (all recent work uses the WSJ corpus).
    As such, the setting of the research is artificial: there is no reason to perform unsupervised learning when an annotated corpus is available.
    The problem is rather approached as a workbench for exploring new learning methods.
    The result is a series of creative algorithms, that have steadily improved results on the same dataset: unsupervised CRF training using contrastive estimation (SE), a fully-bayesian HMM model that jointly performs clustering and sequence learning (GG), and a Baye