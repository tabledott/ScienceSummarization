the literature, but also to our implementation of state-of-the-art DSMs.
    These alternative models have been trained on the same corpus (with the same linguistic preprocessing) used to build the DM tuple tensors.
    This way, we aim at achieving a fairer comparison with alternative approaches in distributional semantics, abstracting away from the effects induced by differences in the training data.
    Most experiments report global (micro-averaged) test set accuracy (alone, or combined with other measures) to assess the performance of the algorithms.
    The number of correctly classified items among all test elements can be seen as a binomially distributed random variable, and we follow the ACL Wiki state-of-the-art site7 in reporting also Clopper&#8211;Pearson binomial 95% confidence intervals around the accuracies (binomial intervals and other statistical quantities were computed using the R package;8 where no further references are given, we used the standard R functions for the relevant analysis).
 