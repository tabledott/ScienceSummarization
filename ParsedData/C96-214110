)}, we can write the IIMM alignment probabilities in the form: 4 i - i') (5) p(ili', i ) = E ' s(1 - i') 1=1 This form ensures that for each word position i', i' = 1, ..., I, the ItMM alignment probabilities satisfy the normMization constraint.
			Note the similarity between Equations (2) and (5).
			The mixtm;e model can be interpreted as a zeroth-order model in contrast to the first-order tlMM model.
			As with the IBM2 model, we use again the max- imum approximation: J Pr(fiSle~) "~ max\]--\[ \[p(asl&lt;*j-1, z)p(fj l&lt;~,)\] (6) a ' / .ll.
			j,,, j= l In th is case, the task o f f ind ing the opt ima l alignment is more involved than in the case of the mixture model (lBM2).
			Thereibre, we have to re- sort to dynainic programming for which we have the following typical reeursion formula: Q(i, j ) = p(f j lel) ,nvax \[p(ili', 1) . Q(i', j - 1)\] i =l , . , , I Here, Q(i, j ) is a sort of partial probability as in time alignment for speech recognition (Jelinek, 197@.
			4.1 The Task and the Corpus.
			Th