agging can reach good results using the robust EMHMM learner when provided with good initial conditions, even with incomplete dictionaries.
    We presented a general family of algorithms to compute effective initial conditions: estimation of p(t|w) relying on an iterative process shifting probabilities between words and their contexts.
    The parameters of this process (definition of the contexts and initial estimations of p(t|w) can safely encapsulate rich linguistic intuitions.
    While recent work, such as GG, aim to use the Bayesian framework and incorporate &#8220;linguistically motivated priors&#8221;, in practice such priors currently only account for the fact that language related distributions are sparse - a very general kind of knowledge.
    In contrast, our method allow the incorporation of much more fine-grained intuitions.
    We tested the method on the challenging task of full morphological disambiguation in Hebrew (which was our original motivation) and on the standard WSJ unsupervised POS