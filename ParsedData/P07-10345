ection, we define and analyze domain adaptation from a theoretical point of view.
    We show that the need for domain adaptation arises from two factors, and the solutions are different for each factor.
    We restrict our attention to those NLP tasks that can be cast into multiclass classification problems, and we only consider discriminative models for classification.
    Since both are common practice in NLP, our analysis is applicable to many NLP tasks.
    Let X be a feature space we choose to represent the observed instances, and let Y be the set of class labels.
    In the standard supervised learning setting, we are given a set of labeled instances {(xi, yi)}&#65533; i&#65533;&#65533;, where xi &#8712; X, yi &#8712; Y, and (xi, yi) are drawn from an unknown joint distribution p(x, y).
    Our goal is to recover this unknown distribution so that we can predict unlabeled instances drawn from the same distribution.
    In discriminative models, we are only concerned with p(y|x).
    Following the maximu