y lack of punctuation handling, noise in paraphrase matching, and lack of discrimination between word types.
    We introduce new resources for all WMT languages including text normalizers, filtered paraphrase tables, and function word lists.
    We show that the addition of these resources to Meteor allows tuning versions of the metric that show higher correlation with human translation rankings and adequacy scores on unseen test data.
    The evaluation resources are modular, usable with any other evaluation metric or MT software.
    We also conduct a MT system tuning experiment on Urdu-English data to compare the effectiveness of using multiple versions of Meteor in minimum error rate training.
    While versions tuned to various types of human judgments do not perform as well as the widely used BLEU metric (Papineni et al., 2002), a balanced Tuning version of Meteor consistently outperforms BLEU over multiple end-to-end tune-test runs on this data set.
    The versions of Meteor corresponding to the tran