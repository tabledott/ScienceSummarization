ce.
  
  
    argument: T = {hx1, y1, d1i ..., hxL, yL, dLi} (xi a tree, yi &#8712; {&#177;1} is a class, and di (PLi=1 di = 1, di &#8805; 0) is a weight) returns: Optimal rule h&#710;t, &#710;yi begin We can thus see that both algorithms are essentially the same in terms of their feature space.
    The difference between them is the metric of margin; the margin of Boosting is measured in l1-norm, while, that of SVMs is measured in l2-norm.
    The question one might ask is how the difference is expressed in practice.
    The difference between them can be explained by sparseness.
    It is well known that the solution or separating hyperplane of SVMs is expressed as a linear combination of the training examples using some coefficients A, (i.e., w = PL i=1 Ai&#934;(xi)).
    Maximizing l2norm margin gives a sparse solution in the example space, (i.e., most of Ai becomes 0).
    Examples that have non-zero coefficient are called support vectors that form the final solution.
    Boosting, in contrast, performs 