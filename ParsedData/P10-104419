uces the risk of topic drift due to lack of identifiability, however we found this to not be a problem in practice.
    During development we found that the topics tend to remain stable across multiple samples after sufficient burn in, and multiple samples improved performance.
    Table 1 lists sample topics and high ranked words for each (for both arguments) as well as relations favoring those topics.
    We first compare the three LDA-based approaches to each other and two state of the art similarity based systems (Erk, 2007) (using mutual information and Jaccard similarity respectively).
    These similarity measures were shown to outperform the generative model of Rooth et al. (1999), as well as class-based methods such as Resnik&#8217;s.
    In this pseudo-disambiguation experiment an observed tuple is paired with a pseudo-negative, which has both arguments randomly generated from the whole vocabulary (according to the corpus-wide distribution over arguments).
    The task is, for each relation-argument