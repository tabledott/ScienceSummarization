hat receives the most votes.
    In order to compare our novel joint phrase representation and classifier learning framework to traditional methods, we use the following baselines: Random Since there are five classes, this gives 20% accuracy.
    Most Frequent Selecting the class which most frequently has the most votes (the class I understand).
    Baseline 1: Binary BoW This baseline uses logistic regression on binary bag-of-word representations that are 1 if a word is present and 0 otherwise.
    Baseline 2: Features This model is similar to traditional approaches to sentiment classification in that it uses many hand-engineered resources.
    We first used a spell-checker and Wordnet to map words and their misspellings to synsets to reduce the total number of words.
    We then replaced sentiment words with a sentiment category identifier using the sentiment lexica of the Harvard Inquirer (Stone, 1966) and LIWC (Pennebaker et al., 2007).
    Lastly, we used tf-idf weighting on the bag-of-word representatio