
  The Kappa Statistic: A Second Look
  
    dialogue structure coding scheme.
    23(1):13&#8211;31.
    Cicchetti, Domenic V. and Alvan R. Feinstein.
    1990.
    High agreement but low kappa: II.
    Resolving the paradoxes. of Clinical 43(6):551&#8211;558.
    Cohen, Jacob.
    1960.
    A coefficient of for nominal scales.
  
  
    In recent years, the kappa coefficient of agreement has become the de facto standard for evaluating intercoder agreement for tagging tasks.
    In this squib, we highlight issues that affect &#954; and that the community has largely neglected.
    First, we discuss the assumptions underlying different computations of the expected agreement component of &#954;.
    Second, we discuss how prevalence and bias affect the &#954; measure.
    In the last few years, coded corpora have acquired an increasing importance in every aspect of human-language technology.
    Tagging for many phenomena, such as dialogue acts (Carletta et al. 1997; Di Eugenio et al.
    2000), requires coder