model-based estimates for an average of 8-10 tokens in any 100-token passage.
    Most readability formulas become unreliable for passages of less than 100 tokens (Fry 1990).
    With Web applications, it is not uncommon for samples to contain as few as 10 tokens or less.
    For example, educational Web sites often segment a story or lesson into a series of image pages, with the only relevant page content being a caption.
    Short passages also arise for tasks such as estimating the reading difficulty of page titles, user queries, or questionnaire items.
    Our hypothesis was that the Smoothed Unigram model, having more fine-grained models of word usage, would be less sensitive to passage length and give superior accuracy for very short passages, compared to traditional semantic statistics.
    In the extreme case, consider two single-word &#8216;passages&#8217;: &#8216;bunny&#8217; and &#8216;bulkheads&#8217;.
    Both words have two syllables and both occur 5 times in the Carroll-DaviesRichman corpus.
  