1 or A2 by its parents, its outside score is: Replacing these quantities in (2) gives us the likelihood Pn(w, T) where these two annotations and their corresponding rules have been merged, around only node n. We approximate the overall loss in data likelihood due to merging A1 and A2 everywhere in all sentences wi by the product of this loss for each local change: This expression is an approximation because it neglects interactions between instances of a symbol at multiple places in the same tree.
    These instances, however, are often far apart and are likely to interact only weakly, and this simplification avoids the prohibitive cost of running an inference algorithm for each tree and annotation.
    We refer to the operation of splitting annotations and re-merging some them based on likelihood loss as a split-merge (SM) cycle.
    SM cycles allow us to progressively increase the complexity of our grammar, giving priority to the most useful extensions.
    In our experiments, merging was quite valuable.
  