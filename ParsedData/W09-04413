it is much more time consuming and taxing on human annotators than other types of human judgments, making it difficult and expensive to use.
    In addition, because HTER treats all edits equally, no distinction is made between serious errors (errors in names or missing subjects) and minor edits (such as a difference in verb agreement or a missing determinator).
    Different types of translation errors vary in importance depending on the type of human judgment being used to evaluate the translation.
    For example, errors in tense might barely affect the adequacy of a translation but might cause the translation be scored as less fluent.
    On the other hand, deletion of content words might not lower the fluency of a translation but the adequacy would suffer.
    In this paper, we examine these differences by taking an automatic evaluation metric and tuning it to these these human judgments and examining the resulting differences in the parameterization of the metric.
    To study this we introduce a new ev