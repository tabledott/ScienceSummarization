an invaluable tool for the day-to-day development of machine translation systems, they are only a imperfect substitute for human assessment of translation quality, or as the acronym BLEU puts it, a bilingual evaluation understudy.
    Many human evaluation metrics have been proposed.
    Also, the argument has been made that machine translation performance should be evaluated via task-based evaluation metrics, i.e. how much it assists performing a useful task, such as supporting human translators or aiding the analysis of texts.
    The main disadvantage of manual evaluation is that it is time-consuming and thus too expensive to do frequently.
    In this shared task, we were also confronted with this problem, and since we had no funding for paying human judgements, we asked participants in the evaluation to share the burden.
    Participants and other volunteers contributed about 180 hours of labor in the manual evaluation.
    We asked participants to each judge 200&#8211;300 sentences in terms of fluency a