 the result that f is the concatenation of the k feature vectors r1 ... rk).
    In our experiments on dependency parsing, we partitioned f into up to over 140 separate feature vectors corresponding to different feature types.
    For example, one feature vector rj might include only those features corresponding to word bigrams involved in dependencies (i.e., indicator functions tied to the word bigram (xm, xh) involved in a dependency (x, h, m, l)).
    We then define a generative model that assigns a probability corporates a second-order parsing model.
    In addition, we evaluate the SS-SCM for English dependency parsing with large amounts (up to 3.72 billion tokens) of unlabeled data.
    Throughout this paper we will use x to denote an input sentence, and y to denote a labeled dependency structure.
    Given a sentence x with n words, a labeled dependency structure y is a set of n dependencies of the form (h, m, l), where h is the index of the head-word in the dependency, m is the index of the modifier w