Our model outperforms their n-gram languagemodel approach by over 5%.
			Since the two approaches are not tested on the same data this com parison is not conclusive, but we are optimistic that there is a real difference in accuracy since the type of texts used are not dissimilar.
			As in the case of the prepositions, it is interesting to see whether this high performance is equally distributed across thethree classes; this information is reported in Ta ble 8.
			Here we can see that there is a very strongcorrelation between amount of data seen in training and precision and recall.
			The indefinite arti cle?s lower ?learnability?, and its lower frequency appears not to be peculiar to our data, as it is also found by Gamon et al among others.The disparity in training is a reflection of the dis tribution of determiners in the English language.
			Perhaps if this imbalance were addressed, the model would more confidently learn contexts of use for a, too, which would be desirable in view of using this informatio