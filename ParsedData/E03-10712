S and HS obtain a maximum likelihood estimate (mLE) of the parameters, and, like other MLE methods, are susceptible to overfitting.
    A simple technique used to avoid overfitting is a frequency cutoff, in which only frequently occurring features are included in the model (Ratnaparkhi, 1998).
    However, more sophisticated smoothing techniques exist, such as the use of a Gaussian prior on the parameters of the model (Chen and Rosenfeld, 1999).
    This technique has been applied to language modelling (Chen and Rosenfeld, 1999), text classification (Nigam et al., 1999) and parsing (Johnson et al., 1999), but to our knowledge it has not been compared with the use of a feature cutoff.
    We explore the combination of Gaussian smoothing and a simple cutoff for two tagging tasks.
    The two taggers used for the experiments are a POS tagger, trained on the WSJ Penn Treebank, and a &amp;quot;supertagger&amp;quot;, which assigns tags from the much larger set of lexical types from Combinatory Categorial Grammar (c