onal resources to participants: to use as a dev set for system combination.
    These were translated by all participating sites, and distributed to system combination participants along with reference translations.
    Table 2 lists the 9 participants in the system combination task.
  
  
    As with past workshops, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores.
    It is our contention is defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/no judgment in the judgment task. that automatic measures are an imperfect substitute for human assessment of translation quality.
    Therefore, we define the manual evaluation to be primary, and use the human judgments to validate automatic metrics.
    Manual evaluation is time consuming, and it requires a large effort to conduct it on the scale of our workshop.
    We distributed the workload across a number of people, including shared-task participants, interested volunt