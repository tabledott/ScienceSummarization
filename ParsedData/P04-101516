ith a transformed context-free grammar (a first-order &#8220;Markov&#8221; grammar) &#8211; for brevity we omit the details here.
    In this way, given a set of candidates .77i(x) for the first i words of the string, we can generate a set of candidates Table 1: Left-child chain type counts (of length &gt; 2) for sections of the Wall St. Journal Treebank, and out-ofvocabulary (OOV) rate on the held-out corpus. for the first i + 1 words, Uh1EY-,(x)ADV(h'), where the ADV function uses the grammar as described above.
    We then calculate &#934;(h)&#183; &#945;&#175; for all of these partial hypotheses, and rank the set from best to worst.
    A FILTER function is then applied to this ranked set to give .77i+1.
    Let hk be the kth ranked hypothesis in Hi+1(x).
    Then hk E .77i+1 if and only if &#934;(hk) &#183; &#945;&#175; &gt; &#952;k.
    In our case, we parameterize the calculation of &#952;k with y as follows: The problem with using left-child chains is limiting them in number.
    With a left-recursive