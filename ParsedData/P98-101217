 similarity).
    The importance of using CAMP to extract summaries is verified by comparing the highest FMeasures achieved by the system for the two cases.
    The highest F-Measure for the former case is 84.6% while the highest F-Measure for the latter case is 78.0%.
    In comparison, for this task, named-entity tools like NetOwl and Textract would mark all the John Smiths the same.
    Their performance using our Threshold Figure 11: Precision, Recall, and F-Measure Using the B-CUBED Algorithm With Training On the Summaries scoring algorithm is 23% precision, and 100% recall.
    Figures 13 and 14 show the precision, recall, and F-Measure calculated using the MUC scoring algorithm.
    Also, the baseline case when all the John Smiths are considered to be the same person achieves 83% precision and 100% recall.
    The high initial precision is mainly due to the fact that the MUC algorithm assumes that all errors are equal.
    We have also tested our system on other classes of cross-document coreference li