Informally, mutual information compares the probability of observing x and y together (the joint probability) with the probabilities of observing x and y independently (chance).
    If there is a genuine association between x and y, then the joint probability P(x,y) will be much larger than chance P(x) P(y), and consequently 1(x,y) &gt;&gt; 0.
    If there is no interesting relationship between x and y, then P(x,y) P(x) P(y), and thus, 1(x,y)2-- 0.
    If x and y are in complementary distribution, then P(x,y) will be much less than P(x) P(y), forcing 1(x,y) &lt;&lt; 0.
    In our application, word probabilities, P(x) and P(y), are estimated by counting the number of observations of x and y in a corpus, f(x) and f(y), and normalizing by N, the size of the corpus.
    (Our examples use a number of different corpora with different sizes: 15 million words for the 1987 AP corpus, 36 million words for the 1988 AP corpus, and 8.6 million tokens for the tagged corpus.)
    Joint probabilities, P(x,y), are estimated b