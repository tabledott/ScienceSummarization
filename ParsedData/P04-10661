e alignment (Moore, 2002), aligning syntactictree fragments (Ding et al., 2003), and estimating phrase translation probabilities (Venugopal et al., 2003).
    Furthermore, at the 2003 Johns Hopkins summer workshop on statistical machine translation, a large number of features were tested to discover which ones could improve a state-of-the-art translation system, and the only feature that produced a &#8220;truly significant improvement&#8221; was the Model 1 score (Och et al., 2004).
    Despite the fact that IBM Model 1 is so widely used, essentially no attention seems to have been paid to whether it is possible to improve on the standard Expectation-Maximization (EM) procedure for estimating its parameters.
    This may be due in part to the fact that Brown et al. (1993a) proved that the log-likelihood objective function for Model 1 is a strictly concave function of the model parameters, so that it has a unique local maximum.
    This, in turn, means that EM training will converge to that maximum from any st