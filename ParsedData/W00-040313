n the second experiment.
    We concede that the time necessary to do a proper job at the second task is partly to blame.
    Since the baseline of random sentence selection is already included in the evaluation formulae, we used the Lead-based method (selecting the positionally first (n*r/c) sentences from each cluster where c = number of clusters) as the baseline to evaluate our system.
    In Table 10 we show the normalized performance (D) of MEAD, for the six clusters at nine compression rates.
    MEAD performed better than Lead in 29 (in bold) out of 54 cases.
    Note that for the largest cluster, Cluster D, MEAD outperformed Lead at all compression rates. showed how MEAD's sentence scoring weights can - be modified to produce summaries significantly better than the alternatives.
    We also looked at a property of multi-document clusters, namely cross-sentence information subsumption (which is related to the MMR metric proposed in [Carbonell and Goldstein, 1998]) and showed how it can be used in evalu