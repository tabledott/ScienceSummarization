the tuples on which the annotators agreed as the gold standard.
			We ran PMI++ and Hu++ on the test data and compared the results against OPINE?s results on the same data.
			In order to quantify the benefits of each of the threesteps of our method for finding SO labels, we also compared OPINE with a version which only finds SO la bels for words and a version which finds SO labels for words in the context of given features, but doesn?t take into account given sentences.
			We have learned from this comparison that OPINE?s precision gain over PMI++ andHu++ is mostly due to to its ability to handle context sensitive words in a large number of cases.
			Although Hu++ does not handle context-sensitive SO label assignment, its average precision was reasonable (75%) and better than that of PMI++.
			Finding a word?s SO label is good enough in the case of strongly positiveor negative opinion words, which account for the major ity of opinion instances.
			The method?s loss in recall is due to not recognizing words a