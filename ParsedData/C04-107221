en choose BLEUS1; however, if we would like to optimize for fluency then choose BLEUS4 or BLEUS5.
			According to Table 2, we know that BLEUS6 on average places reference translations at rank 235 in a 1024-best list machine translations that is significantly better than BLEUS1 and BLEUS2.
			Therefore, we have better chance of finding more human-like translations on the top of an n-best list by choosing BLEUS6 instead of BLEUS2.
			To design automatic metrics better than BLEUS6, we can carry out error analysis over the machine translations that are ranked higher than their references.
			Based on the results of error analysis, promising modifications can be identified.
			This indicates that the ORANGE evaluation method provides a natural automatic evaluation metric development cycle.
			Table 4 shows the ORANGE scores for ROUGE-L and ROUGE-W-1.1 to 2.0.
			ROUGE-W 1.1 does have better ORANGE score but it is equivalent to other ROUGE-W variants and ROUGE-L.
			Table 5 lists performance of different ROUGE-S va