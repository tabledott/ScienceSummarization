information, which we will refer to as specific mutual information S/(X, Y), is The quantity /(X, Y) is the average of SI(X, Y) taken over the four combinations of values of X and Y according to the joint probability distribution p(X, Y), so sometimes the term average mutual information is used for /(X, Y).
    Average mutual information expresses the difference between the entropy (information) of one of the variables and the conditional entropy of that variable given the other variable (Cover and Thomas 1991).
    Thus, average mutual information measures the reduction in the uncertainty about the value of one variable that knowledge of the value of the other variable provides, averaged over all possible values of the two variables.
    Equivalently, average mutual information is &amp;quot;the information about X contained in Y&amp;quot; (Papoulis 1984, 518) (or the information about Y contained in X).
    Specific mutual information represents the log-likelihood ratio of the joint probability of seeing a &