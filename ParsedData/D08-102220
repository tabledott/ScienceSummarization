ring more parses; here we introduce new rules never seen on any 1-best parses.
    We also conduct experiments on a larger training dataset, FBIS, which contains 239K sentence pairs with about 6.9M/8.9M words in Chinese/English, respectively.
    We also use a bigger trigram model trained on the first 1/3 of the Xinhua portion of Gigaword corpus.
    To integrate with forest-based decoding, we use both 1-best trees and packed forests during both rule extraction and decoding phases.
    Since the data scale is larger than the small data, we are forced to use harsher pruning thresholds, with pe = 5 for extraction and pd = 10 for decoding.
    The final BLEU score results are shown in Table 4.
    With both tree-based and forest-based decoding, rules extracted from forests significantly outperform those extracted from 1-best trees (p &lt; 0.01).
    The final result with both forest-based extraction and forest-based decoding reaches a BLEU score of 0.2816, outperforming that of Hiero (Chiang, 2005), one of the b