he head, then making the independence assumptions that the left and right modifiers are generated by separate 0th-order markov processes 4. .
    For example, the probability of the rule S (bought) -&gt; NP (week) NP (Marks) VP (bought) would be estimated as but in general the probabilities could be conditioned on any of the preceding modifiers.
    In fact, if the derivation order is fixed to be depth-first &#8212; that is, each modifier recursively generates the sub-tree below it before the next modifier is generated &#8212; then the model can also condition on any structure below the preceding modifiers.
    For the moment we exploit this by making the approximations where distancei and distance,. are functions of the surface string from the head word to the edge of the constituent (see figure 2).
    The distance measure is the same as in (Collins 96), a vector with the following 3 elements: (1) is the string of zero length?
    (Allowing the model to learn a preference for rightbranching structures); (2)