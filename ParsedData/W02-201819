ethods.
    More dramatically, both iterative scaling methods perform very poorly on the &#8216;shallow&#8217; dataset.
    In this case, the training data is very sparse.
    Many features are nearly &#8216;pseudo-minimal&#8217; in the sense of Johnson et al. (1999), and so receive weights approaching &#8722;&#8734;.
    Smoothing the reference probabilities would likely improve the results for all of the methods and reduce the observed differences.
    However, this does suggest that gradient-based methods are robust to certain problems with the training data.
    Finally, the most significant lesson to be drawn from these results is that, with the exception of steepest ascent, gradient-based methods outperform iterative scaling by a wide margin for almost all the datasets, as measured by both number of function evaluations and by the total elapsed time.
    And, in each case, the limited memory variable metric algorithm performs substantially better than any of the competing methods.
  
  
    In this pape