mposes it by assuming the fi&#8217;s are conditionally independent given Our training method consists of relative-frequency estimation of P(c) and P(fi  |c), using add-one smoothing.
    Despite its simplicity and the fact that its conditional independence assumption clearly does not hold in real-world situations, Naive Bayes-based text categorization still tends to perform surprisingly well (Lewis, 1998); indeed, Domingos and Pazzani (1997) show that Naive Bayes is optimal for certain problem classes with highly dependent features.
    On the other hand, more sophisticated algorithms might (and often do) yield better results; we examine two such algorithms next.
    Maximum entropy classification (MaxEnt, or ME, for short) is an alternative technique which has proven effective in a number of natural language processing applications (Berger et al., 1996).
    Nigam et al. (1999) show that it sometimes, but not always, outperforms Naive Bayes at standard text classification.
    Its estimate of P(c  |d) takes 