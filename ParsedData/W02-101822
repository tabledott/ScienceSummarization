e decoder described in (Marcu, 2001) starts from a gloss that uses the translations in the translation memory and then tries to improve on the gloss translation by modifying it incrementally, in the style described in Section 4.
    However, because the decoder hill-climbs on a word-forword translation model probability, it often discards good phrasal translations in favour of word-for-word translations of higher probability.
    The decoder in Section 4 does not have this problem because it hillclimbs on translation model probabilities in which phrases play a crucial role.
  
  
    Most of the noisy-channel-based models used in statistical machine translation (MT) (Brown et al., 1993) are conditional probability models.
    In the noisy-channel framework, each source sentence e in a parallel corpus is assumed to &#8220;generate&#8221; a target sentence f by means of a stochastic process, whose parameters are estimated using traditional EM techniques (Dempster et al., 1977).
    The generative model explains