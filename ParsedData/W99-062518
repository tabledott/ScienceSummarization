ns in both our model and the subsequent validation experiment receive the value &amp;quot;not similar&amp;quot;.
    This large percentage is due to our finegrained notion of similarity, and is parallel to happens in randomly sampled collections, since in that case most documents will not be relevant to any given query.
    Nevertheless, we can account for the high probability of inter-reviewer agreement expected by chance, 0.97.0.97+ (1 &#8212;0.97)- (1-0.97) --- 0.9418, by referring to the kappa statistic [Cohen 1960; Carletta 1996].
    The kappa statistic is defined as PA PO K &#8212; the probability that two reviewers agree in practice, and Po is the probability that they would agree solely by chance.
    In our case, 0.9418, and = indicating that the observed agreement by the is indeed If Po is estimated from the particular sample used in this experiment rather than from our entire corpus, it would be only 0.9, producing a value of 0.76 In addition to this validation experiment that used randomly sample