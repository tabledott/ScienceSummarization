 scoring algorithm for evaluating the cross-document coreference chains produced by our system and we compare our algorithm to the scoring algorithm used in the MUC-6 (within document) coreference task.
  
  
    Cross-document coreference is a distinct technology from Named Entity recognizers like IsoQuest's NetOwl and IBM's Textract because it attempts to determine whether name matches are actually the same individual (not all John Smiths are the same).
    Neither NetOwl or Textract have mechanisms which try to keep same-named individuals distinct if they are different people.
    Cross-document coreference also differs in substantial ways from within-document coreference.
    Within a document there is a certain amount of consistency which cannot be expected across documents.
    In addition, the problems encountered during within document coreference are compounded when looking for coreferences across documents because the underlying principles of linguistics and discourse context no longer apply across 