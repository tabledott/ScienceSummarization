r smoothing and feature selection techniques.
    Section 6 evaluates our model's generalization performance, accuracy on short passages, and sensitivity to the amount of training data.
    Sections 7 and 8 discuss the evaluation results and give our observations and conclusions.
  
  
    First, we define the following standard terms when referring to word frequencies in a corpus.
    A token is defined as any word occurrence in the collection.
    A type refers to a specific word-string, and is counted only once no matter how many times the word token of that type occurs in the collection.
    For training our model, we were aware of no significant collection of Web pages labeled by reading difficulty level, so we assembled our own corpus.
    There are numerous commercial reading comprehension tests available that have graded passages, but this would have reduced the emphasis we wanted on Web documents.
    Also, some commercial packages themselves use readcorpus of Web documents.
    Curves showing word f