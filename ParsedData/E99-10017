should not be marked up, and how the markup should proceed.
    They also received a few hundred articles from the New York Times Service, marked up by the organisers according to the rules of the coding manual.
    For the competition itself, participants received 100 articles.
    They then had 5 days to perform the chosen information extraction tasks (in our case: Named Entity recognition) without human intervention, and markup the text with the Named Entities found.
    The resulting marked up file then had to be returned to the organisers for scoring.
    Scoring of the results is done automatically by the organisers.
    The scoring software compares a participant's answer file against a carefully prepared key file; the key file is considered to be the &amp;quot;correctly&amp;quot; annotated file.
    Amongst many other things, the scoring software calculates a system's recall and precision scores: Recall: Number of correct tags in the answer file over total number of tags in the key file.
    Precision