0%.
    Many metrics failed to reach the baseline (including most metrics in the out-of-English direction).
    This indicates that sentence-level evaluation of machine translation quality is very difficult.
    RTE and ULC again do the best overall for the intoEnglish direction.
    They are followed closely by wpF and wcd6p4er, which considerably improve their performance over their system-level correlations.
    We tried a variant on measuring sentence-level consistency.
    Instead of using the scores assigned to each individual sentence, we used the systemlevel score and applied it to every sentence that was produced by that system.
    These can be thought of as a metric&#8217;s prior expectation about how a system should preform, based on their performance on the whole data set.
    Tables 12 and 13 show that using the system-level scores in place of the sentence-level scores results in considerably higher consistency with human judgments.
    This suggests an interesting line of research for improving