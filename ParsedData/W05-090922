re normalized by a group at the MITRE Corporation* To see the effect of this noise on the correlation, we computed the correlation between the METEOR Score (computed using the stages used in the 4th experiment in section 7 above) and both the raw human assessments as well as the normalized human assessments* Table 8 shows that indeed METEOR Scores correlate better with normalized human assessments* In other words, the noise in the human assessments hurts the correlations between automatic scores and human assessments*
  
  
    The METEOR metric we described and evaluated in this paper, while already demonstrating great promise, is still relatively simple and naive* We are in the process of enhancing the metric and our experimentation in several directions: Train the Penalty and Score Formulas on Data: The formulas for Penalty and METEOR score were manually crafted based on empirical tests on a separate set of development data* However, we plan to optimize the formulas by training them on a separate data set,