 tokenizer (a conservative approximation).
    The analogous results, using the classifier from Section 5.2, are shown for comparison.
    To summarize the results, using the content-based similarity score as a feature not only improved precision, it increased the size of the corpus (in words) by 51&#8211;63%, depending on the tokenization scheme.29
  
  
    A number of the techniques we have used to mine parallel data from the Web can be improved, and we suggest here some directions.
    28 There were 1,796 unique English URLs and 1,779 unique Arabic URLs, giving document duplication rates of 1.4% and 2.4%, respectively.
    29 A list of Wayback Machine URLs is available at (http://umiacs.umd.edu/&#8764;resnik/strand/); a sample of the document pairs is included in Appendix A.
    With respect to classifying document pairs as translations, the reader will notice that our approach to content-based cross-lingual similarity essentially boils down to a greedy matching of some of the words in a document pair usi