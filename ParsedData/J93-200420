tics, participated in the exper- iment.
  All completed a training sequence consisting of 15 hours of correcting followed by 6 hours of tagging.
  The training material was selected from a variety of nonfiction genres in the Brown Corpus.
  All the annotators were familiar with GNU Emacs at the outset of the experiment.
  Eight 2,000-word samples were selected from the Brown Cor- pus, two each from four different genres (two fiction, two nonfiction), none of which any of the annotators had encountered in training.
  The texts for the correction task were automatically tagged as described in Section 2.3.
  Each annotator first manually tagged four texts and then corrected four automatically tagged texts.
  Each annotator completed the four genres in a different permutation.
  A repeated measures analysis of annotation speed with annotator identity, genre, and annotation mode (tagging vs. correcting) as classification variables howed a sig- nificant annotation mode effect (p = .05).
  No other effects or intera