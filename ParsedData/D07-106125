ture.
			One is Jensen-Shannon divergence (Lin, 1991), a symmetric measure based onKL-divergence defined as the average of the KL diver gences of each distribution to their average distribution.Jensen-Shannon is well defined for all distributions be cause the average of pi and qi is non-zero whenever either number is. These measures and others are surveyed in (Lee, 2001), who finds that Jensen-Shannon is outperformed by the Skew divergence measure introduced by Lee in (1999).
			The skew divergence2 accounts for zeros in q by mixing in a small amount of p. s?(p, q) = D(p ? ?q + (1?
			?)p) = ? i pi log pi ?qi+(1??)piLee found that as ? ?
			1, the performance of skew divergence on natural language tasks improves.
			In partic ular, it outperforms most other models and even beatspure KL divergence modified to avoid zeros with sophis ticated smoothing models.
			In exploring the performanceof divergence measures on our model?s stationary distri butions, we observed the same phenomenon.
			Note thatin the limit 