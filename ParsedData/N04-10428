The three methods below all provide ways to alter this rate by changing the variance of the Gaussian prior dependent on feature counts. ckx&#963;2 where &#963; is a constant over all features.
    In this way, we increase the smoothing on the low frequency features more so than the high frequency features. &#955;k fck/ x&#963;2 where ck is the count of features, N is the bin size, and ra] is the ceiling function.
    Alternatively, the variance in each bin may be set independently by cross-validation.
    Whereas the Gaussian prior penalizes according to the square of the weights (an L2 penalizer), the intention here is to create a smoothly differentiable analogue to penalizing the absolute-value of the weights (an L1 penalizer).
    L1 penalizers often result in more &#8220;sparse solutions,&#8221; in which many features have weight nearly at zero, and thus provide a kind of soft feature selection that improves generalization.
    Goodman (2003) proposes an exponential prior, specifically a Laplacian prior, 