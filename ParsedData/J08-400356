the classifiers were generated by parsing each sentence in the training set using the goldstandard dependency graph as an oracle.
    For each transition t(c) in the oracle parse, a training instance (&#934;(c), t) was created, where &#934;(c) is a feature vector representation of the parser configuration c. Because the purpose of the experiments was not to optimize parsing accuracy as such, no work was done on feature selection for the different algorithms and languages.
    Instead, all parsers use a variant of the simple feature model used for parsing English and Swedish in Nivre (2006b), with minor modifications to suit the different algorithms.
    Table 2 shows the feature sets used for different parsing algorithms.7 Each row represents a node defined relative to the current parser configuration, where nodes defined relative to the stack &#963; are only relevant for stack-based algorithms, whereas nodes defined relative to the lists A1 and A2 are only relevant for list-based algorithms.
    We use the n