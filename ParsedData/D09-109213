 the number of topics increases, greater variability in topic distributions causes divergence to increase.
    Smoothed histograms of inter&#8722;language JS divergence A topic model specifies a probability distribution over documents, or in the case of PLTM, document tuples.
    Given a set of training document tuples, PLTM can be used to obtain posterior estimates of &#934;', ... , &#934;L and &#945;m.
    The probability of previously unseen held-out document tuples given these estimates can then be computed.
    The higher the probability of the held-out document tuples, the better the generalization ability of the model.
    Analytically calculating the probability of a set of held-out document tuples given &#934;1, ... , &#934;L and &#945;m is intractable, due to the summation over an exponential number of topic assignments for these held-out documents.
    However, recently developed methods provide efficient, accurate estimates of this probability.
    We use the &#8220;left-to-right&#8221; method of 