 will converge to a local optimum in the sense that the initial probabilities will be modified to increase the likelihood of the corpus given the grammar and 'stabilize' within some threshold after a number of iterations over the training corpus.
    However, there is no guarantee that the global optimum will be found, and the a priori initial probabilities chosen are critical for convergence on useful probabilities (e.g.
    Lan i and Young 1990).
    The main application of these techniques to written input has been in the robust, lexical tagging of corpora with part-of-speech labels (e.g.
    Garside, Leech, and Sampson 1987; de Rose 1988; Meteer, Schwartz, and Weischedel 1991; Cutting et al. 1992).
    Fujisaki et al. (1989) describe a corpus analysis experiment using a probabilistic CNF CFG containing 7550 rules on a corpus of 4206 sentences (with an average sentence length of approximately 11 words).
    The unsupervised training process involved automatically assigning probabilities to each CF rule on 