by one (Figure 4).
    We first note that the amount of data we obtained from MTurk is so large, that we could afford to eliminate close to 30% of the labels, and we would still have twice as much data than using the expert data alone.
    We also note that two workers in particular (the 103rd and 130th to be removed) are likely responsible for the majority of the bad data, since removing their data leads to noticeable jumps in the reference preference rate and the inter-annotator agreement rate (right two curves of Figure 4).
    Indeed, examining the data for those two workers, we find that their RPR values are 55.7% and 51.9%, which is a clear indication of random clicking.15 Looking again at those two curves shows degrading values as we continue to remove workers in large droves, indicating a form of &#8220;overfitting&#8221; to agreement with experts (which, naturally, continues to increase until reaching 1.0; bottom left curve).
    It is therefore important, if one were to filter out the MTurk data by 