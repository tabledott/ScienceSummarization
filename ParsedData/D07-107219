ained.
			timates.
			Because of this procedural similarity, our method is able to exploit the desirable properties of EM such as simplicity, modularity, and efficiency.
			2.7 Structured mean-field approximation.
			We denote parameters of the HDP-PCFG as ? =(?,?), where ? denotes the top-level symbol prob abilities and ? denotes the rule probabilities.
			The hidden variables of the model are the training parse trees z. We denote the observed sentences as x. The goal of Bayesian inference is to compute the posterior distribution p(?, z | x).
			The central idea behind variational inference is to approximate this intractable posterior with a tractable approximation.
			In particular, we want to find the best distribution q?
			as defined by q?
			def = argmin q?Q KL(q(?, z)||p(?, z | x)), (4) where Q is a tractable subset of distributions.
			We use a structured mean-field approximation, meaning that we only consider distributions that factorize as follows (Figure 4): Q def = { q(z)q(?)
			K? z=1 q(?Tz )q(?
