
  Coreference Resolution in a Modular Entity-Centered Model
  
    Coreference resolution is governed by syntactic, semantic, and discourse constraints.
    We present a generative, model-based approach in which each of these factors is modularly encapsulated and learned in a primarily unsupervised manner.
    Our semantic representation first hypothesizes an underlying set of latent which generate specific entities that in turn render individual mentions.
    By sharing lexical statistics at the level of abstract entity types, our model is able to substantially reduce semantic compatibility errors, resulting in the best results to date on the complete end-to-end coreference task.
  
  
    Coreference systems exploit a variety of information sources, ranging from syntactic and discourse constraints, which are highly configurational, to semantic constraints, which are highly contingent on lexical meaning and world knowledge.
    Perhaps because configurational features are inherently easier to learn from sma