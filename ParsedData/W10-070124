s differs on the different ends of the political spectrum.
			6.4 Information Retrieval.
			Large scale evaluations requiring significant humanlabor for evaluation have a long history in the in formation retrieval community (TREC).
			Grady and Lease (2010) study four factors that influence Turker performance on a document relevance search task.
			The authors present some negative results on how these factors influence data collection.
			For further work on MTurk and information retrieval, readers are encouraged to see the SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation.8 8http://www.ischool.utexas.edu/?cse2010/ call.htm 6 6.5 Information Extraction.
			Information extraction (IE) seeks to identify specific types of information in natural languages.
			The IE papers in the shared tasks focused on new domains and genres as well as new relation types.The goal of relation extraction is to identify rela tions between entities or terms in a sentence, such asborn in or religion.
			Gormley et al (2010)