notonically with training iterations.11 Figure 4 shows a graph of training iterations versus accuracy for the second pair of models on the development set.
  
  
    We have shown how broad feature use, when combined with appropriate model regularization, produces a superior level of tagger performance.
    While experience suggests that the final accuracy number presented here could be slightly improved upon by classifier combination, it is worth noting that not only is this tagger better than any previous single tagger, but it also appears to outperform Brill and Wu (1998), the best-known combination tagger (they report an accuracy of 97.16% over the same WSJ data, but using a larger training set, which should favor them).
    While part-of-speech tagging is now a fairly well-worn road, and our ability to win performance increases in this domain is starting to be limited by the rate of errors and inconsistencies in the Penn Treebank training data, this work also has broader implications.
    Across the many