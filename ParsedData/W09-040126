parison, metrics performed worse for Hungarian-English, where half of the systems had negative correlation.
    The ULC metric once again had strongest correlation with human judgments of translation quality.
    This was followed closely by MaxSim and RTE, with Meteor and TERp doing respectably well in 4th and 5th place.
    Notably, Bleu and its variants were the worst performing metrics in this translation direction.
    Table 8 shows correlation for metrics which operated on languages other than English.
    Most of the best performing metrics that operate on English do not work for foreign languages, because they perform some linguistic analysis or rely on a resource like WordNet.
    For translation into foreign languages TERp was the best system overall.
    The wpBleu and wpF metrics also did extremely well, performing the best in the language pairs that they were applied to. wpBleu and wpF were not applied to Czech because the authors of the metric did not have a Czech tagger.
    English-German prov