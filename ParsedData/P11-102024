t as well as the correlation between BLEU, PINC, PEM (Liu et al., 2010) and the average human ratings on the sentence level.
    Results are shown in Table 4.
    In order to measure correlation, we need to score each paraphrase individually.
    Thus, we recomputed BLEU on the sentence level and left the PINC scores unchanged.
    While BLEU is typically not reliable at the single sentence level, our large number of reference sentences makes BLEU more stable even at this granularity.
    Empirically, BLEU correlates fairly well with human judgments of semantic equivalence, although still not as well as the inter-annotator agreement.
    On the other hand, PINC correlates as well as humans agree with each other in assessing lexical dissimilarity.
    We also computed each metric&#8217;s correlation with the overall ratings, although neither should be used alone to assess the overall quality of paraphrases.
    PEM had the worst correlation with human judgments of all the metrics.
    Since PEM was trained on 