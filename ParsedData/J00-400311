nnotators over and above chance agreement (Siegel and Castellan 1988).
    The K coefficient of agreement is defined as: where P(A) is the proportion of times the annotators agree, and P(E) the proportion of times that we would expect them to agree by chance.
    The interpretation of K figures is art open question, but in the field of content analysis, where reliability has long been an issue (Krippendorff 1980), K &gt; 0.8 is generally taken to indicate good reliability, whereas 0.68 &lt;K &lt; 0.8 allows tentative conclusions to be drawn.
    Carletta et al. (1997) observe, however, that in other areas, such as medical research, much lower levels of K are considered acceptable (Landis and Koch 1977).
    An interesting overall result of our study was that the most reliable distinction that our annotators could make was that between first-mention and subsequent-mention (K = 0.76); the measure of agreement for the three-way distinction just discussed was K = 0.73.
    The second interesting result concerned 