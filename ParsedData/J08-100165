otely over the Internet.
    Participants first saw a set of instructions that explained the task, and defined the notion of coherence using multiple examples.
    The summaries were randomized in lists following a Latin square design ensuring that no two summaries in a given list were generated from the same document cluster.
    Participants were asked to use a sevenpoint-scale to rate how coherent the summaries were without having seen the source texts.
    The ratings (approximately 23 per summary) given by our subjects were averaged to provide a rating between 1 and 7 for each summary.
    The reliability of the collected judgments is crucial for our analysis; we therefore performed several tests to validate the quality of the annotations.
    First, we measured how well humans agree in their coherence assessment.
    We employed leave-one-out resampling10 (Weiss and Kulikowski 1991), by correlating the data obtained from each participant with the mean coherence ratings obtained from all other participan