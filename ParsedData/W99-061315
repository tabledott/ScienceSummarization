, each element x E X can also be represented as (xi, x2) E X1 x X2.
    Thus the method makes the fairly strong assumption that the features can be partitioned into two types such that each type alone is sufficient for classification.
    Now assume we have n pairs (xi,, x2,i) drawn from X1 X X2, where the first m pairs have labels whereas for i = m+ 1...n the pairs are unlabeled.
    In a fully supervised setting, the task is to learn a function f such that for all i = 1...m, f (xi,i, 12,i) = yz.
    In the cotraining case, (Blum and Mitchell 98) argue that the task should be to induce functions Ii and f2 such that So Ii and 12 must (1) correctly classify the labeled examples, and (2) must agree with each other on the unlabeled examples.
    The key point is that the second constraint can be remarkably powerful in reducing the complexity of the learning problem.
    (Blum and Mitchell 98) give an example that illustrates just how powerful the second constraint can be.
    Consider the case where IX].
    I =