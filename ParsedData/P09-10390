
  Concise Integer Linear Programming Formulations for Dependency Parsing
  
    We formulate the problem of nonprojective dependency parsing as a polynomial-sized integer linear program.
    Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data.
    In particular, our model is able to learn correlations among neighboring arcs (siblings and grandparents), word valency, and tendencies toward nearlyprojective parses.
    The model parameters are learned in a max-margin framework by employing a linear programming relaxation.
    We evaluate the performance of our parser on data in several natural languages, achieving improvements over existing state-of-the-art methods.
  
  
    Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yi