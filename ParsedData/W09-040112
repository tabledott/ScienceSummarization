re ranked based on how frequently they were judged to be better than or equal to any other system.
    The results of this are reported in Section 4.
    Appendix A provides detailed tables that contain pairwise comparisons between systems.
    We experimented with a new type of evaluation this year where we asked judges to edit the output of MT systems.
    We did not show judges the reference translation, which makes our edit-based evaluation different than the Human-targeted Translation Error Rate (HTER) measure used in the DARPA GALE program (NIST, 2008).
    Rather than asking people to make the minimum number of changes to the MT output in order capture the same meaning as the reference, we asked them to edit the translation to be as fluent as possible without seeing the reference.
    Our hope was that this would reflect people&#8217;s understanding of the output.
    The instructions that we gave our judges were the following: Each translated sentence was shown in isolation without any additional cont