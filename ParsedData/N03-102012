owed promising results.
    However, BLEU is a precision-based metric while the human evaluation protocol in DUC is essentially recall-based.
    We therefore prefer the metric given by equation 6 and use it in all our experiments.
    Using DUC 2001 data, we compute average Ngram(1,4) scores for each peer system at different summary sizes and rank systems according to their scores.
    We then compare the Ngram(1,4) ranking with the human ranking.
    Figure 2 shows the result of DUC 2001 multi-document data.
    Stopwords are ignored during the computation of Ngram(1,4) scores and words are stemmed using a Porter stemmer (Porter 1980).
    The x-axis is the human ranking and the y-axis gives the corresponding Ngram(1,4) rankings for summaries of difference sizes.
    The straight line marked by AvgC is the ranking given by human assessment.
    For example, a system at (5,8) means that human ranks its performance at the 5th rank while Ngram(1,4)400 ranks it at the 8th.
    If an automatic ranking fully matc