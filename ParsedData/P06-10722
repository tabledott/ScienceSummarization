 for short dependencies, then relax the preference.
    The new approach, structural annealing, often gives superior performance.
    An alternative structural bias is explored in &#167;5.
    This approach views a sentence as a sequence of one or more yields of separate, independent trees.
    The points of segmentation are a hidden variable, and during learning all possible segmentations are entertained probabilistically.
    This allows the learner to accept hypotheses that explain the sentences as independent pieces.
    In &#167;6 we briefly review contrastive estimation (Smith and Eisner, 2005a), relating it to the new method, and show its performance alone and when augmented with structural bias.
  
  
    In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004).
    The model is a probabilistic head automaton grammar (Alshawi, 1996) with a &#8220;split&#8221; form that renders it parseable in cubic time (Eisner, 1997).
    Let x = (x1, x2, ..., xn) be the sentence. 