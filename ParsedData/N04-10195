itten summaries can score as low as 0.1 while machine summaries can score as high as 0.5.
    For each of the 30 test sets, three of the four humanwritten summaries and the machine summaries were scored against the fourth human model summary: each human was scored on ten summaries.
    Figure 1 shows a scatterplot of human scores for all 30 sets, and illustrates an apparently random relation of summarizers to each other, and to document sets.
    This suggests that the DUC scores cannot be used to distinguish a good human summarizer from a bad one.
    In addition, the DUC method is not powerful enough to distinguish between systems.
  
  
    Our analysis of summary content is based on Summarization Content Units, or SCUs and we will now proceed to define the concept.
    SCUs emerge from annotation of a corpus of summaries and are not bigger than a clause.
    Rather than attempting to provide a semantic or functional characterisation of what an SCU is, our annotation procedure defines how to compare summar