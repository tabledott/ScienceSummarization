times faster inside-outside algorithm.
    We show that when used to learn phrase tables for the Pharaoh decoder, the phrasal ITG is superior to the constrained joint phrasal model, producing tables that result in a 2.5 point improvement in BLEU when used alone, and a 1 point improvement when used with IBM Model 1 features.
    This suggests that ITG&#8217;s perfect expectation counting does matter; other phrasal models could benefit from either adopting the ITG formalism, or improving their sampling heuristics.
    We have explored, for the first time, the utility of a joint phrasal model as a word alignment method.
    We present a non-compositional constraint that turns the phrasal ITG into a high-recall phrasal aligner with an F-measure that is comparable to GIZA++.
    With search and sampling no longer a concern, the remaining weaknesses of the system seem to lie with the model itself.
    Phrases are just too efficient probabilistically: were we to remove all lexicon constraints, EM would always align 