size of search space is huge, the pruning criterion proposed in this paper effectively prunes the search space.
    The pruning conditions in Fig.4 are fulfilled with more than 90% probabitity.
    The training speed of our method is 1,384 sec./5,741 instances when we set K = 60, 000 (# of iterations for Boosting).
    It takes only 0.023 (=1,384/60,000) sec. to invoke the weak learner, Find Optimal Rule.
  
  
    In this paper, we focused on an algorithm for the classification of semi-structured text in which a sentence is represented as a labeled ordered tree7.
    Our proposal consists of i) decision stumps that use subtrees as features and ii) Boosting algorithm in which the subtree-based decision stumps are applied as weak learners.
    Two experiments on opinion/modality classification tasks confirmed that subtree features are important.
    One natural extension is to adopt confidence rated predictions to the subtree-based weak learners.
    This extension is also found in BoosTexter and shows better 