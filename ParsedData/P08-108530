 tagging task.
    In Hebrew, our model includes an improved version of the similar words algorithm of (Levinger et al., 1995), a model of lexical context, and a small set of tag ngrams.
    The combination of these knowledge sources in the initial conditions brings an error reduction of more than 25% over a strong uniform distribution baseline.
    In English, our model is competitive with recent state-of-the-art results, while using simple and efficient learning methods.
    The comparison with other algorithms indicates directions of potential improvement: (1) our initialconditions method might benefit the other, more sophisticated learning algorithms as well.
    (2) Our models were designed under the assumption of a relatively complete dictionary.
    As such, they are not very good at assigning ambiguity-classes to OOV tokens when starting with a very small dictionary.
    While we demonstrate competitive results using a simple suffix-based ambiguity-class guesser which ignores capitalization and hyphen