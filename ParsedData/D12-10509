word to the probability of the context word overall.
    More formally, let us consider the BNC as a set of sentences: ni ) from the BNC&#8217;s vocabulary VocBNC.
    Then f reqw is the amount of times that each word w &#8712; VocBNC appears in the BNC.
    Mitchell and Lapata (2010) collect the M most frequent non-stoplist words in the set ctxttop = {w(top)1, , wM(top) } and let them consitute the word vectors&#8217; dimensions.
    Each dimension&#8217;s value is obtained from a co-occurrence count: for w &#8712; VocBNC and j = 1,...,M. Using these counts, they define word vectors component-wise. for j = 1,...,M, where totalCount is the total number of words in the BNC.
    This space is relatively simple, it has few parameters, requires no preprocessing other than tokenization and involves no syntactic information or parameter learning.
    Despite its simplicity, it is a good starting point for studying representations for compositional models as a baseline against which to evaluate more elaborate models