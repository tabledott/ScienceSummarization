was first introduced in (Freund and Schapire 97); (Schapire and Singer 98) gave a generalization of AdaBoost which we will use in this paper.
    For a description of the application of AdaBoost to various NLP problems see the paper by Abney, Schapire, and Singer in this volume.
    The input to AdaBoost is a set of training examples ((xi , yi), , (x&#8222;.&#8222; yrn)).
    Each xt E 2x is the set of features constituting the ith example.
    For the moment we will assume that there are only two possible labels: each y, is in { &#8212;1, +1}.
    AdaBoost is given access to a weak learning algorithm, which accepts as input the training examples, along with a distribution over the instances.
    The distribution specifies the relative weight, or importance, of each example &#8212; typically, the weak learner will attempt to minimize the weighted error on the training set, where the distribution specifies the weights.
    The weak learner for two-class problems computes a weak hypothesis h from the input spac