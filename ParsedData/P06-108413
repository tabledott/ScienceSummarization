matrix (which models the emission probabilities from two tags to a word) is about 5M.
    Despite the sparseness of these matrices, the number of their entries is still high, since we model the whole set of features of the complex word forms.
    Let us assume, that the right segmentation for the sentence is provided to us &#8211; for example: b clm hn`im &#8211; as is the case for English text.
    In such a way, the observation is composed of morphemes, generated by a Markov process, based on a morpheme-based tagset.
    The size of such a tagset for Hebrew is about 200, where the size of the II,A,B,A2 and B2 matrices is reduced to 145, 16K, 140K, 700K, and 1.7M correspondingly, as described in line M of Table 2 &#8211; a reduction of 90% when compared with the size of a word-based model.
    The problem in this approach, is that &#8221;someone&#8221; along the way, agglutinates the morphemes of each word leaving the observed morphemes uncertain.
    For example, the word bclm can be segmented in four diffe