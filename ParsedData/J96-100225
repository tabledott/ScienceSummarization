le, yet only include features whose expected values can be reliably estimated.
    A nested sequence of subsets C(Si) D C(S2) D C(S3) ... of P corresponding to increasingly large sets of features Si C 52 C $3.
    To find S, we adopt an incremental approach to feature selection, similar to the strategy used for growing decision trees (Bahl et al. 1989).
    The idea is to build up S by successively adding features.
    The choice of feature to add at each step is determined by the training data.
    Let us denote the set of models determined by the feature set S as C(S).
    &amp;quot;Adding&amp;quot; a feature f is shorthand for requiring that the set of allowable models all satisfy the equality f9(f) = p(f).
    Only some members of C(S) will satisfy this equality; the ones that do we denote by C(S U f).
    Thus, each time a candidate feature is added to S, another linear constraint is imposed on the space C(S) of models allowed by the features in S. As a result, C(S) shrinks; the model 13* in C with the g