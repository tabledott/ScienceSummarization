
  Two Languages are Better than One (for Syntactic Parsing)
  
    We show that jointly parsing a bitext can substantially improve parse quality on both sides.
    In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them.
    Features include monolingual parse scores and various measures of syntactic divergence.
    Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables.
    The resulting bitext parser outperforms state-of-the-art monoparser baselines by 2.5 predicting side trees and 1.8 predicting Chinese side trees (the highest published numbers on these corpora).
    Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.
  
  
    Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; C