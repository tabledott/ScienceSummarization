t the widely-used SVM classifier (Joachims, 2002).
    Viewing input data as two sets of vectors in a feature space, SVM constructs a separating hyperplane in the space by maximizing the margin between the two data sets.
    The English or Chinese features used in this study include both unigrams and bigrams5 and the feature weight is simply set to term frequency6.
    Feature selection methods (e.g.
    Document Frequency (DF), Information Gain (IG), and Mutual Information (MI)) can be used for dimension reduction.
    But we use all the features in the experiments for comparative analysis, because there is no significant performance improvement after applying the feature selection techniques in our empirical study.
    The output value of the SVM classifier for a review indicates the confidence level of the review&#8217;s classification.
    Usually, the sentiment polarity of a review is indicated by the sign of the prediction value.
    Given: In the training phase, the co-training algorithm learns two sep