ery of Latent Dirichlet Allocation (LDA) (Blei et al., 2003), a recent model that has gained popularity among theoreticians and practitioners alike as a tool for automatic corpus summarization and visualization.
    LDA is a completely unsupervised algorithm that models each document as a mixture of topics.
    The model generates automatic summaries of topics in terms of a discrete probability distribution over words for each topic, and further infers per-document discrete distributions over topics.
    Most importantly, LDA makes the explicit assumption that each word is generated from one underlying topic.
    Although LDA is expressive enough to model multiple topics per document, it is not appropriate for multi-labeled corpora because, as an unsupervised model, it offers no obvious way of incorporating a supervised label set into its learning procedure.
    In particular, LDA often learns some topics that are hard to interpret, and the model provides no tools for tuning the generated topics to suit an en