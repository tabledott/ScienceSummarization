roximation is particularly important, as it leads to the efficient algorithm in Figure 4, which avoids a pass over the training set at each iteration of feature selection (note that in sparse feature spaces, f rounds of feature selection in our approach can take considerably fewer than f passes over the training set, in contrast to other work on feature selection within log-linear models).
    Note that there are other important differences among the approaches.
    Both Della Pietra, Della Pietra, and Lafferty (1997) and McCallum (2003) describe methods that induce conjunctions of &#8220;base&#8221;features, in a way similar to decision tree learners.
    Thus a relatively small number of base features can lead to a very large number of possible conjoined features.
    In future work it might be interesting to consider these kinds of approaches for the parsing problem.
    Another difference is that both McCallum, and Riezler and Vasserman, describe approaches that use a regularizer in addition to feature se