learning algorithm is equivalent to a hyperplane in this feature space.
    Overfitting is avoided in the SVM formulation by requiring that positive and negative training instances be maximally separated by the decision hyperplane.
    Gartner et al. (2002) adapted SVMs to the MIL setting using various multi-instance kernels.
    Two of these &#8211; the normalized set kernel, and the statistic kernel &#8211; have been experimentally compared to other methods by Ray and Craven (2005), with competitive results.
    Alternatively, a simple approach to MIL is to transform it into a standard supervised learning problem by labeling all instances from positive bags as positive.
    An interesting outcome of the study conducted by Ray and Craven (2005) was that, despite the class noise in the resulting positive examples, such a simple approach often obtains competitive results when compared against other more sophisticated MIL methods.
    We believe that an MIL method based on multiinstance kernels is not appropria