 of naive Bayes: the observed variables are all conditionally independent of one another, given the value of the latent variable.
    The latent variable represents the true state of the object, and is the source of the correlations among the observed variables.
    As applied here, the observed variables are the classifications assigned by the judges.
    Let B, D, J, and M be these variables, and let L be the latent variable.
    Then, the latent class model is: (by definition) The parameters of the model are {p(b, 1) , p(d, 1), p(j , 1) , p(m, 1)p(1)} .
    Once estimates of these parameters are obtained, each clause can be assigned the most probable latent category given the tags assigned by the judges.
    The EM algorithm takes as input the number of latent categories hypothesized, i.e., the number of values of L, and produces estimates of the parameters.
    For a description of this process, see Goodman (1974), Dawid &amp; Skene (1979), or Pedersen &amp; Bruce (1998).
    Three versions of the latent 