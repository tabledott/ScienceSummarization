 containing classifier phrases like --- ^ or --- %, which are rendered as English indefinite determiners.
    The right-hand three columns in Table 2 present supervised results on our Chinese English data set using block features.
    We note that almost all of our performance gains (relative to both the HMM and 1-1 matchings) come from BITG and block features.
    The maximum likelihood-trained normal form ITG model outperforms the HMM, even without including any features derived from the unlabeled data.
    Once we include the posteriors of the HMM as a feature, the AER decreases to 14.4.
    The previous best AER result on this data set is 15.9 from Ayan and Dorr (2006), who trained stacked neural networks based on GIZA++ alignments.
    Our results are not directly comparable (they used more labeled data, but did not have the HMM posteriors as an input feature).
    We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline Jo