wed as an instance of Lagrangian relaxation; we describe this connection, and give convergence guarantees for the method.
    We describe a generalization to models that include grandparent dependencies.
    We then introduce a perceptron-driven training algorithm that makes use of point 1 above.
    We describe experiments on non-projective parsing for a number of languages, and in particular compare the dual decomposition algorithm to approaches based on general-purpose linear programming (LP) or integer linear programming (ILP) solvers (Martins et al., 2009).
    The accuracy of our models is higher than previous work on a broad range of datasets.
    The method gives exact solutions to the decoding problem, together with a certificate of optimality, on over 98% of test examples for many of the test languages, with parsing times ranging between 0.021 seconds/sentence for the most simple languages/models, to 0.295 seconds/sentence for the most complex settings.
    The method compares favorably to previous 