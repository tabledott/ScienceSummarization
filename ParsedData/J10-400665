e are principled ways to select the optimal space configuration for a given semantic task.
    In this article, we limit ourselves to proving that each space derived through tensor matricization is semantically interesting in the sense that it provides the proper ground to address some semantic task.
    Feature selection/reweighting and dimensionality reduction have been shown to improve DSM performance.
    For instance, the feature bootstrapping method proposed by Zhitomirsky-Geffet and Dagan (2009) boosts the precision of a DSM in lexical entailment recognition.
    Even if these methods can be applied to DM as well, we did not use them in our experiments.
    The results presented subsequently should be regarded as a &#8220;baseline&#8221; performance that could be enhanced in future work by exploring various task-specific parameters (we will come back in the conclusion to the role of parameter tuning in DM).
    This is consistent with our current aim of focusing on the generality and adaptivity of DM, 