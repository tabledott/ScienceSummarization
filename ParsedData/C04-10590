
	Language Model Adaptation For Statistical Machine Translation Via Structured Query Models
		We explore unsupervised language model adaptation techniques for Statistical Machine Translation.
		The hypotheses from the machine translation output are converted into queries at different levels of representation power and used to extract similar sentences from very large monolingual text collection.
		Specific language models are then build from the retrieved data and interpolated with a general background model.
		Experiments show significant improvements when translating with these adapted language models.
	
	
			Language models (LM) are applied in many natural language processing applications, such as speech recognition and machine translation, to encapsulate syntactic, semantic and pragmatic information.
			For systems which learn from given data we frequently observe a severe drop in performance when moving to a new genre or new domain.
			In speech recognition a number of adaptation techniques have been dev