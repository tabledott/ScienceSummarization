arget word, and collocations containing words and/or parts of speech.
    The memory-based learner used only the word before and word after as features.
    We collected a 1-billion-word training corpus from a variety of English texts, including news articles, scientific abstracts, government transcripts, literature and other varied forms of prose.
    This training corpus is three orders of magnitude greater than the largest training corpus previously used for this problem.
    We used 1 million words of Wall Street Journal text as our test set, and no data from the Wall Street Journal was used when constructing the training corpus.
    Each learner was trained at several cutoff points in the training corpus, i.e. the first one million words, the first five million words, and so on, until all one billion words were used for training.
    In order to avoid training biases that may result from merely concatenating the different data sources to form a larger training corpus, we constructed each consecutive trai