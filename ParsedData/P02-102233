r annotations which have a compatible (or partially compatible) annotation in the response set, and orange for annotations which are missing in the response set.
    Annotations in the response set have three possible colours: green if they are compatible with the key annotation, blue if they are partially compatible, and red if they are spurious.
    In the viewer, two annotations will be positioned on the same row if they are co-extensive, and on different rows if not.
    GATE's benchmarking tool differs from the AnnotationDiff in that it enables evaluation to be carried out over a whole corpus rather than a single document.
    It also enables tracking of the system's performance over time.
    The tool requires a clean version of a corpus (with no annotations) and an annotated reference corpus.
    First of all, the tool is run in generation mode to produce a set of texts annotated by the system.
    These texts are stored for future use.
    The tool can then be run in three ways: In each case, performa