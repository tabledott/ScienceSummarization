onary constitutes a major factor in the performance of the various approaches, possibly more important than the particular set of methods used in the segmentation.
    Furthermore, even the size of the dictionary per se is less important than the appropriateness of the lexicon to a particular test corpus: as Fung and Wu (1994) have shown, one can obtain substantially better segmentation by tailoring the lexicon to the corpus to be segmented.
  
  
    Chinese word segmentation can be viewed as a stochastic transduction problem.
    More formally, we start by representing the dictionary D as a Weighted Finite State Transducer (WFST) (Pereira, Riley, and Sproat 1994).
    Let H be the set of hanzi, p be the set of pinyin syllables with tone marks, and P be the set of grammatical part-of-speech labels.
    Then each arc of D maps either from an element of H to an element of p, or from c&#8212;i.e., the empty string&#8212;to an element of P. More specifically, each word is represented in the dictionary as a seque