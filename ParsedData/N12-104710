ple to optimize and fits nicely into our mathematical framework.
    Variants that use the expected sufficient statistics of BLEU also exist (Smith and Eisner, 2006; Pauls et al., 2009; Rosti et al., 2011).
    We again assume a MERT-like tuning architecture.
    Let Ai(e) = &#8722;BLEUi(e) and let This expected cost becomes increasingly small as greater probability mass is placed on derivations with high BLEU scores.
    This smooth, non-convex objective can be solved to a local minimum using gradient-based optimizers; we have found stochastic gradient descent to be quite effective (Bottou, 2010).
    Like PRO, MR requires no oracle derivation, and fits nicely into the established MERT architecture.
    The expectations needed to calculate the gradient 2Hopkins and May (2011) advocate a maximum-entropy version of PRO, which is what we evaluate in our empirical comparison.
    It can be obtained using a logit loss fi(97) _ are trivial to extract from a k-best list of derivations.
    Each downward step along 