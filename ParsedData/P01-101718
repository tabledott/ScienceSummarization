 defined above, then j is the grandparent head of c, else Case 1 now covers both &#8220;united states of america&#8221; and &#8220;monday night football&#8221; examples.
    Case 2 handles other flat constituents in Penn tree-bank style (e.g., quantifier-phrases) for which we do not have a good analysis.
    Case three says that this feature is a no-op in all other situations.
    The results for this model, again trained on F0F20 and tested on F23-24, are given in Figure 3 under the heading &#8221;Immediate-trihead model&#8221;.
    We see that the grammar perplexity is reduced to 130.20, a reduction of 10% over our first model, 14% over the previous best grammar model (152.26%), and 22% over the best of the above trigram models for the task (167.02).
    When we run the trigram and new grammar model in tandem we get a perplexity of 126.07, a reduction of 8% over the best previous tandem model and 24% over the best trigram model.
    One interesting fact about the immediate-trihead model is that of the 3761 