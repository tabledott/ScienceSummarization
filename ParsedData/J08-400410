e common than the other.
    This problem, already raised by Hsu and Field (2003, page 207) among others, can be illustrated using the following example (Di Eugenio and Glass 2004, example 3, pages 98&#8211;99).
    Suppose 95% of utterances in a particular domain are statement, and only 5% are inforequest.
    We would then expect by chance that 0.95 &#215; 0.95 = 0.9025 of the utterances would be classified as statement by both coders, and 0.05 &#215; 0.05 = 0.0025 as inforequest, so the coders would agree on 90.5% of the utterances.
    Under such circumstances, a seemingly high observed agreement of 90% is actually worse than expected by chance.
    The conclusion reached in the literature is that in order to get figures that are comparable across studies, observed agreement has to be adjusted for chance agreement.
    These are the measures we will review in the remainder of this article.
    We will not look at the variants of percentage agreement used in CL work on discourse before the introduction of 