can be applied to any evaluation criterion.
    Hence, if an improved automatic evaluation criterion is developed that has an even better correlation with human judgments than BLEU and NIST, we can plug this alternative criterion directly into the training procedure and optimize the model parameters for it.
    This means that improved translation evaluation measures lead directly to improved machine translation quality.
    Of course, the approach presented here places a high demand on the fidelity of the measure being optimized.
    It might happen that by directly optimizing an error measure in the way described above, weaknesses in the measure might be exploited that could yield better scores without improved translation quality.
    Hence, this approach poses new challenges for developers of automatic evaluation criteria.
    Many tasks in natural language processing, for instance summarization, have evaluation criteria that go beyond simply counting the number of wrong system decisions and the framework