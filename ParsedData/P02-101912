   For example, if we want to find how to pronounce the second b in abbot we would chose the empty phone because the first rule mentioned above is more specific than the second.
    We implemented five extensions to the initial model which together decreased the error rate of the letterto-phone model by around 20%.
    These are: for a word using a fourgram vowel sequence language model The performance figures reported by Fisher (1999) are significantly higher than our figures using the basic model, which is probably due to the cleaner data used in their experiments and the differences in phoneset size.
    The extensions we implemented are inspired largely by the work on letter-to-phone conversion using decision trees (Jiang et al., 1997).
    The last extension, rescoring based on vowel fourgams, has not been proposed previously.
    We tested the algorithms on the NETtalk and Microsoft Speech dictionaries, by splitting them into training and test sets in proportion 80%/20% training-set to test-set size.
  