ting hit rates for each word separately and then taking an unweighted average ensures that a correct translation of a common source word does not contribute more to the score than correct translations of rare words.
    The evaluation is uniform over the whole lexicon.
    BiBLE evaluation is quite harsh, because many translations are not word for word in real bitexts.
    To put BiBLE scores reported here into proper perspective, human performance was evaluated on a similar task: The 1994 ARPA-sponsored machine translation evaluation effort generated two independent English translations of one hundred French newspaper texts [Whi93].
    I hand-aligned each pair of translations by paragraph; most paragraphs contained between one and four sentences.
    For each pair of translations, the fraction of times (by type) that identical words were used in corresponding ,paragraphs was computed.
    The average of these 100 fractions was 0.6182 with a standard deviation of 0.0647.
    This is a liberal estimate of the