cations like machine translation (Wu, 1997) and question answering (Wang et al., 2007).
    An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning&#8212;both from labeled and unlabeled data.
    Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors.
    There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007).
    Bayesian methods offer an elegant framework for combining prior knowledge with data.
    The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inf