on of several features* These currently include unigram-precision, unigram-recall, and a direct measure of how out-oforder the words of the MT output are with respect to the reference* The score assigned to each individual sentence of MT output is derived from the best scoring match among all matches over all reference translations* The maximal-scoring match
  
  
    ing is then also used in order to calculate an aggregate score for the MT system over the entire test set* Section 2 describes the metric in detail, and provides a full example of the matching and scoring* In previous work (Lavie et al*, 2004), we compared METEOR with IBM's BLEU metric and it's derived NIST metric, using several empirical evaluation methods that have been proposed in the recent literature as concrete means to assess the level of correlation of automatic metrics and human judgments* We demonstrated that METEOR has significantly improved correlation with human judgments* Furthermore, our results demonstrated that recall plays a mo