 conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: Ratnaparkhi (1996), Toutanova and Manning (2000), and Collins (2002) all present unregularized models.
    Indeed, the result of Collins (2002) that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized.
    Table 5 shows results on the development set from two pairs of experiments.
    The first pair of models use common word templates 40, w0), 40, t_1, t_2) and the same rare word templates as used in the models in table 2.
    The second pair of models use the same features as model BEST with a higher frequency cutoff of 5 for common word features.
    For the first pair of models, the error reduction from smoothing is 5.3% overall and 20.1% on unknown words.
    For the second pair of models, the error reduction is even bigger: 16.2% overall after convergence and 5.8%