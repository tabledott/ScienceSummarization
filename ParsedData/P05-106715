ion D( t) is defined on a dependency tree t , and outputs a certain ET derivation tree of t , which is generated by decomposing t into ETs.
    Given t , there could be multiple decompositions.
    Conditioned on decomposition D , we can rewrite (2) as: By definition, the ET derivation trees of the input and output trees should be isomorphic: D(T(f )) &#8773; D(T(e)) .
    Let Tran(u) be a set of possible translations for the ET u .
    We have: For any ET v in a given ET derivation tree d , let Root(d) be the root ET of d , and let Parent(v) denote the parent ET of v .
    We have: an HMM.
    While HMM is defined on a sequence our model is defined on the derivation tree of ETs.
    In reality, the learned parallel ETs are unlikely to cover all the structures that we may encounter in decoding.
    As a unified approach, we augment the SDIG by adding all the possible word pairs (fj, ei) as a parallel ET pair and using the IBM Model 1 (Brown et al., 1993) word to word translation probability as the ET translat