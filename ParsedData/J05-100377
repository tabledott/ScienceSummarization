 Figure 4 is, however, a new algorithm that could perhaps be generalized to the full problem of Freund et al. (1998); we leave this to future research.
    Altun, Hofmann, and Johnson (2003) and Altun, Johnson, and Hofmann (2003) describe experiments on tagging tasks using the ExpLoss function, in contrast to the LogLoss function used in Lafferty, McCallum, and Pereira (2001).
    Altun, Hofmann, and Johnson (2003) describe how dynamic programming methods can be used to calculate gradients of the ExpLoss function even in cases in which the set of candidates again includes all possible tagged sequences, a set which grows exponentially in size with the length of the sentence being tagged.
    Results in Altun, Johnson, and Hofmann (2003) suggest that the choice of ExpLoss versus LogLoss does not have a major impact on accuracy for the tagging task in question.
    Perceptron-based algorithms, or the voted perceptron approach of Freund and Schapire (1999), are another alternative to boosting and LogLoss methods.