sts of similarity using this metric (pairwise, or comparing all labelers) gave probability near zero.
    We concluded that this statistic did not serve, for example, to capture the differences observed between labelings from text alone versus labelings from text and speech.
    Recent discourse annotation studies (Isard and Carletta, 1995; Flammia and Zue, 1995) have measured reliability using the K coefficient, which factors out chance agreement taking the expected distribution of categories into account.
    This coefficient is defined as where Po represents the observed agreement and PE represents the expected agreement.
    Typically, values of .7 or higher for this measure provide evidence of good reliability, while values of .8 or greater indicate high reliability.
    Isard and Carletta (1995) report pairwise K scores ranging from .43 to .68 in a study of naive and expert classifications of types of 'moves' in the Map Task dialogues.
    For theory-neutral discourse segmentations of information-seekin