F1 Size F1 Size F1 Size 1 60.47 2558 60.36 2597 60.5 2557 2 69.53 3788 69.38 4614 71.08 4264 8 74.32 4262 79.26 120598 79.15 50629 12 70.99 7297 78.8 160403 78.94 86386 16 66.99 19616 79.2 261444 78.24 131377 20 64.44 27593 79.27 369699 77.81 202767 Table 2: Shows development F1 and grammar sizes (the number of effective rules) as we increase the truncation K. in K. We used these values of ?B in the following experiments.
			3.2.2 Results The regime in which Bayesian inference is most important is when training data is scarce relative tothe complexity of the model.
			We train on just sec tion 2 of the Penn Treebank.
			Table 2 shows how the HDP-PCFG-GR can produce compact grammars that guard against overfitting.
			Without smoothing,ordinary PCFGs trained using EM improve as K in creases but start to overfit around K = 4.
			Simple add-1.01 smoothing prevents overfitting but at thecost of a sharp increase in grammar sizes.
			The HDP PCFG obtains comparable performance with a much smaller number of rules.We 