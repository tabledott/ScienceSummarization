 IBM models.
    One advantage that Brown et al. 's Model 1 has over our word-to-word model is that their objective function has no local maxima.
    By using the EM algorithm (Dempster et al., 1977), they can guarantee convergence towards the globally optimum parameter set.
    In contrast, the dynamic nature of the competitive linking algorithm changes the Pr(datalmodel) in a non-monotonic fashion.
    We have adopted the simple heuristic that the model &amp;quot;has converged&amp;quot; when this probability stops increasing.
  
  
    Many multilingual NLP applications need to translate words between different languages, but cannot afford the computational expense of modeling the full range of translation phenomena.
    For these applications, we have designed a fast algorithm for estimating word-to-word models of translational equivalence.
    The estimation method uses a pair of hidden parameters to measure the model's uncertainty, and avoids making decisions that it's not likely to make correctly.
    T