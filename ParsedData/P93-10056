egs the following question: How much information about the context of a sentence or phrase is necessary and sufficient to determine its meaning?
    This question is at the crux of the debate among computational linguists about the application and implementation of statistical methods in natural language understanding.
    Previous work on disambiguation and probabilistic parsing has offered partial answers to this question.
    Hidden Markov models of words and their tags, introduced in (5) and (5) and popularized in the natural language community by Church (5), demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity.
    Hindle and Rooth (5) use a statistical measure of lexical associations to resolve structural ambiguities.
    Brent (5) acquires likely verb subcategorization patterns using the frequencies of verbobject-preposition triples.
    Magerman and Marcus (5) propose a model of context that combines the n-gram model with information from dominating constituents.
    Al