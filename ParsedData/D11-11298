discussed in Widdows (2005).
    The principal drawback of such models is their non-compositional nature: they ignore grammatical structure and logical words, and hence cannot compute the meanings of phrases and sentences in the same efficient way that they do for words.
    Common operations discussed in (Mitchell and Lapata, 2008) such as vector addition (+) and componentwise multiplication (O, cf.
    &#167;4 for details) are commutative, hence if vw&#8722;&#8594; = &#8594;&#8722; v + &#8594;&#8722;w or &#8594;&#8722;v O &#8594;&#8722;w , then the dog bit the man = the man bit the dog Non-commutative operations, such as the Kronecker product (cf.
    &#167;4 for definition) can take word-order into account (Smolensky, 1990) or even some more complex syntactic relations, as described in Clark and Pulman (2007).
    However, the dimensionality of sentence vectors produced in this manner differs for sentences of different length, barring all sentences from being compared in the same vector space, and growing 