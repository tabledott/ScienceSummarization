d thus finding the global optimum is guaranteed.
    It has recently been shown that quasi-Newton methods, such as L-BFGS, are significantly more efficient than traditional iterative scaling and even conjugate gradient (Malouf, 2002; Sha and Pereira, 2003).
    This method approximates the second-derivative of the likelihood by keeping a running, finite-sized window of previous first-derivatives.
    L-BFGS can simply be treated as a black-box optimization procedure, requiring only that one provide the first-derivative of the function to be optimized.
    Assuming that the training labels on instance j make its state path unambiguous, let s(j) denote that path, and then the first-derivative of the log-likelihood is where Ck(s, o) is the &#8220;count&#8221; for feature k given s and o, equal to PTt=1 fk(st&#8722;1, st, o, t), the sum of fk(st&#8722;1, st, o, t) values for all positions, t, in the sequence s. The first two terms correspond to the difference between the empirical expected value of feature fk and