ory power of a model based on this assumption may be raised to an arbitrary level by extending Western notions of what words are to include words that contain spaces (e.g., in English) or several characters (e.g., in Chinese).
    For example, I have shown elsewhere how to estimate word-to-word translation models where a word can be a noncompositional compound consisting of several space-delimited tokens (Melamed, to appear).
    For the purposes of this article, however, words are the tokens generated by my tokenizers and stemmers for the languages in question.
    Therefore, the models in this article are only a first approximation to the vast complexities of translational equivalence between natural languages.
    They are intended mainly as stepping stones towards better models.
  
  
    Most methods for estimating translation models from bitexts start with the following intuition: Words that are translations of each other are more likely to appear in corresponding bitext regions than other pairs of word