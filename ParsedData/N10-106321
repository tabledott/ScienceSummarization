t of including additional parallel sentences automatically extracted from Wikipedia in the system training data.
    For German-English and Spanish-English, we extracted data with the null loss adjusted to achieve an estimated precision of 95 percent, and for English-Bulgarian a precision of 90 percent.
    Table 4 summarizes the characteristics of these data sets.
    We were pleasantly surprised at the amount of parallel sentences extracted from such a varied comparable corpus.
    Apparently the average Wikipedia article contains at least a handful of parallel sentences, suggesting this is a very fertile ground for training MT systems.
    The extracted Wikipedia data is likely to make the greatest impact on broad domain test sets &#8211; indeed, initial experimentation showed little BLEU gain on in-domain test sets such as Europarl, where out-of-domain training data is unlikely to provide appropriate phrasal translations.
    Therefore, we experimented with two broad domain test sets.
    First, Bing Tran