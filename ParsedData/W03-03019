his set of evaluations pertains to full coverage word alignments.
    We conducted therefore 14 evaluations for each submission file: AER, Sure/Probable Precision, Sure/Probable Recall, and Sure/Probable F-measure, with a different figure determined for NULL-Align and NO-NULL-Align alignments.
  
  
    Seven teams from around the world participated in the word alignment shared task.
    Table 1 lists the names of the participating systems, the corresponding institutions, and references to papers in this volume that provide detailed descriptions of the systems and additional analysis of their results.
    All seven teams participated in the Romanian-English subtask, and five teams participated in the English-French subtask.3 There were no restrictions placed on the number of submissions each team could make.
    This resulted in a total of 27 submissions from the seven teams, where 14 sets of results were submitted for the English-French subtask, and 13 for the Romanian-English subtask.
    Of the 27 total su