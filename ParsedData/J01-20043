 more interest among computational linguists.
    While language models built around shallow local dependencies are still the standard in state-of-the-art speech recognition systems, there is reason to hope that better language models can and will be developed by computational linguists for this task.
    This paper will examine language modeling for speech recognition from a natural language processing point of view.
    Some of the recent literature investigating approaches that use syntactic structure in an attempt to capture long-distance dependencies for language modeling will be reviewed.
    A new language model, based on probabilistic top-down parsing, will be outlined and compared with the previous literature, and extensive empirical results will be presented which demonstrate its utility.
    Two features of our top-down parsing approach will emerge as key to its success.
    First, the top-down parsing algorithm builds a set of rooted candidate parse trees from left to right over the string, which 