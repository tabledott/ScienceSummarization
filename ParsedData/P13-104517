vanishing gradient problems.
    Socher et al. (2012) proposed to give every single word a matrix and a vector.
    The matrix is then applied to the sibling node&#8217;s vector during the composition.
    While this results in a powerful composition function that essentially depends on the words being combined, the number of model parameters explodes and the composition functions do not capture the syntactic commonalities between similar POS tags or syntactic categories.
    Based on the above considerations, we propose the Compositional Vector Grammar (CVG) that conditions the composition function at each node on discrete syntactic categories extracted from a Figure 3: Example of a syntactically untied RNN in which the function to compute a parent vector depends on the syntactic categories of its children which we assume are given for now.
    PCFG.
    Hence, CVGs combine discrete, syntactic rule probabilities and continuous vector compositions.
    The idea is that the syntactic categories of the children