, for a sampled history h, we can propose alternative histories h' that result in the same commands and parameters with different word spans.
    We can again apply equation 5 for each h', weighted by its probability under the current policy, p(h&#65533;|&#952;) The algorithm we have presented belongs to a family of policy gradient algorithms that have been successfully used for complex tasks such as robot control (Ng et al., 2003).
    Our formulation is unique in how it represents natural language in the reinforcement learning framework.
    We can design a range of reward functions to guide learning, depending on the availability of annotated data and environment feedback.
    Consider the case when every training document d E D is annotated with its correct sequence of actions, and state transitions are deterministic.
    Given these examples, it is straightforward to construct a reward function that connects policy gradient to maximum likelihood.
    Specifically, define a reward function r(h) that retur