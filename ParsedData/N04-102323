features, cf.
    (Shen and Joshi, 2004).
    If the number of the non-discriminative features is large enough, the data set becomes unsplittable.
    We have tried using the trick as in (Li et al., 2002) to make data separable artificially, but the performance could not be improved with such features.
    We achieve similar results with Algorithm 2, the ordinal regression with uneven margin.
    It converges on the first 3 feature sets too.
    On the Baseline, it achieves 31.4%.
    We notice that the model is over-trained on the development data according to the learning curve.
    In the Best Feature category, it achieves 32.7%, and on the Top Twenty features, it achieves 32.9%.
    This algorithm does not converge on the Large Set in 10000 iterations.
    We compare our perceptron-like algorithms with the minimum error training used in (SMT Team, 2003) as shown in Table 2.
    The splitting algorithm achieves slightly better results on the Baseline and the Best Feature set, while the minimum error traini