rpose optimization methods are a simple, competitive solution to learning shallow parsers.
    These models combine the best features of generative finite-state models and discriminative (log-)linear classifiers, and do NP chunking as well as or better than &#8220;ad hoc&#8221; classifier combinations, which were the most accurate approach until now.
    In a longer version of this work we will also describe shallow parsing results for other phrase types.
    There is no reason why the same techniques cannot be used equally successfully for the other types or for other related tasks, such as POS tagging or named-entity recognition.
    On the machine-learning side, it would be interesting to generalize the ideas of large-margin classification to sequence models, strengthening the results of Collins (2002) and leading to new optimal training algorithms with stronger guarantees against overfitting.
    On the application side, (log-)linear parsing models have the potential to supplant the currently dominant lex