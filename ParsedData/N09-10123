lly combined for unsupervised dependency parsing.
    A brief overview of the paper follows.
    In Section 2, we discuss the relevant background.
    Section 3 presents how we will extend DMV with additional that satisfies the following properties: features.
    We describe smoothing in an unsupervised context in Section 4.
    In Section 5, we discuss search issues.
    We present our experiments in Section 6 and conclude in Section 7.
  
  
    In this paper, the observed variables will be a corpus of n sentences of text s = s1 ... sn, and for each word sij an associated part-of-speech &#964;ij.
    We denote the set of all words as Vw and the set of all parts-ofspeech as V&#964;.
    The hidden variables are parse trees t = t1 ... tn and parameters &#952;&#175; which specify a distribution over t. A dependency tree ti is a directed acyclic graph whose nodes are the words in si.
    The graph has a single incoming edge for each word in each sentence, except one called the root of ti.
    An edge from word 