
  Two-Level Many-Paths Generation
  
    Large-scale natural language generation requires the integration of vast amounts of knowledge: lexical, grammatical, and conceptual.
    A robust generator must be able to well pieces of knowledge are missing.
    It must also be robust against incomplete or inaccurate inputs.
    To attack these problems, we have built a hybrid generator, in which gaps in symbolic knowledge are filled by statistical methods.
    We describe algorithms and show experimental results.
    We also discuss how the hybrid generation model can be used to simplify current generators and enhance their portability, even when perfect knowledge is in principle obtainable.
  
  
    A large-scale natural language generation (NLG) system for unrestricted text should be able to operate in an environment of 50,000 conceptual terms and 100,000 words or phrases.
    Turning conceptual expressions into English requires the integration of large knowledge bases (KBs), including grammar, ontology, lexicon