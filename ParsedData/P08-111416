(i.e.
    IP=, IP+, VP= and VP+), and NP_ (tying weights of NP= and NP+; see Section 3).
    Since component features in those combinations were informed by individual-feature performance on the test set, we tested the best performing conditions from MT06 on a new test set, NIST MT08.
    NP= and VP+ yielded significant improvements of up to 1.53 BLEU.
    Combination conditions replicated the pattern of results from MT06, including the same increasing order of gains, with improvements up to 1.11 BLEU.
    For Arabic-English translation, we used the training corpora in Table 3, approximately 100,000 sentence pairs after GIZA++ length-ratio filtering.
    We trained a trigram language model using the English side of this training set, plus the English Gigaword v2 AFP and Gigaword v1 Xinhua corpora.
    Development and minimum error rate training were done using the NIST MT02 set.
    Table 4 presents our results.
    We first tested on on the NIST MT03 and MT06 (nist-text) sets.
    On MT03, the original, undi