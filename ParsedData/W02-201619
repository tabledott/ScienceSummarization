 (99) report that using the EDR Corpus for their training and testing, they achieve around 85.03% accuracy with Decision Tree and Boosting.
    Although Decision Tree can take combinations of features as SVMs, it easily overfits on its own.
    To avoid overfitting, Decision Tree is usually used as an weak learner for Boosting.
    Combining Boosting technique with Decision Tree, the performance may be improved.
    However, Haruno et al. (99) report that the performance with Decision Tree falls down when they added lexical entries with lower frequencies as features even using Boosting.
    We think that Decision Tree requires a careful feature selection for achieving higher accuracy.
  
  
    We presented a new Japanese dependency parser using a cascaded chunking model which achieves 90.46% accuracy using the Kyoto University Corpus.
    Our model parses a sentence deterministically only deciding whether the current segment modifies the segment on its immediate right hand side.
    Our model outperforms the