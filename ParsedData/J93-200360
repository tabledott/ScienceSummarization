shown in the table.
    As the likelihood increases, the perplexity decreases.
    We see a steady decrease in perplexity as the iterations progress except when we switch from Model 2 as the In model to Model 3.
    This sudden jump is not because Model 3 is a poorer model than Model 2, but because Model 3 is deficient: the great majority of its probability is squandered on objects that are not strings of French words.
    As we have argued, deficiency is not a problem.
    In our description of Model 1, we left Pr(m I e) unspecified.
    In quoting perplexities for Models 1 and 2, we have assumed that the length of the French string is Poisson with a mean that is a linear function of the length of the English string.
    Specifically, we have assumed that Pr(M = m le) = (A Oine-Al I M!, with A equal to 1.09.
    It is interesting to see how the Viterbi alignments change as the iterations progress.
    In Figure 5, we show for several sentences the Viterbi alignment after iterations 1, 6, 7, and 12.
    Itera