usands of features.
    Specifically, we use 15 baseline features for PBMT, similar to the baseline features described by Watanabe et al. (2007).
    We use 19 baseline features for SBMT, similar to the baseline features described by Chiang et al. (2008b).
    We used the following feature classes in SBMT and PBMT extended scenarios: We used the following feature classes in SBMT extended scenarios only (cf.
    Chiang et al. (2009), Section 4.1):10 We used the following feature classes in PBMT extended scenarios only: The feature classes and number of features used within those classes for each language pair are summarized in Table 3.
    Each of the three approaches we compare in this study has various details associated with it that may prove useful to those wishing to reproduce our results.
    We list choices made for the various tuning methods here, and note that all our decisions were made in keeping with best practices for each algorithm.
    We used David Chiang&#8217;s CMERT implementation of MERT th