obability matrix,we also know the corresponding state and observa tion which is nothing but the word pair (sj , ti).
			We will then align the pair and continue to find the next posterior maximum and align the resulting pair.
			At each iteration of the process, a word pair is aligned.
			The process is repeated until either every word in one (or both) language is aligned or no more maximum can be found, whichever happens first.
			7.3.2 GIZA Alignment In order to contrast our algorithm, we ranGIZA++ in the standard configuration which im plies 5 iterations of IBM Model 1, HMM, Model 3 and Model 4.
			All parameters are left to their default values.
			The results using the three different aligners is shown in Table 4.
			The reduction in AER over theGIZA++ system is 40.5% and over the HMM sys tem is 48.5%.
			The Wilcoxon signed-rank test yieldsa probability of 0.39 for rejecting the GIZA++ align ment over the HMM alignment, whereas the MaxEnt algorithm should be rejected with a probability of1.7e-6 over the