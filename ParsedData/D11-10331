e Translation (SMT) system performance is dependent on the quantity and quality of available training data.
    The conventional wisdom is that more data is better; the larger the training corpus, the more accurate the model can be.
    The trouble is that &#8211; except for the few all-purpose SMT systems &#8211; there is never enough training data that is directly relevant to the translation task at hand.
    Even if there is no formal genre for the text to be translated, any coherent translation task will have its own argot, vocabulary or stylistic preferences, such that the corpus characteristics will necessarily deviate from any all-encompassing model of language.
    For this reason, one would prefer to use more in-domain data for training.
    This would empirically provide more accurate lexical probabilities, and thus better target the task at hand.
    However, parallel in-domain data is usually hard to find1, and so performance is assumed to be limited by the quantity of domain-specific training dat