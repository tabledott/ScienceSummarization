related sentences).
    A model that exhibits high agreement with human judges not only accurately captures the coherence properties of the summaries in question, but ultimately holds promise for the automatic evaluation of machine-generated texts.
    Existing automatic evaluation measures such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), are not designed for the coherence assessment task, since they focus on content similarity between system output and reference texts.
    Data Our evaluation was based on materials from the Document Understanding Conference (DUC, 2003), which include multi-document summaries produced by human writers and by automatic summarization systems.
    In order to learn a ranking, we require a set of summaries, each of which have been rated in terms of coherence.
    We therefore elicited judgments from human subjects.3 We randomly selected 16 input document clusters and five systems that had produced summaries for these sets, along with summaries composed by seve