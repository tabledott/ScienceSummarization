
  Early Results For Named Entity Recognition With Conditional Random Fields Feature Induction And Web-Enhanced Lexicons
  
  
    Models for many natural language tasks benefit from the flexibility to use overlapping, non-independent features.
    For example, the need for labeled data can be drastically reduced by taking advantage of domain knowledge in the form of word lists, part-of-speech tags, character ngrams, and capitalization patterns.
    While it is difficult to capture such inter-dependent features with a generative probabilistic model, conditionally-trained models, such as conditional maximum entropy models, handle them well.
    There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998).
    Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines.
    While based on the same exponential form as maximum entropy m