acy of the CCG parser is over 81% F-score on labeled dependencies, against an upper bound of 84.8%.
    The CCG parser also outperforms RASP overall and on the majority of dependency types.
    The contributions of this article are as follows.
    First, we explain how to estimate a full log-linear parsing model for an automatically extracted grammar, on a scale as large as that reported anywhere in the NLP literature.
    Second, the article provides a comprehensive blueprint for building a wide-coverage CCG parser, including theoretical and practical aspects of the grammar, the estimation process, and decoding.
    Third, we investigate the difficulties associated with cross-formalism parser comparison, evaluating the parser on DepBank.
    And finally, we develop new models and decoding algorithms for CCG, and give a convincing demonstration that, through use of a supertagger, highly efficient parsing is possible with CCG.
  
  
    The first application of log-linear models to parsing is the work of Ratna