tures and a language model, work which we have left for a later date.
    The relative scores confirm that our model, with its minimalist feature set, achieves comparable performance to the standard feature set without the language model.
    This is encouraging as our model was trained to optimise likelihood rather than BLEU, yet it is still competitive on that metric.
    As expected, the language model makes a significant difference to BLEU, however we believe that this effect is orthogonal to the choice of base translation model, thus we would expect a similar gain when integrating a language model into the discriminative system.
    An informal comparison of the outputs on the development set, presented in Table 4, suggests that the 7Although the most direct comparison for the discriminative model would be with pd model alone, omitting the gr, rc and wc features and MERT training produces poor translations. d , plex r , gr, rc, wc) (H) models, relative to the source (S) and reference (R). translation opt