of the paper, and described in Section 5.
    We measured the correlation of the automatic evaluation metrics with the different types of human judgments on 12 data conditions, and report these in Section 6.
  
  
    The results of the human evaluation are given in Tables 9, 10, 11 and 12.
    Each of those tables present four scores: There was reasonably strong agreement between these four measures at which of the entries was the best in each data condition.
    There was complete 5Since different annotators can vary widely in how they assign fluency and adequacy scores, we normalized these scores on a per-judge basis using the method suggested by Blatz et al. (2003) in Chapter 5, page 97. agreement between them in 5 of the 14 conditions, and agreement between at least three of them in 10 of the 14 cases.
    Table 3 gives a summary of how often different participants&#8217; entries were ranked #1 by any of the four human evaluation measures.
    SYSTRAN&#8217;s entries were ranked the best most often, foll