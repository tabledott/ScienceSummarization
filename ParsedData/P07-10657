like distribution of n-gram frequencies in natural language corpora: most events occur an extremely small number of times, while a small number are very frequent.
    We quantise raw frequencies, c(x), using a logarithmic codebook as follows, The precision of this codebook decays exponentially with the raw counts and the scale is determined by the base of the logarithm b; we examine the effect of this parameter in experiments below.
    Given the quantised count qc(x) for an n-gram x, the filter is trained by entering composite events consisting of the n-gram appended by an integer counter j that is incremented from 1 to qc(x) into the filter.
    To retrieve the quantised count for an n-gram, it is first appended with a count of 1 and hashed under the k functions; if this tests positive, the count is incremented and the process repeated.
    The procedure terminates as soon as any of the k hash functions hits a 0 and the previous count is reported.
    The one-sided error of the BF and the training scheme en