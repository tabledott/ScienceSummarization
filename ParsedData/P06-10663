es, but also integrates some phrasal generalizations into the global model.
    In this paper, we propose a novel solution for phrasal reordering.
    Here, under the ITG constraint (Wu, 1997; Zens et al., 2004), we need to consider just two kinds of reorderings, straight and inverted between two consecutive blocks.
    Therefore reordering can be modelled as a problem of classification with only two labels, straight and inverted.
    In this paper, we build a maximum entropy based classification model as the reordering model.
    Different from lexicalized reordering, we do not use the whole block as reordering evidence, but only features extracted from blocks.
    This is more flexible.
    It makes our model reorder any blocks, observed in training or not.
    The whole maximum entropy based reordering model is embedded inside a log-linear phrase-based model of translation.
    Following the Bracketing Transduction Grammar (BTG) (Wu, 1996), we built a CKY-style decoder for our system, which makes it possib