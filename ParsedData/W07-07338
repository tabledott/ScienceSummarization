eloped by Birch et al. (2007).
    For our purposes, we use two decoding paths, each consisting of only one translation step.
    One decoding path is the in-domain translation table, and the other decoding path is the out-of-domain translation table.
    Again, respective weights are set with minimum error rate training.
  
  
    Table 1 shows results of our domain adaptation experiments on the development test set (nc-devtest2007).
    The results suggest that the language model is a useful tool for domain adaptation.
    While training on all the data is essential for good performance, using an in-domain language model alone already gives fairly high performance (27.46).
    The performance with the interpolated language model (27.12) and two language models (27.30) are similar.
    All perform better than the three baseline approaches.
    The results also suggest that higher performance can be obtained by using two translation models through the Moses decoder&#8217;s alternative decoding path framework.