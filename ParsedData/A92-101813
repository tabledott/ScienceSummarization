a tagged training corpus.
    In this situation the Baum-Welch algorithm (also known as the forward-backward algorithm) can be used [Baum, 1972].
    Under this regime the model is called a hidden Markov model (HMM), as state transitions (i.e., part-of-speech categories) are assumed to be unobservable.
    Jelinek has used this method for training a text tagger [Jelinek, 1985].
    Parameter smoothing can be conveniently achieved using the method of deleted interpolation in which weighted estimates are taken from secondand first-order models and a uniform probability distribution [Jelinek and Mercer, 1980].
    Kupiec used word equivalence classes (referred to here as ambiguity classes) based on parts of speech, to pool data from individual words [Kupiec, 1989b].
    The most common words are still represented individually, as sufficient data exist for robust estimation.
    However all other words are represented according to the set of possible categories they can assume.
    In this manner, the vocabulary 