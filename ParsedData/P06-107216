 , 0, 2 }.
    After EM training, &#946; was diminished by 10;this was repeated down to a value of &#946;f = &#8722;3.
    Performance after training at each &#946; value is shown in Fig.
    3.7 We see that, typically, there is a sharp increase in performance somewhere during training, which typically lessens as &#946; -* &#8722;oo.
    Starting &#946; too high can also damage performance.
    This method, then, is not robust to the choice of &#955;, &#946;0, or &#946;f, nor does it always do as well as annealing &#948;, although considerable gains are possible; see the fifth column of Table 3.
    By testing models trained with afixed value of &#946; (for values in [&#8722;1,1]), we ascertained that the performance improvement is due largely to annealing, not just the injection of segmentation bias (fourth vs. fifth column of Table 3).8
  
  
    Contrastive estimation (CE) was recently introduced (Smith and Eisner, 2005a) as a class of alternatives to the likelihood objective function locally maximized by 