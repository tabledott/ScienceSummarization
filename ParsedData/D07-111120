sumes input tokens, while the for ward model must consume most tokens before it starts making attachments.
			In other words, context in general in the backward model has more struc ture, and attachments are made while there are still look-ahead tokens, while the opposite is generally true in the forward model.
	
	
			Our results demonstrate the effectiveness of even small ensembles of parsers that are relatively similar (using the same features and the same algorithm).
			There are several possible extensions and improvements to the approach we have described.
			For example, in section 3 we mention the use of different weighting schemes in dependency voting.
			We list additional ideas that were not attempted due to time constraints, but that are likely to produce improved results.
			One of the simplest improvements to our approach is simply to train more models with no oth er changes to our set-up.
			As mentioned in section 5, the addition of a backward SVM model did im prove accuracy on the Turkish set 