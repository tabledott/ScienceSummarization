with &#966;(e |f) less than 10&#8722;5 was used by the decoder.
    Given this empirical threshold, nearly 60% of entries in &#966;EM are unusable, compared with 1% in &#966;H.
    While this determinism of &#966;EM may be desirable in some circumstances, we found that the ambiguity in &#966;H is often preferable at decoding time.
    Several learned distributions have very low entropy.
    30 In particular, the pattern of translation-ambiguous 0 phrases receiving spuriously peaked distributions (as 0 - 01 01 - .5 5 - 1 1 described in section 3.1) introduces new traslation Entropy errors relative to the baseline.
    We now investigate both positive and negative effects of the learning process.
    The issue that motivated training a generative model is sometimes resolved correctly: for a word that translates differently alone than in the context of an idiom, the translation probabilities can more accurately reflect this.
    Returning to the previous example, the phrase table for chat has been corrected thro