her techniques could be borrowed.
    How does the approach presented here differ from these various earlier models, particularly those based on HMMs?
    Apart from corpus and tag set differences, our approach differs primarily in that it generalizes the simple HMM approach to cope with new kinds of problems, based on the Bayes network representations depicted in Figures 2 and 4.
    For the DA classification task, our framework allows us to do classification given unreliable words (by marginalizing over the possible word strings corresponding to the acoustic input) and given nonlexical (e.g., prosodic) evidence.
    For the speech recognition task, the generalized model gives a clean probabilistic framework for conditioning word probabilities on the conversation context via the underlying DA structure.
    Unlike previous models that did not address speech recognition or relied only on an intuitive 1-best approximation, our model allows computation of the optimum word sequence by effectively summing over al