from the Internet.3 To get a sense of the relative need for lexical normalisation, we perform analysis of the distribution of OOV words in different text types.
    In particular, we calculate the proportion of OOV tokens per message (or sentence, in the case of edited text), bin the messages according to the OOV token proportion, and plot the probability mass contained in each bin for a given text type.
    The three corpora we compare are the New York Times (NYT),4 SMS,5 and Twitter.6 The results are presented in Figure 1.
    Both SMS and Twitter have a relatively flat distribution, with Twitter having a particularly large tail: around 15% of tweets have 50% or more OOV tokens.
    This has implications for any context modelling, as we cannot rely on having only isolated occurrences of OOV words.
    In contrast, NYT shows a more Zipfian distribution, despite the large number of proper names it contains.
    While this analysis confirms that Twitter and SMS are similar in being heavily laden with OOV token