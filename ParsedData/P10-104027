ers and C&amp;W embeddings incur almost identical numbers of errors, and errors are concentrated around the more common words.
    We hypothesize that non-rare words have good representations, regardless of the choice of word representation technique.
    For tasks like chunking in which a syntactic decision relies upon looking at several token simultaneously, compound features that use the word representations might increase accuracy more (Koo et al., 2008).
    Using word representations in NER brought larger gains on the out-of-domain data than on the in-domain data.
    We were surprised by this result, because the OOD data was not even used during the unsupervised word representation induction, as was the in-domain data.
    We are curious to investigate this phenomenon further.
    Ando and Zhang (2005) present a semisupervised learning algorithm called alternating structure optimization (ASO).
    They find a lowdimensional projection of the input features that gives good linear classifiers over auxili