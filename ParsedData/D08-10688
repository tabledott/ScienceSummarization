ook McCallum &amp; Wellner (2005) nontrivial effort to incorporate, can be handled in Markov logic with the addition of a single formula (Poon &amp; Domingos, 2008)).
  
  
    In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor &amp; Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy.
    One of the most powerful representations for joint inference is Markov logic, a probabilistic extension of first-order logic (Richardson &amp; Domingos, 2006).
    A Markov logic network (MLN) is a set of weighted first-order clauses.
    Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause.
    The weight of a feature is the weight of the firstorder clause that originated it.
    The probability of a state x in such a network is given by P(x) = (1/Z) exp (Ez wzfz(x)), where Z is a normalization constant, wz 