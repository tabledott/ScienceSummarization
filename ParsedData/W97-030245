ely according to both are especially good.
    The entropy of a sentence measures its likelihood according to the second pass, but ignores the fact that the returned parse must also be likely according to the first pass.
    Thus, entropy, our measure in the previous experiments, which measures only likelihood according to the final pass, is not necessarily the right measure to use.
    We therefore give precision and recall results in this section.
    We still optimized our thresholding parameters using the same 31 sentence held out corpus, and minimizing entropy versus number of productions, as before.
    We should note that when we used a first pass grammar that captured a strict subset of the information in the second pass grammar, we have found that entropy is a very good measure of performance.
    As in our earlier experiments, it tends to be well correlated with precision and recall but less subject to noise.
    It is only because of the grammar mismatch that we have changed the evaluation.
    Fig