 perceptron version of SNoW (Roth, 1998).
    Positive examples used for iterative training are pairs of NEs and their best temporally aligned (thresholded) transliteration candidates.
    Negative examples are English non-NEs paired with random Russian words.
  
  
    We ran experiments using a bilingual comparable English-Russian news corpus we built by crawling a Russian news web site (www.lenta.ru).
    The site provides loose translations of (and pointers to) the original English texts.
    We collected pairs of articles spanning from 1/1/2001 through 10/05/2005.
    The corpus consists of 2,327 documents, with 0-8 documents per day.
    The corpus is available on our web page at http://L2R.cs.uiuc.edu/ cogcomp/.
    The English side was tagged with a publicly available NER system based on the SNoW learning architecture (Roth, 1998), that is available on the same site.
    This set of English NEs was hand-pruned to remove incorrectly classified words to obtain 978 single word NEs.
    In order to reduce