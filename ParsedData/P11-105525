s are unlikely to scale to the thousands of relations found in text on the Web.
    Open IE systems, which perform selfsupervised learning of relation-independent extractors (e.g., Preemptive IE (Shinyama and Sekine, 2006), TEXTRUNNER (Banko et al., 2007; Banko and Etzioni, 2008) and WOE (Wu and Weld, 2010)) can scale to millions of documents, but don&#8217;t output canonicalized relations.
    Weak supervision (also known as distant- or self supervision) refers to a broad class of methods, but we focus on the increasingly-popular idea of using a store of structured data to heuristicaly label a textual corpus.
    Craven and Kumlien (1999) introduced the idea by matching the Yeast Protein Database (YPD) to the abstracts of papers in PubMed and training a naive-Bayes extractor.
    Bellare and McCallum (2007) used a database of BibTex records to train a CRF extractor on 12 bibliographic relations.
    The KYLIN system aplied weak supervision to learn relations from Wikipedia, treating infoboxes as the associat