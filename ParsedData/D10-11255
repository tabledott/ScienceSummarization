 has strong theoretical guarantees, such as guaranteed convergence and the possibility of a certificate of optimality.
    Finally, in other recent work, Rush et al. (2010) describe dual decomposition approaches for other NLP problems.
  
  
    This section describes a particular class of models, sibling models; the next section describes a dualdecomposition algorithm for decoding these models.
    Consider the dependency parsing problem for a sentence with n words.
    We define the index set for dependency parsing to be Z = {(i, j) : i E {0 ... n}, j E {1... n}, i =&#65533; j}.
    A dependency parse is a vector y = {y(i, j) : (i, j) E Z}, where y(i, j) = 1 if a dependency with head word i and modifier j is in the parse, 0 otherwise.
    We use i = 0 for the root symbol.
    We define Y to be the set of all well-formed non-projective dependency parses (i.e., the set of directed spanning trees rooted at node 0).
    Given a function f : Y H R that assigns scores to parse trees, the optimal parse is A partic