 symbols, , and , were trained.
    The relationships between the number of parameters in the models and their parsing performances are shown in Figure 7.
    Note that models created using different binarization methods have different numbers of parameters for the same .
    The parsing performances were measured using F scores of the parse trees that were obtained by re-ranking of 1000-best parses by a PCFG.
    We can see that the parsing performance gets better as the model size increases.
    We can also see that models of roughly the same size yield similar performances regardless of the binarization scheme used for them, except the models created using LEFT binarization with small numbers of parameters ( and ).
    Taking into account the dependency on initial values at the level shown in the previous experiment, we cannot say that any single model is superior to the other models when the sizes of the models are large enough.
    The results shown in Figure 7 suggest that we could further improve parsi