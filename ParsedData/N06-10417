servation dependent; we are modeling the joint probability of x and y instead of y given x.
    As a result, learning an MRF is slightly harder than learning a CRF; we discuss this issue in section 4.4.
    We assume prior knowledge about the target structure via a prototype list, which specifies the set of target labels Y and, for each label y E Y, a set of prototypes words, py E Py.
    See figures 2 and 4 for examples of prototype lists.1 1Note that this setting differs from the standard semisupervised learning setup, where a small number of fully labeled examples are given and used in conjunction with a larger amount of unlabeled data.
    In our prototype-driven approach, we never provide a single fully labeled example sequence.
    See section 5.3 for further comparison of this setting to semi-supervised learning.
    Broadly, we would like to learn sequence models which both explain the observed data and meet our prior expectations about target structure.
    A straightforward way to implement this is 