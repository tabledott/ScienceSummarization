 the world is represented by a set of prediChoosing the closest preceding phrase is common because nearby phrases are a priori more likely to be coreferent.
    We refer to the training and inference methods described in this section as the Pairwise Model.
  
  
    where ZX; is a normalizer that sums over the two settings of yj.
    Note that this model gives us the representational power of recently proposed Markov logic networks (Richardson and Domingos, 2006); that is, we can construct arbitrary formulae in first-order logic to characterize the noun coreference task, and can learn weights for instantiations of these formulae.
    However, naively grounding the corresponding Markov logic network results in a combinatorial explosion of variables.
    Below we outline methods to scale training and prediction with this representation.
    As in the Pairwise Model, we must decide how to sample training examples and how to combine independent classifications at testing time.
    It is important to note that by 