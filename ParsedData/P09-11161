ed learning to an NLP problem, one first represents the problem as a vector of features.
    The learning algorithm then optimizes a regularized, convex objective function that is expressed in terms of these features.
    The performance of such learning-based solutions thus crucially depends on the informativeness of the features.
    The majority of the features in these supervised classifiers are predicated on lexical information, such as word identities.
    The long-tailed distribution of natural language words implies that most of the word types will be either unseen or seen very few times in the labeled training data, even if the data set is a relatively large one (e.g., the Penn Treebank).
    While the labeled data is generally very costly to obtain, there is a vast amount of unlabeled textual data freely available on the web.
    One way to alleviate the sparsity problem is to adopt a two-stage strategy: first create word clusters with unlabeled data and then use the clusters as features in supervis