response data of all but one subject) and a set of size 1 (i.e., the response data of a single subject).
    We then correlated the mean ratings of the former set with the ratings of the latter.
    This was repeated n times (see the number of participants in Table 6); the mean of the correlation coefficients for the seen and unseen bigrams is shown in Table 11 in the rows labeled &#8220;Agreement.&#8221; For both seen and unseen bigrams, we found no significant difference between the upper bound (intersubject agreement) and the correlation coefficients obtained using either AltaVista or Google counts.
    This finding holds for all three types of bigrams.
    The same picture emerged for the BNC and NANTC counts: These correlation coefficients were not significantly different from the upper limit, for all three types of bigrams, both for seen and for unseen bigrams.
    To conclude, our evaluation demonstrated that Web counts reliably predict human plausibility judgments, both for seen and for unseen predica