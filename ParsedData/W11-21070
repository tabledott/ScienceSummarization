
  Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems
  
    This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks.
    New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words.
    We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system.
  
  
    The Meteor1 metric (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010b) has been shown to have high correlation with human judgments in evaluations such as the 2010 ACL Workshop on Statistical Machine Translation and NIST Metrics MATR (Callison-Burch et al., 2010).
    However, previous versions of the metric are still limited b