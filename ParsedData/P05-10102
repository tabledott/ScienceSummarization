rd, we are forced to use some approximation of it.
    We empirically compared three different approximation methods.
    One of the three methods gives a performance of 86.6% (F, sentences 40 words) on the standard test set of the Penn WSJ corpus.
    Utsuro et al. (1996) proposed a method that automatically selects a proper level of generalization of non-terminal symbols of a PCFG, but they did not report the results of parsing with the obtained PCFG.
    Henderson&#8217;s parsing model (Henderson, 2003) has a similar motivation as ours in that a derivation history of a parse tree is compactly represented by induced hidden variables (hidden layer activation of a neural network), although the details of his approach is quite different from ours.
  
  
    PCFG-LA is a generative probabilistic model of parse trees.
    In this model, an observed parse tree is considered as an incomplete data, and the correplete data) and observed tree (incomplete data). sponding complete data is a tree with latent annotations