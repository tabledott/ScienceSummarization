ts and is competitive with the Gibbs samplers.
    In terms of times of convergence, we find that Variational Bayes was the fastest of all the estimators, especially on large data sets, and that explicit Gibbs sampler (both pointwise and sentence-blocked) were generally faster than their collapsed counterparts on large data sets.
  
  
    Probabilistic models now play a central role in computational linguistics.
    These models define a probability distribution P(x) over structures or analyses x.
    For example, in the part-of-speech (POS) tagging application described in this paper, which involves predicting the part-of-speech tag ti of each word wi in the sentence w = (w1,... , wn), the structure x = (w, t) consists of the words w in a sentence together with their corresponding parts-ofspeech t = (t1, ... , tn).
    In general the probabilistic models used in computational linguistics have adjustable parameters 0 which determine the distribution P(x 1 0).
    In this paper we focus on bitag Hidden Markov