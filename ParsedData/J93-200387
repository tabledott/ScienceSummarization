g-likelihood for a large training sample of translations.
    For our models, the only alignments that have positive probability are those for which each word of f is connected to at most one word of e. Relative Objective Function.
    We can compare hidden alignment models Po and Po using the relative objective function where P -0(a I f, e) = (a, f I e)/P0(f I e).
    Note that R(P6, Po) = 0.
    R is related to by Jensen's inequality Summing over e and f and using the Definitions (51) and (54) we arrive at Equation (55).
    We cannot create a good model or find good parameter values at a stroke.
    Rather we employ a process of iterative improvement.
    For a given model we use current parameter values to find better ones, and in this way, from initial values we find locally optimal ones.
    Then, given good parameter values for one model, we use them to find initial parameter values for another model.
    By alternating between these two steps we proceed through a sequence of gradually more sophisticat