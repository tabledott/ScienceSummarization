ing samples nearest to the hyperplane.
    Maximizing the margin insists that these nearest samples (support vectors) exist on both sides of the separating hyperplane and the hyperplane lies exactly at the midpoint of these support vectors.
    This margin maximization tightly relates to the fine generalization power of SVMs.
    Assuming that |w&#183;xi+b |= 1 at the support vectors without loss of generality, the SVM training can be formulated as the following optimization problem.3 The solution of this problem is known to be written as follows, using only support vectors and weights for them.
    In the SVM learning, we can use a function k(xi, xj) called a kernel function instead of the inner product in the above equation.
    Introducing a kernel function means mapping an original input x using (D(x), s.t.
    (D(xi)&#183;(D(xj) = k(xi, xj) to another, usually a higher dimensional, feature space.
    We construct the optimal hyperplane in that space.
    By using kernel functions, we can construct a non-