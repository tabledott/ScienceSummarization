matically unrelated sentences).
    A model that exhibits high agreement with human judges not only accurately captures the coherence properties of the summaries in question, but ultimately holds promise for the automatic evaluation of machine-generated texts.
    Existing automatic evaluation measures such as BLEU (Papineni et al. 2002) and ROUGE (Lin and Hovy 2003) are not designed for the coherence assessment task, because they focus on content similarity between system output and reference texts.
    Barzilay and Lapata Modeling Local Coherence Summary coherence rating can be also formulated as a ranking learning task.
    We are assuming that the learner has access to several summaries corresponding to the same document or document cluster.
    Such summaries can be produced by several systems that operate over identical inputs or by a single system (e.g., by varying the compression length or by switching on or off individual system modules, for example a sentence compression or anaphora resolution modul