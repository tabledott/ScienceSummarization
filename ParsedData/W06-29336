: 0.12&#8211;0.20; r: 0.0&#8211;0.6; C: 0.1&#8211;0.7; c: 0.01&#8211;1.0.
    Data has been split on the POS of the next input token for Czech (t = 200), German (t = 1000), and Spanish (t = 1000), and on the CPOS of the next input token for Bulgarian (t = 1000), Slovene (t = 600), and Turkish (t = 100).
    (For the remaining languages, the training data has not been split at all.
    )5 A dry run at the end of the development phase gave a labeled attachment score of 80.46 over the twelve required languages.
    Table 2 shows final test results for each language and for the twelve required languages together.
    The total score is only 0.27 percentage points below the score from the dry run, which seems to indicate that models have not been overfitted to the training data.
    The labeled attachment score varies from 91.65 to 65.68 but is above average for all languages.
    We have the best reported score for Japanese, Swedish and Turkish, and the score for Arabic, Danish, Dutch, Portuguese, Spanish, and ov