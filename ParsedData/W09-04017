e navigation.
    We further eliminated sentence pairs that varied from previous sentences by only numbers, which helped eliminate template web pages such as expense reports.
    We used a Bloom Filter (Talbot and Osborne, 2007) to do de-duplication, so it may have discarded more sentence pairs than strictly necessary.
    After deduplication, the parallel corpus contained 28 million sentence pairs with 0.8 billion French words and 0.7 billion English words.
    We have crawled the news sources that were the basis of our test sets (and a few more additional sources) since August 2007.
    This allowed us to assemble large corpora in the target domain to be mainly used as training data for language modeling.
    We collected texts from the beginning of our data collection period to one month before the test set period, segmented these into sentences and randomized the order of the sentences to obviate copyright concerns.
    To lower the barrier of entry for newcomers to the field, we provided Moses, an open s