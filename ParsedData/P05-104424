) throughout learning; bootstrapping either chooses, for each example, a single label, or remains completely agnostic.
    One can envision a mixed objective function that tries to fit the labeled examples while discriminating unlabeled examples from their neighborhoods.8 Regardless of how much (if any) data are labeled, the question of good smoothing techniques requires more attention.
    Here we used a single zero-mean, constant-variance Gaussian prior for all parameters.
    Better performance might be achieved by allowing different variances for different feature types.
    This 8Zhu and Ghahramani (2002) explored the semi-supervised classification problem for spatially-distributed data, where some data are labeled, using a Boltzmann machine to model the dataset.
    For them, the Markov random field is over labeling configurations for all examples, not, as in our case, complex structured labels for a particular example.
    Hence their B (Eq.
    5), though very large, was finite and could be sampled. w