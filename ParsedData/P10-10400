
  Word Representations: A Simple and General Method for Semi-Supervised Learning
  
    If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features.
    We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih &amp; Hinton, 2009) embeddings of words on both NER and chunking.
    We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines.
    We find further by combining word representations.
    You can download word features, for use in existing NLP systems, as well as our here:
  
  
    By using unlabelled data to reduce data sparsity in the labeled training data, semi-supervised approaches improve generalization accuracy.
    Semi-supervised models such as Ando and Zhang (2005), Suzuki and Isozaki (2008), and Suzuki et al. (2009) achieve state-of-the-art accuracy.
    However, these approaches dict