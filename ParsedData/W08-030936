The goal of this shared-task is two-fold: First we want to compare state-of-the-art machine translation systems, and secondly we aim to measure to what extent different evaluation metrics can be used to assess MT quality.
    With respect to MT quality we noticed that the introduction of test sets from a different domain did have an impact on the ranking of systems.
    We observed that rule-based systems generally did better on the News test set.
    Overall, it cannot be concluded that one approach clearly outperforms other approaches, as systems performed differently on the various translation tasks.
    One general observation is that for the tasks where statistical combination approaches participated, they tended to score relatively high, in particular with respect to Bleu.
    With respect to measuring the correlation between automated evaluation metrics and human judgments we found that using Meteor and ULCh (which utilizes a variety of metrics, including Meteor) resulted in the highest Spearman correl