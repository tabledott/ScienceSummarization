 if looking at the best accuracy achieved by the unsmoothed model (by stopping training after 75 iterations; see below).
    The especially large reduction in unknown word error reflects the fact that, because penalties are effectively stronger for rare features than frequent ones, the presence of penalties increases the degree to which more general cross-word signature features (which apply to unknown words) are used, relative to word-specific sparse features (which do not apply to unknown words).
    Secondly, use of regularization allows us to incorporate features with low support into the model while improving performance.
    Whereas Ratnaparkhi (1996) used feature support cutoffs and early stopping to stop overfitting of the model, and Collins (2002) contends that including low support features harms a maximum entropy model, our results show that low support features are useful in a regularized maximum entropy model.
    Table 6 contrasts our results with those from Collins (2002).
    Since the models 