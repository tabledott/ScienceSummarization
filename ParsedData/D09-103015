luation is conducted annually by the Linguistics Data Consortium, and it is used to determine whether the teams participating the program have met that year&#8217;s benchmarks.
    These evaluations are used as a &#8220;Go / No Go&#8221; determinant of whether teams will continue to receive funding.
    Thus, each team have a strong incentive to get as good a result as possible under the metric.
    Each of the three GALE teams encompasses multiple sites and each has a collection of machine translation systems.
    A general strategy employed by all teams is to perform system combination over these systems to produce a synthetic translation that is better than the sum of its parts (Matusov et al., 2006; Rosti et al., 2007).
    The contribution of each component system is weighted by the expectation that it will produce good output.
    To our knowledge, none of the teams perform their own HTER evaluations in order to set these weights.
    We evaluated the feasibility of using Mechanical Turk to perform HTER