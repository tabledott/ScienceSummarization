orm of maximum-likelihood estimation is often used for parameter estimation.
    The parameters are chosen to maximize the log-likelihood of the training set; equivalently: we talk (to emphasize the similarities to the boosting approach) about minimizing the negative log-likelihood.
    The negative log-likelihood, LogLoss(&#175;a), is defined as There are many methods in the literature for minimizing LogLoss(&#175;a) with respect to &#175;a, for example, generalized or improved iterative scaling (Berger, Della Pietra, and Della Pietra 1996; Della Pietra, Della Pietra, and Lafferty 1997), or conjugate gradient methods (Malouf 2002).
    In the next section we describe feature selection methods, as described in Berger, Della Pietra, and Della Pietra (1996) and Della Pietra, Della Pietra, and Lafferty (1997).
    Once the parameters a&#175; are estimated on training examples, the output for an example x is the most likely label under the model, where as before, sign (z) = 1 if z &gt; 0, sign (z) = &#8212;1 othe