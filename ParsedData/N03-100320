rmance gap for all the judges of about 38%, although the absolute scores vary (for example, Judge 4 seems lenient).
    The judges&#8217; assessment of correctness was fairly constant between the full 100-instance set and just the 50-instance common set alone.
    In terms of agreement, the Kappa value (measuring pairwise agreement discounting chance occurrences9) on the common set was 0.54, which corresponds to moderate agreement (Landis and Koch, 1977).
    Multiway agreement is depicted in Figure 4 &#8212; there, we see that in 86 of 100 cases, at least three of the judges gave the same correctness assessment, and in 60 cases all four judges concurred.
    Finally, we evaluated the quality of the paraphrase sentences generated by our system, thus (indirectly) testing all the system components: pattern selection, paraphrase acquisition, and generation.
    We are not aware of another system generating sentence-level paraphrases.
    Therefore, we used as a baseline a simple paraphrasing system that just rep