
  Evaluating Content Selection In Summarization: The Pyramid Method
  
    We present an empirically grounded method for evaluating content selection in summarization.
    It incorporates the idea that no single best model summary for a collection of documents exists.
    Our method quantifies the relative importance of facts to be conveyed.
    We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.
  
  
    Evaluating content selection in summarization has proven to be a difficult problem.
    Our approach acknowledges the fact that no single best model summary exists, and takes this as a foundation rather than an obstacle.
    In machine translation, the rankings from the automatic BLEU method (Papineni et al., 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003