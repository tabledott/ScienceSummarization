 Furthermore, transformationbased learning achieves state-of-the-art performance on several tasks, and is fairly resistant to overtraining (Ramshaw and Marcus, 1994).
    Despite its attractive features as a machine learning algorithm, TBL does have a serious drawback in its lengthy training time, especially on the larger-sized corpora often used in NLP tasks.
    For example, a well-implemented transformation-based part-of-speech tagger will typically take over 38 hours to finish training on a 1 million word corpus.
    This disadvantage is further exacerbated when the transformation-based learner is used as the base learner in learning algorithms such as boosting or active learning, both of which require multiple iterations of estimation and application of the base learner.
    In this paper, we present a novel method which enables a transformation-based learner to reduce its training time dramatically while still retaining all of its learning power.
    In addition, we will show that our method scales bett