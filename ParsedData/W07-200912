a (Brants and Franz, 2006) (IRST2, KU, 4We used 0.99 as the parameter for ? for this measure.
			SWAG1, SWAG2, USYD, UNT) to obtain counts fordisambiguation, with some using algorithms to derive domain (IRST1) or co-occurrence (TOR) infor mation from the BNC.
			Most systems did not use sense tagged data for disambiguation though MELBdid use SemCor (Miller et al, 1993b) for filtering in frequent synonyms and UNT used a semi-supervised word sense disambiguation combined with a host ofother techniques, including machine translation en gines.
	
	
			In tables 1 to 3 we have ordered systems according to R on the best task, and in tables 4 to 6 according to R on oot.
			We show all scores as per centages i.e. we multiply the scores in section 3 by 100.
			In tables 3 and 6 we show results using the subset of items which were i) NOT identified asmultiwords (NMWT) ii) scored only on non multi word substitutes from both annotators and systems (i.e. no spaces) (NMWS).
			Unfortunately we do not have space to show the 