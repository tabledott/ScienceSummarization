s, in addition to the performance comparison at various stages, we present also the overall system performance for the different scenarios.
  In all cases, the overall system performance is derived after the inference stage.
  4.1 Experimental Setting We use PropBank Sections 02 through 21 as training data, Section 23 as testing, and Section 24 as a validation set when necessary.
  In order to apply the standard CoNLL shared task evaluation script, our system conforms to both the input and output format defined in the shared task.
  The goal of the experiments in this section is to understand the effective contribu- tion of full parsing information versus shallow parsing information (i.e., using only the part-of-speech tags, chunks, and clauses).
  In addition, we also compare performance when using the correct (gold-standard) data versus using automatic parse data.
  The performance is measured in terms of precision, recall, and the F1 measure.
  Note that all the numbers reported here do not take into accou