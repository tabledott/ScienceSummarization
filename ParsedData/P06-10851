  Many previous methods for unsupervised word segmentation are based on the observation that transitions between units (characters, phonemes, or syllables) within words are generally more predictable than transitions across word boundaries.
    Statistics that have been proposed for measuring these differences include &#8220;successor frequency&#8221; (Harris, 1954), &#8220;transitional probabilities&#8221; (Saffran et al., 1996), mutual information (Sun et al., &#8727;This work was partially supported by the following grants: NIH 1R01-MH60922, NIH RO1-DC000314, NSF IGERT-DGE-9870676, and the DARPA CALO project.
    1998), &#8220;accessor variety&#8221; (Feng et al., 2004), and boundary entropy (Cohen and Adams, 2001).
    While methods based on local statistics are quite successful, here we focus on approaches based on explicit probabilistic models.
    Formulating an explicit probabilistic model permits us to cleanly separate assumptions about the input and properties of likely segmentations from details of