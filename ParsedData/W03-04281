sophisticated unknown word models are generally required for good performance.
    A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al., 1997, Bikel et al., 1997).
    One then treats the unknown word as a collection of such features.
    Having such unknown-word models as an add-on is perhaps a misplaced focus: in these tasks, providing correct behavior on unknown words is typically the key challenge.
    Here, we examine the utility of taking character sequences as a primary representation.
    We present two models in which the basic units are characters and character -grams, instead of words and word phrases.
    Earlier papers have taken a character-level approach to named entity recognition (NER), notably Cucerzan and Yarowsky (1999), which used prefix and suffix tries, though to our knowledge incorporating all character grams is new.
    In section 2, we discuss a character-level HMM, whil