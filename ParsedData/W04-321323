on of chunked data, while theirs (and other supervised results) are compared to manually annotated full sentences.
    Our percentage correct values therefore do not take into account argument constituents that are simply missed by the chunker.
    Table 3 summarizes our results.
    In this section, we focus on argument slots as identified by our human judges (the first panel of results in the table).
    There are a number of things to note.
    First, our performance on these slots is very high, 90.1% correct at the end of the algorithm, with 7.0% incorrect, and delimited arguments, others train, as well as test, only on such arguments.
    In our approach, all previously annotated slots are used in the iterative training of the probability model.
    Thus, even when we report results on argument slots only, adjunct and &#8220;bad&#8221; slots may have induced errors in their labelling. only 2.9% left unassigned.
    (The latter have null candidate lists.)
    This is a 56% reduction in error rate over the