f our model using exact (ILP) inference at test time, for increasing sets of features (see &#167;3.2&#8211;&#167;3.5).
    The rightmost column shows the results obtained with the full set of features using relaxed LP inference followed by projection onto the feasible set.
    Differences are with respect to exact inference for the same set of features.
    Bold indicates the best result for a language.
    As for overall performance, both the exact and relaxed full model outperform the arcfactored model and the second order model of McDonald and Pereira (2006) with statistical significance (p &lt; 0.01) according to Dan Bikel&#8217;s randomized method (http://www.cis.upenn.edu/-dbikel/software.html). tures and constraints can lead to further improvements on accuracy.
    We now turn to a different issue: scalability.
    In previous work (Martins et al., 2009), we showed that training the model via LP-relaxed inference (as we do here) makes it learn to avoid fractional solutions; as a consequence, ILP solver