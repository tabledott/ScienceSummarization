ause they are classic application of HMM.
    Because of paper length limit, unknown words recognition is omitted.
    Our previous papers (Zhang et al. 2003) gave more Given a word wi, classc i is defined in Figure 2.
    Suppose ILEXI to be the lexicon size, then the total number of word classes is ILEXI+9.
    Given the atom sequence A=(al,...an), let W=(wl,...wm) be the words sequence, C= (cl,...cm) be a corresponding class sequence of W, and W# be the choice of word segmentation with the maximized probability, respectively.
    Then, we could get: For a specific atom sequence A, P(A) is a constant and P(W,A)= P(W).
    So, On the basis of Baye's Theorem, it can be induced that: where co is begin of sentence.
    For convenience, we often use the negative log probability instead of the proper form.
    That is:
  
  
    We apply to word segmentation class-based HMM, which is a generalized approach covering both common words and unknown words. wi iff wi is listed in the segmentation lexicon; PER iff wi is