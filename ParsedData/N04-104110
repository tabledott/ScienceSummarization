taining the similarity between each pair of words ei and ej in c using the cosine coefficient of their mutual information vectors (Salton and McGill 1983): For each word e, we then cluster its most similar instances using group-average clustering (Han and Kamber 2001) and we store as a candidate committee the highest scoring cluster c' according to the following metric: where |c' |is the number of elements in c' and avgsim(c') is the average pairwise similarity between words in c'.
    The assumption is that the best representative for a concept is a large set of very similar instances.
    The committee for class c is then the highest scoring candidate committee containing only words from c. For example, below are the committee members discovered for the semantic classes (A), (B), and (C) from Section 1: equency count of all features of all words.
    Mutual information is commonly used to measure the association strength between two words (Church and Hanks 1989).
    A well-known problem is that mutual info