ory is rated from 1 to 4, with 4 being the best.
    A paraphrase identical to the source sentence would receive a score of 4 for meaning and 1 for dissimilarity and overall.
    We randomly selected 200 source sentences and generated 2 paraphrases for each, representing the two extremes: one paraphrase produced by the model trained with single parallel sentences, and the other by the model trained with all parallel sentences.
    The average scores of the two human judges are shown in Table 3.
    The results confirm our finding that the system trained with single parallel sentences preserves the meaning better but is also more conservative.
    Having established rough correspondences between BLEU/PINC scores and human judgments of semantic equivalence and lexical dissimilarity, we quantified the correlation between these automatic metrics and human ratings using Pearson&#8217;s correlation coefficient, a measure of linear dependence between two random variables.
    We computed the inter-annotator agreemen