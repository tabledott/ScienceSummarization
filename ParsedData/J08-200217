  Before we start presenting models, we describe the data and evaluation measures used in Section 3.
    Readers can skip the next section and continue on to Section 4 if they are not interested in the details of the evaluation.
  
  
    For most of our experiments we used the February 2004 release of Propbank.
    We also report results on the CoNLL 2005 shared task data (Propbank I) in Section 6.2.
    For the latter, we used the standard CoNLL evaluation measures, and we refer readers to the description of that task for details of the evaluation (Carreras and M`arquez 2005).
    In this section we describe the data and evaluation measures we used for the February 2004 data.
    We use our own set of measures on the February 2004 data for three reasons.
    Firstly, we wish to present a richer set of measures, which can better illustrate the performance of the system on core arguments as against adjuncts and the performance on identifying versus classifying arguments.
    Secondly, we technically could not