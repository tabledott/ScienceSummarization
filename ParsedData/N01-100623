epositional phrase attachment and text chunking.
    The results obtained indicate that the algorithmic improvement generated by our method is not linked to a particular task, but extends to any classification task where transformation-based learning can be applied.
    Furthermore, our algorithm scales better with training data size; therefore the relative speed-up obtained will increase when more samples are available for training, making the procedure a good candidate for large corpora tasks.
    The increased speed of the Fast TBL algorithm also enables its usage in higher level machine learning algorithms, such as adaptive boosting, model combination and active learning.
    Recent work (Florian et al., 2000) has shown how a TBL framework can be adapted to generate confidences on the output, and our algorithm is compatible with that framework.
    The stability, resistance to overtraining, the existence of probability estimates and, now, reasonable speed make TBL an excellent candidate for solving classi