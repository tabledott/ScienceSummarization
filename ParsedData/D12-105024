ifferent compositional methods, we create the following features: (a) a vector representing the pair of input sentences either via concatenation (&#8220;con&#8221;) or subtraction (&#8220;sub&#8221;); (b) a vector encoding which words appear therein (&#8220;enc&#8221;); and (c) a vector made up of the following four other pieces of information: the cosine similarity of the sentence vectors, the length of Seni1, the length of Seni2, and the unigram overlap among the two sentences.
    In order to encode which words appear in each sentence and how often, we define a vector wdCounti for sentence Seni and enumerate all words occuring in the MSRPC: giving the word count vectors nMSRPC dimensions.
    Thus the k-th component of wdCounti is the frequency with which the word w(MSRPC) appears in for k = 1,...,nMSRPC.
    Even though nMSRPC may be large, the computer files storing our feature vectors do not explode in size because wdCount contains many zeros and the classifier allows a sparse notation of (non-zero) fea