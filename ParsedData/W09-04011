of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics.
    The performance on each of these shared task was determined after a comprehensive human evaluation.
    There were a number of differences between this year&#8217;s workshop and last year&#8217;s workshop: Beyond ranking the output of translation systems, we evaluated translation quality by having people edit the output of systems.
    Later, we asked annotators to judge whether those edited translations were correct when shown the source and reference translation.
    The primary objectives of this workshop are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation methodologies for machine translation.
    All of the data, translations, and human judgments produced for our workshop are publicly available.1 We hope they form a valuable resource for re