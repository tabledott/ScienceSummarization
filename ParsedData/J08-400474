tructure.
    (Agreement was measured several times; initially, they obtained K = 0.87, and in the final analysis K = 0.97.)
    This, however, was achieved by employing experienced annotators, and with considerable training.
    One important reason why most agreement results on segmentation are on the lower end of the reliability scale is the fact, known to researchers in discourse analysis from as early as Levin and Moore (1978), that although analysts generally agree on the &#8220;bulk&#8221; of segments, they tend to disagree on their exact boundaries.
    This phenomenon was also observed in more recent studies: See for example the discussion in Passonneau and Litman (1997), the comparison of the annotations produced by seven coders of the same text in Figure 5 of Hearst (1997, page 55), or the discussion by Carlson, Marcu, and Okurowski (2003), who point out that the boundaries between elementary discourse units tend to be &#8220;very blurry.&#8221; See also Pevzner and Hearst (2002) for similar commen