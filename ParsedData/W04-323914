the computation explicitly in the feature space.
    The concept behind Boosting is that only a few hypotheses are needed to express the final solution.
    The l1-norm margin allows us to realize this property.
    Boosting thus finds a sparse solution in the feature space.
    The accuracies of these two methods depends on the given training data.
    However, we argue that Boosting has the following practical advantages.
    First, sparse hypotheses allow us to build an efficient classification algorithm.
    The complexity of SVMs with tree kernel is O(L'|N1||N2|), where N1 and N2 are trees, and L' is the number of support vectors, which is too heavy to realize real applications.
    Boosting, in contrast, runs faster, since the complexity depends only on the small number of decision stumps.
    Second, sparse hypotheses are useful in practice as they provide &#8220;transparent&#8221; models with which we can analyze how the model performs or what kind of features are useful.
    It is difficult to give s