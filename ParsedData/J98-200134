e is no agreement other than that expected by chance, K = 0.
    According to Carletta, in the field of content analysis&#8212; where the Kappa statistic originated&#8212;K &gt; 0.8 is generally taken to indicate good reliability, whereas 0.68 &lt; K &lt; 0.8 allows tentative conclusions to be drawn.
    We will illustrate the method for computing K proposed in Siegel and Castellan (1988) by means of an example from one of our texts, shown in Table 5.
    The first column in Table 5 (Definite description) shows the definite description being classified.
    The columns ASH, ASS, and LSU stand for the classification options presented to the subjects (anaphoric (same head), associative, and larger situation/unfamiliar, respectively).
    The numbers in each nil entry of the matrix indicate the number of classifiers that assigned the description in row i to the class in column j.
    The final column (labeled S) represents the percentage agreement for each definite description; we explain below how this percenta