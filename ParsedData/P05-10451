sks.
  
  
    Most statistical models currently used in natural language processing represent only local structure.
    Although this constraint is critical in enabling tractable model inference, it is a key limitation in many tasks, since natural language contains a great deal of nonlocal structure.
    A general method for solving this problem is to relax the requirement of exact inference, substituting approximate inference algorithms instead, thereby permitting tractable inference in models with non-local structure.
    One such algorithm is Gibbs sampling, a simple Monte Carlo algorithm that is appropriate for inference in any factored probabilistic model, including sequence models and probabilistic context free grammars (Geman and Geman, 1984).
    Although Gibbs sampling is widely used elsewhere, there has been extremely little use of it in natural language processing.1 Here, we use it to add non-local dependencies to sequence models for information extraction.
    Statistical hidden state sequence mo