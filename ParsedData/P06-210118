n of g about A&#8217;s mean &#181;A = E[A]: Here &#181;A = Pi &#181;ai and Q2A = Pi Q2ai, since A is a sum of independent random variables ai (i.e., given the current model parameters 0, our randomized decoder decodes each sentence independently).
    In other words, given our quadratic approximation to g, E[g(A)] depends on the (true) distribution of A only through the single-sentence means &#181;ai and variances a2ai, which can be found by enumerating the Ki decodings of sentence i.
    The approximation becomes arbitrarily good as we anneal -y &#8212;* oc, since then Q2A &#8212;* 0 and E[g(A)] focuses on g near &#181;A.
    For equation (8), and E[log C] is found similarly.
    Similar techniques can be used to compute the expected logarithms of some other non-linear metrics, such as F-measure (the harmonic mean of precision and recall)6 and Papineni et al. (2002)&#8217;s BLEU translation metric (the geometric mean of several precisions).
    In particular, the expectation of log BLEU distributes over its 