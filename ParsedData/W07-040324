e; by exploring more thoroughly, it is extracting more phrase pairs from the same amount of data.
    Both systems improve drastically with the addition of IBM Model 1 features for lexical preference.
    These features also narrow the gap between the two systems.
    To help calibrate the contribution of these features, we parameterized the ITG&#8217;s phrase table using only Model 1 features, which scores 27.17.
    Although ITG+M1 comes close, neither phrasal model matches the performance of the surface heuristic.
    Whatever the surface heuristic lacks in sophistication, it makes up for in sheer coverage, as demonstrated by its huge table sizes.
    Even the Phrasal ITG Viterbi alignments, which over-commit wildly and have horrible precision, score slightly higher than the best phrasal model.
    The surface heuristic benefits from capturing as much context as possible, while still covering smaller translation events with its flat counts.
    It is not held back by any lexicon constraints.
    When GIZA+