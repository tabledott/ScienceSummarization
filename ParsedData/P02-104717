course relations.
    To test this hypothesis, we decided to carry out a second experiment that used as predictors only a subset of the word pairs in the cartesian product defined over the words in two given text spans.
    To achieve this, we used the patterns in Table 2 to extract examples of discourse relations from the BLIPP corpus.
    As expected, the BLIPP corpus yielded much fewer learning cases: 185,846 CONTRAST; 44,776 CAUSE-EXPLANATION-EVIDENCE; 55,699 CONDITION; and 33,369 ELABORATION relations.
    To these examples, we added 58,000 NO-RELATION-SAME-TEXT and 58,000 NO-RELATION-DIFFERENT-TEXTS relations.
    To each text span in the BLIPP corpus corresponds a parse tree (Charniak, 2000).
    We wrote a simple program that extracted the nouns, verbs, and cue phrases in each sentence/clause.
    We call these the most representative words of a sentence/discourse unit.
    For example, the most representative words of the sentence in example (4), are those shown in italics.
    Italy&#8217;s unadjust