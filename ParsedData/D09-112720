akes are often caused by previous ones, so only the first mistake in each sentence (if there is one) is easily identifiable;7 this is also the argument for &#8220;early update&#8221; in applying perceptron learning to these incremental parsing algorithms (Collins and Roark, 2004) (see also Section 2).
    Among the 197 first mistakes (other 76 sentences have perfect output), the vast majority, 190 of them (96.4%), are shift-reduce errors (equally distributed between shift-becomesreduce and reduce-becomes-shift), and only 7 (3.6%) are due to reduce-reduce conflicts.8 These statistics confirm our intuition that shift-reduce decisions are much harder to make during parsing, and contribute to the overwhelming majority of errors, which is studied in the next hypothesis.
    Hypothesis 2 is verified in Table 7.
    We take the gold-standard shift-reduce sequence on the English dev set, and classify them into the four categories based on bilingual contiguity features: (a) c(st&#8722;1, st), i.e. whether the top 2 sp