
  Vector-based Models of Semantic Composition
  
    This paper proposes a framework for representing the meaning of phrases and sentences in vector space.
    Central to our approach is vector composition which we operationalize in terms of additive and multiplicative functions.
    Under this framework, we introduce a wide range of composition models which we evaluate empirically on a sentence similarity task.
    Experimental results demonstrate that the multiplicative models are superior to the additive alternatives when compared against human judgments.
  
  
    Vector-based models of word meaning (Lund and Burgess, 1996; Landauer and Dumais, 1997) have become increasingly popular in natural language processing (NLP) and cognitive science.
    The appeal of these models lies in their ability to represent meaning simply by using distributional information under the assumption that words occurring within similar contexts are semantically similar (Harris, 1968).
    A variety of NLP tasks have made good u