tactic constituents into other languages.
  
  
    In addition to scoring the shared task entries, we also continued on our campaign for improving the process of manual evaluation.
    We measured pairwise agreement among annotators using the kappa coefficient (K) which is widely used in computational linguistics for measuring agreement in category judgments (Carletta, 1996).
    It is defined as annotator agreement for the different types of manual evaluation where P(A) is the proportion of times that the annotators agree, and P(E) is the proportion of time that they would agree by chance.
    We define chance agreement for ranking tasks as s since there are three possible outcomes when ranking the output of a pair of systems: A &gt; B, A = B, A &lt; B, and for the Yes/No judgments as 2 since we ignored those items marked &#8220;Not Sure&#8221;.
    For inter-annotator agreement we calculated P(A) for the yes/no judgments by examining all items that were annotated by two or more annotators, and calculating 