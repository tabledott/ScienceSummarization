 kind of decision tree conditioned on word patterns as features.
    Finally, Ries (1999a) shows that neural networks using only unigram features can be superior to higher-order n-gram DA models.
    Warnke et al. (1999) and Ohler, Harbeck, and Niemann (1999) use related discriminative training algorithms for language models.
    Woszczyna and Waibel (1994) and Suhm and Waibel (1994), followed by ChuCarroll (1998), seem to have been the first to note that such a combination of word and dialogue n-grams could be viewed as a dialogue HMM with word strings as the observations.
    (Indeed, with the exception of Samuel, Carberry, and Vijay-Shanker (1998), all models listed in Table 14 rely on some version of this HMM metaphor.)
    Some researchers explicitly used HMM induction techniques to infer dialogue grammars.
    Woszczyna and Waibel (1994), for example, trained an ergodic HMM using expectation-maximization to model speech act sequencing.
    Kita et al. (1996) made one of the few attempts at unsupervised 