on the results.
    Models are evaluated by their labeled attachment score (LAS) on the test set, i.e., the percentage of tokens that are assigned both the correct head and the correct label, using the evaluation software from the CoNLL-X shared task with default settings.4 Statistical significance was assessed using Dan Bikel&#8217;s randomized parsing evaluation comparator with the default setting of 10,000 iterations.5
  
  
    Table 2 shows the results, for each language and on average, for the two base models (MST, Malt) and for the two guided models (MSTMlt, MaltMST).
    First of all, we see that both guided models show a very consistent increase in accuracy compared to their base model, even though the extent of the improvement varies across languages from about half a percentage point (MaltMST on Chinese) up to almost four percentage points (MaltMST on Slovene).6 It is thus quite clear that both models have the capacity to learn from features generated by the other model.
    However, it is also cle