e number of sentences is 10.4 and 11.5, respectively.
    For each set, we used 100 source articles with 20 randomly generated permutations for training.
    The same number of pairwise rankings (i.e., 2000) was used for testing.
    We held out 10 documents (i.e., 200 pairwise rankings) from the training data for development purposes.
    We further test the ability of our method to assess coherence by comparing model induced rankings against rankings elicited by human judges.
    Admittedly, the information ordering task only partially approximates degrees of coherence violation using different sentence permutations of a source document.
    A stricter evaluation exercise concerns the assessment of texts with naturally occurring coherence violations as perceived by human readers.
    A representative example of such texts are automatically generated summaries which often contain sentences taken out of context and thus display problems with respect to local coherence (e.g., dangling anaphors, thematically un