g majority of cases, the disambiguation decision is as trivial as string matching.
    In an attempt to generate more challenging data, we extracted 10,000 random paragraphs for which choosing the top disambiguation according to P(t|m) results in at least a 10% ranker error rate.
    40 paragraphs of this data was utilized for testing, while the remainder was used for training.
    The data sets are summarized in Table 2.
    The table shows the number of annotated mentions which were hyperlinked to non-null Wikipedia pages, and the number of titles in the documents (without counting repetitions).
    For example, the AQUAINT data set contains 727 mentions,4 all of which refer to distinct titles.
    The MSNBC data set contains 747 mentions mapped to non-null Wikipedia pages, but some mentions within the same document refer to the same titles.
    There are 372 titles in the data set, when multiple instances of the same title within one document are not counted.
    To isolate the performance of the individua