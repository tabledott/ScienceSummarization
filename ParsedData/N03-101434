best current parsers for this task, despite using a smaller vocabulary and less prior linguistic knowledge.
    The neural network architecture we use, Simple Synchrony Networks, not only allows us to avoid imposing hard independence assumptions, it also allows us to impose linguistically appropriate soft biases on the learning process.
    SSNs are specifically designed for processing structures, which allows us to design the SSN so that the induced representations of the parse history are biased towards recording structurally local information about the parse.
    When we modify these biases so that some structurally local information tends to be ignored, performance degrades.
    When we introduce independence assumptions by cutting off access to information from more distant parts of the structure, performance degrades dramatically.
    On the other hand, we find that biasing the learning to pay more attention to lexical heads does not improve performance.
  

