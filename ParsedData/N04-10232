sider the special issues of applying reranking techniques to the MT task and introduce two perceptron-like reranking algorithms for MT reranking.
    We provide experimental results that show that the proposed algorithms achieve start-of-the-art results on the NIST 2003 Chinese-English large data track evaluation.
    The seminal IBM models (Brown et al., 1990) were the first to introduce generative models to the MT task.
    The IBM models applied the sequence learning paradigm well-known from Hidden Markov Models in speech recognition to the problem of MT.
    The source and target sentences were treated as the observations, but the alignments were treated as hidden information learned from parallel texts using the EM algorithm.
    This sourcechannel model treated the task of finding the probability , where is the translation in the target (English) language for a given source (foreign) sentence , as two generative probability models: the language model which is a generative probability over candidate tran