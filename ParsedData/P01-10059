whether this decrease in complementarity meant that voting loses its effectiveness as the training set increases.
    To examine the impact of voting when using a significantly larger training corpus, we ran 3 out of the 4 learners on our set of 10 confusable pairs, excluding the memory-based learner.
    Voting was done by combining the normalized score each learner assigned to a classification choice.
    In Figure 3, we show the accuracy obtained from voting, along with the single best learner accuracy at each training set size.
    We see that for very small corpora, voting is beneficial, resulting in better performance than any single classifier.
    Beyond 1 million words, little is gained by voting, and indeed on the
  
  
    While the observation that learning curves are not asymptoting even with orders of magnitude more training data than is currently used is very exciting, this result may have somewhat limited ramifications.
    Very few problems exist for which annotated data of this size is avail