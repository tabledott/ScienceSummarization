icipants, interested volunteers, and a small number of paid annotators (recruited by the participating sites).
    More than 130 people participated in the manual evaluation, with 91 people putting in more than an hour&#8217;s worth of effort, and 29 putting in more than four hours.
    There was a collective total of 361 hours of labor.
    We asked annotators to evaluate system outputs by ranking translated sentences relative to each other.
    This was our official determinant of translation quality.
    The total number of judgments collected for the different ranking tasks is given in Table 6.
    We performed the manual evaluation of the individual systems separately from the manual evaluation of the system combination entries, rather than comparing them directly against each other.
    Last year&#8217;s results made it clear that there is a large (expected) gap in performance between the two groups.
    This year, we opted to reduce the number of pairwise comparisons with the hope that we would be more