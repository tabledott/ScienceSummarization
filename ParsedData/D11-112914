rue) and J0) (False).
  
  
    In this section we present a general scheme to build matrices for relational words.
    Recall that given a vector space A with basis {+-ni}i, the Kronecker product of two vectors -+v = E i can and = Ei cbi+-ni is defined as follows: i z -+ v &#174; &#65533;-+ w = caicbj (+-ni &#174; -+nj) ij where (+-ni &#174; +-nj) is just the pairing of the basis of A, i.e.
    (+-ni, n-+j).
    The Kronecker product vectors belong in the tensor product of A with itself: A &#174; A, hence if A has dimension r, these will be of dimensionality r xr.
    The point-wise multiplication of these vectors is defined as follows -+v O &#65533;-+ w = cai cbi ni -+ i The intuition behind having a matrix for a relational word is that any relation R on sets X and Y , i.e.
    R C_ X x Y can be represented as a matrix, namely one that has as row-bases x E X and as column-bases y E Y , with weight cxy = 1 where (x, y) E R and 0 otherwise.
    In a distributional setting, the weights, which are natural or re