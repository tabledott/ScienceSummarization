e number of model units is factored in.
    The selection of units with the same content is facilitated by the use of the Summary Evaluation Environment (SEE)2 developed at ISI, which displays the model and peer summary side by side and allows the user to make selections by using a mouse.
    There are numerous problems with the DUC human evaluation method.
    The use of a single model summary is one of the surprises &#8211; all research in summarization evaluation has indicated that no single good model exists.
    Also, since not much agreement is expected between two summaries, many model units will have no counterpart in the peer and thus the expected scores will necessarily be rather low.
    Additionally, the task of determining the percentage overlap between two text units turns out to be difficult to annotate reliably &#8211; (Lin and Hovy, 2002) report that humans agreed with their own prior judgment in only 82% of the cases.
    These methodological anomalies lead to unreliable scores.
    Human-wr