ature grammar for DAs, to be used in improving speech recognition.
    The idea caught on very quickly: Suhm and Waibel (1994), Mast et al. (1996), Warnke et al.
    (1997), Reithinger and Klesen (1997), and Taylor et al. (1998) all use variants of backoff, interpolated, or class n-gram language models to estimate DA likelihoods.
    Any kind of sufficiently powerful, trainable language model could perform this function, of course, and indeed Alexandersson and Reithinger (1997) propose using automatically learned stochastic context-free grammars.
    Jurafsky, Shriberg, Fox, and Curl (1998) show that the grammar of some DAs, such as appreciations, can be captured by finite-state automata over part-of-speech tags.
    N-gram models are likelihood models for DAs, i.e., they compute the conditional probabilities of the word sequence given the DA type.
    Word-based posterior probability estimators are also possible, although less common.
    Mast et al. (1996) propose the use of semantic classification trees, a