measures.
    The bigram probability scores highest out of all the measures, while the score performed the worst.
    The results on the new corpus are even more surprising, with the adjacency model outperforming the dependency model by a wide margin.
    The measure gives the highest accuracy, but still only just outperforms the raw counts.
    Our analysis shows that the good performance of the adjacency model comes from the large number of named entities in the corpus.
    When we remove all items that have any word as an entity, the results change, and the dependency model is superior.
    We also suspect that another cause of the unusual results is the different proportions of left and right-branching NPs.
    With a large annotated corpus, we can now run supervised NP bracketing experiments.
    We present two configurations in Table 7: training on our corpus and testing on Lauer&#8217;s set; and performing 10-fold cross validation using our corpus alone.
    The feature set we explore encodes the infor