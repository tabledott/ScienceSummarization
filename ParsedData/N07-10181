) procedure (Lari andYoung, 1990).
			However, much recent work in ma chine learning and statistics has turned away from maximum-likelihood in favor of Bayesian methods, and there is increasing interest in Bayesian methods in computational linguistics as well (Finkel et al, 2006).
			This paper presents two Markov chain Monte Carlo (MCMC) algorithms for inferring PCFGs and their parses from strings alone.
			These can be viewed as Bayesian alternatives to the IO algorithm.
			The goal of Bayesian inference is to compute a distribution over plausible parameter values.
			This ?posterior?
			distribution is obtained by combining thelikelihood with a ?prior?
			distribution P(?)
			over pa rameter values ?.
			In the case of PCFG inference ? is the vector of rule probabilities, and the prior mightassert a preference for a sparse grammar (see be low).
			The posterior probability of each value of ? is given by Bayes?
			rule: P(?|D) ? P(D|?)P(?).
			(1)In principle Equation 1 defines the posterior prob ability of