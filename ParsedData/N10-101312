third edition English Gigaword corpus, with articles containing fewer than 100 words removed, leaving 6.6M articles and 3.913 words (Graff, 2003).
    Wikipedia covers a wider range of sense distributions, whereas Gigaword contains only newswire text and tends to employ fewer senses of most ambiguous words.
    Our method outperforms baseline methods even on Gigaword, indicating its advantages even when the corpus covers few senses.
    To evaluate the quality of various models, we first compared their lexical similarity measurements to human similarity judgements from the WordSim353 data set (Finkelstein et al., 2001).
    This test corpus contains multiple human judgements on 353 word pairs, covering both monosemous and polysemous words, each rated on a 1&#8211;10 integer scale.
    Spearman&#8217;s rank correlation (p) with average human judgements (Agirre et al., 2009) was used to measure the quality of various models.
    Figure 2 plots Spearman&#8217;s p on WordSim-353 against the number of clusters (K)