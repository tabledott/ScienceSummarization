obtained from an MTurk worker&#8217;s labels are comparisons for which we already have an expert judgment.
    When we calculate the rate of agreement on this data, we find that MTurk workers agree with the expert workers 53.2% of the time, or K = 0.297, and when references are excluded, the agreement rate is 50.0%, or K = 0.249.
    Ideally, we would want those values to be in the 0.4&#8211;0.5 range, since that is where the inter-annotator kappa coefficient lies for the expert annotators.
    We can use the agreement rate with experts to identify MTurk workers who are not performing the task as required.
    For each worker w of the 669 workers for whom we have such data, we compute the worker&#8217;s agreement rate with the experts, and from it a kappa coefficient Kexp(w) for that worker.
    (Given that P(E) is 0.333, Kexp(w) ranges between &#8722;0.5 and +1.0.)
    We sort the workers based on Kexp(w) in ascending order, and examine properties of the MTurk data as we remove the lowest-ranked workers one 