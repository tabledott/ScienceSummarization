sequence filtering was used (FTR) and whether it was used in conjunction with the baseline trigram (+EP-KN-3).
    Our first set of experiments examines the relationship between memory allocated to the BF and BLEU score.
    We present results using the Boolean BFLM in isolation and then both the Boolean and logfrequency BF-LMS to add 4-grams to our baseline 3-gram model.Our second set of experiments adds 3-grams and 5-grams from the Gigaword Corpus to our baseline.
    Here we constrast the Boolean BFLM with the log-frequency BF-LM with different quantisation bases (2 = fine-grained and 5 = coarsegrained).
    We then evaluate the sub-sequence filtering approach to reducing the actual error rate of these models by adding both 3 and 4-grams from the Gigaword Corpus to the baseline.
    Since the BF-LMs easily allow us to deploy very high-order n-gram models, we use them to evaluate the impact of different order n-grams on the translation process presenting results using the Boolean and log-frequency BF-LM in 