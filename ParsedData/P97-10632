he method assumes that words are translated one-to-one.
    This assumption reduces the explanatory power of our model in comparison to the IBM models, but, as shown in Section 3.1, it helps us to avoid what we call indirect associations, a major source of errors in other models.
    Section 3.1 also shows how the oneto-one assumption enables us to use a new greedy competitive linking algorithm for re-estimating the model's parameters, instead of more expensive algorithms that consider a much larger set of word correspondence possibilities.
    The model uses two hidden parameters to estimate the confidence of its own predictions.
    The confidence estimates enable direct control of the balance between the model's precision and recall via a simple threshold.
    The hidden parameters can be conditioned on prior knowledge about the bitext to improve the model's accuracy.
  
  
    With the exception of (Fung, 1995b), previous methods for automatically constructing statistical translation models begin by looki