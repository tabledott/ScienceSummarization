 while performance is relatively consistent across corpora, it varies widely with respect to training set size and n-gram order.
    The method interp-del-int performs significantly worse than interp-held-out, though they differ only in the data used to train the A's.
    However, we delete one word at a time in int erp-del-int; we hypothesize that deleting larger chunks would lead to more similar performance.
    In Figure 7, we show how the values of the parameters 6 and emir, affect the performance of methods katz and new-avg-count, respectively, over several training data sizes.
    Notice that poor parameter setting can lead to very significant losses in performance, and that optimal parameter settings depend on training set size.
    To give an informal estimate of the difficulty of implementation of each method, in Table 1 we display the number of lines of C++ code in each implementation excluding the core code common across techniques.
  
  
    To our knowledge, this is the first empirical comparison