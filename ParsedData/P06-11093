This raises the question whether we can create an unsupervised DOP model which is also statistically consistent.
    In this paper we will show that an unsupervised version of ML-DOP can be constructed along the lines of U-DOP.
    We will start out by summarizing DOP, U-DOP and ML-DOP, and next create a new unsupervised model called UML-DOP.
    We report that UML-DOP not only obtains higher parse accuracy than U-DOP on three different domains, but that it also achieves this with fewer subtrees than U-DOP.
    To the best of our knowledge, this paper presents the first unsupervised parser that outperforms a widely used supervised parser on the WSJ, i.e. a treebank PCFG.
    We will raise the question whether the end of supervised parsing is in sight.
  
  
    The key idea of DOP is this: given an annotated corpus, use all subtrees, regardless of size, to parse new sentences.
    The DOP1 model in Bod (1998) computes the probabilities of parse trees and sentences from the relative frequencies of the subtrees