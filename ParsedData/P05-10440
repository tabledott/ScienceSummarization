
  Contrastive Estimation: Training Log-Linear Models On Unlabeled Data
  
    Conditional random fields (Lafferty et al., 2001) are quite effective at sequence labeling tasks like shallow parsing (Sha and Pereira, 2003) and namedentity extraction (McCallum and Li, 2003).
    CRFs allowing the incorporation of arbifeatures into the model.
    To train on we require methods for log-linear models; few exist.
    We describe a novel We show that the new technique can be intuitively understood as exnegative evidence is computationally efficient.
    Applied to a sequence labeling problem&#8212;POS tagging given a tagging dictionary and unlabeled text&#8212;contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features.
  
  
    Finding linguistic structure in raw text is not easy.
    The classical forward-backward and inside-outside algorithms try to guide probabilistic models to discover structure in t