most if not all of these requirements* Nevertheless, when appropriately quantified and converted into concrete test measures, such requirements can set an overall standard by which different MT evaluation metrics can be compared and evaluated* In this paper, we describe METEOR', an automatic metric for MT evaluation which we have been developing* METEOR was designed to explicitly address several observed weaknesses in IBM's BLEU metric* It is based on an explicit word-to-word matching between the MT output being evaluated and one or more reference translations* Our current matching supports not only matching between words that are identical in the two strings being compared, but can also match words that are simple morphological variants of each other (i*e* they have an identical stem), and words that are synonyms of each other* We envision ways in which this strict matching can be further expanded in the future, and describe these at the end of the paper* Each possible matching is scored based on a combinati