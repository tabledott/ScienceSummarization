 a particular word or word-sequence, the improvements will carry over to occurrences of similar words and sequences.
    By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which may be used for other, potentially unrelated, tasks.
    This has been used to good effect, for example in (Collobert and Weston, 2008; Turian et al., 2010) where induced word representations are used with sophisticated classifiers to improve performance in many NLP tasks.
    In this work, we find that the learned word representations in fact capture meaningful syntactic and semantic regularities in a very simple way.
    Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship.
    For example, if we denote the vector for word i as xi, and focus on the singular/plural relation, we observe that x apple&#8722;xapples &#8776; xcar&#8722;xcars, xfamily&#8722;xfamilies &#8776; xcar&#8722;xcars