el generation probabilities, along with examples from Figure 1.
    For brevity, we omit the smoothing details of BBN's model (see (Miller et al., 1998) for a complete description); we note that all smoothing weights are computed via the technique described in (Bikel et al., 1997).
    The probability of generating p as the root label is predicted conditioning on only +TOP+, which is the hidden root of all parse trees: The probability of generating a head node h with a parent p is The probability of generating a left-modifier ii is when generating the NP for NP(Apple&#8212;NNP), and the probability of generating a right modwhen generating the NP for NP(Microsoft&#8212; NNP).1 The probabilities for generating lexical elements (part-of-speech tags and words) are as follows.
    The part of speech tag of the head of the entire sentence, th, is computed conditioning only on the top-most symbol p:2 Part of speech tags of modifier constituents, and tri, are predicted conditioning on the modifier constituent i or ri