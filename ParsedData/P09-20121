tion, or maximum likelihood estimators which tend to extract large subtrees that overfit the training data.
    These problems are common in natural language processing tasks that search for a hidden segmentation.
    Recently, many groups have had success using Gibbs sampling to address the complexity issue and nonparametric priors to address the overfitting problem (DeNero et al., 2008; Goldwater et al., 2009).
    In this paper we apply these techniques to learn a tree substitution grammar, evaluate it on the Wall Street Journal parsing task, and compare it to previous work.
  
  
    TSGs extend CFGs (and their probabilistic counterparts, which concern us here) by allowing nonterminals to be rewritten as subtrees of arbitrary size.
    Although nonterminal rewrites are still context-free, in practice TSGs can loosen the independence assumptions of CFGs because larger rules capture more context.
    This is simpler than the complex independence and backoff decisions of Markovized grammars.
    Furthermore,