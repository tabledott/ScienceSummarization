rs in using Wikipedia revisions to learn interesting types of directional lexical relations, e.g, &#8220;eggcorns&#8221;3 [7] and entailments [8].
  
  
    As mentioned above, a key idea in our work is to utilize SimpleEW edits.
    The primary difficulty in working with these modifications is that they include not only simplifications but also edits that serve other functions, such as spam removal or correction of grammar or factual content (&#8220;fixes&#8221;).
    We describe two main approaches to this problem: a probabilistic model that captures this mixture of different edit operations (&#167;2.1), and the use of metadata to filter out undesirable revisions (&#167;2.2).
    We say that the kth article in a Wikipedia corresponds to (among other things) a title or topic (e.g., ~ &#8220;Cat&#8221;) and a sequence dk of article versions caused by successive edits.
    For a given lexical item or ~ phrase A, we write A E dk if there is any version ~ ~ in dk that contains A.
    From each dk we extract a co