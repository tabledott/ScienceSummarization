ter models used an extended set of 15 DAs.
    Examples are from the English translations given by Nagata (1992).
    The use of n-grams to model the probabilities of DA sequences, or to predict upcoming DAs on-line, has been proposed by many authors.
    It seems to have been first employed by Nagata (1992), and in follow-up papers by Nagata and Morimoto (1993, 1994) on the ATR Dialogue database.
    The model predicted upcoming DAs by using bigrams and trigrams conditioned on preceding DAs, trained on a corpus of 2,722 DAs.
    Many others subsequently relied on and enhanced this n-grams-of-DAs approach, often by applying standard techniques from statistical language modeling.
    Reithinger et al. (1996), for example, used deleted interpolation to smooth the dialogue n-grams.
    Chu-Carroll (1998) uses knowledge of subdialogue structure to selectively skip previous DAs in choosing conditioning for DA prediction.
    Nagata and Morimoto (1993, 1994) may also have been the first to use word ngrams as a mini