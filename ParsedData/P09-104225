 penalties based on posterior expectations of features not necessarily in the model in order to guide learning.
    They call their method generalized expectation constraints or alternatively expectation regularization.
    In this volume (Druck et al., 2009) use this framework to train a dependency parser based on constraints stated as corpus-wide expected values of linguistic rules.
    The rules select a class of edges (e.g. auxiliary verb to main verb) and require that the expectation of these be close to some value.
    The main difference between this work and theirs is the source of the information (a linguistic informant vs. cross-lingual projection).
    Also, we define our regularization with respect to inequality constraints (the model is not penalized for exceeding the required model expectations), while they require moments to be close to an estimated value.
    We suspect that the two learning methods could perform comparably when they exploit similar information.
  
  
    In this paper, we pro