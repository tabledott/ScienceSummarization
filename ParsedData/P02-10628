as trained on 4/5 of the data, then used to decode the remaining 1/5.
    The top 20 hypotheses under a beam search, together with their log probabilities, were recovered for each training sentence.
    In a similar way, a model trained on the 41,992 sentence set was used to produce 20 hypotheses for each sentence in the development set. generator, and hashes them to integers.
    For example, suppose the three strings WE=Gen Xer, WE=The Day They Shot John Lennon, WE=Dougherty Arts Center were hashed to 100, 250, and 500 respectively.
    Conceptually, the candidate is represented by a large number of features for where is the number of distinct feature strings in training data.
    In this example, only take the value, all other features being zero.
    We now introduce some notation with which to describe the full set of global features.
    First, we assume the following primitives of an input candidate: for is the&#8217;th tag in the tagged sequence.
    The module we describe in this section generates gl