edia,3 about 820 million tokens; and (3) the British National Corpus,4 about 95 million tokens.
    The resulting concatenated corpus was tokenized, POS-tagged, and lemmatized with the TreeTagger5 and dependency-parsed with the MaltParser.6 It contains about 2.83 billion tokens.
    The ukWaC and Wikipedia sections can be freely downloaded, with full annotation, from the ukWaC corpus site.
    For all our models, the label sets W1 = W2 contain 30,693 lemmas (20,410 nouns, 5,026 verbs, and 5,257 adjectives).
    These terms were selected based on their frequency in the corpus (they are approximately the top 20,000 most frequent nouns and top 5,000 most frequent verbs and adjectives), augmenting the list with lemmas that we found in various standard test sets, such as the TOEFL and SAT lists.
    In all models, the words are stored in POS-suffixed lemma form.
    The weighted tuple structures differ for the choice of links in L and/or for the scoring function &#963;. DepDM.
    Our first DM model relies on the 