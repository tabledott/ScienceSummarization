91; West and Stanovich, 1986) and modulate cognitive behavior in sentence priming (Till et al., 1988) and inference tasks (Heit and Rubinstein, 1994).
    Computational models of semantics which use symbolic logic representations (Montague, 1974) can account naturally for the meaning of phrases or sentences.
    Central in these models is the notion of compositionality &#8212; the meaning of complex expressions is determined by the meanings of their constituent expressions and the rules used to combine them.
    Here, semantic analysis is guided by syntactic structure, and therefore sentences (1-a) and (1-b) receive distinct representations.
    The downside of this approach is that differences in meaning are qualitative rather than quantitative, and degrees of similarity cannot be expressed easily.
    In this paper we examine models of semantic composition that are empirically grounded and can represent similarity relations.
    We present a general framework for vector-based composition which allows us to 