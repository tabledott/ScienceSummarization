ent to warrant the use of a metaclassifier.
    In these experiments there is a clear difference between prepositions and articles.
    We can reduce the amount of training data for prepositions to 10% of the original data and still outperform the language model baseline.
    10% of the data corresponds to 6,800 annotated preposition errors and 58,400 sentences.
    When we reduce the training data to 1% of the original amount (680 annotated errors, 5,800 sentences) we clearly see degraded results compared to the language model.
    With articles, the system is much less data-hungry.
    Reducing the training data to 1% (580 annotated errors, 7,400 sentences) still outperforms the language model alone.
    This result can most likely be explained by the different complexity of the preposition and article tasks.
    Article operations include only six distinct operations: deletion of the, deletion of a/an, insertion of the, insertion of a/an, change of the to a/an, and change of a/an to the.
    For the twelve