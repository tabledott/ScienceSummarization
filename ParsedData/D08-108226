om Table 5, our model is able to achieve performance comparable to WASP as reported by Wong (2007). ments on this paper.
    The research is partially supported by ARF grant R-252-000-240-112.
    Our model is generic, which requires no domaindependent knowledge and should be applicable to a wide range of different domains.
    Like all research in this area, the ultimate goal is to scale to more complex, open-domain language understanding problems.
    In future, we would like to create a larger corpus in another domain with multiple natural language annotations to further evaluate the scalability and portability of our approach.
  
  
    We presented a new generative model that simultaneously produces both NL sentences and their corresponding MR structures.
    The model can be effectively applied to the task of transforming NL sentences to their MR structures.
    We also developed a new dynamic programming algorithm for efficient training and decoding.
    We demonstrated that this approach, augmented wi