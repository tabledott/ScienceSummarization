g, but is beginning to adopt other methods from the machine learning community, such as Gibbs sampling, rejection sampling, and certain variational approximations.
    We propose borrowing a different approximation technique from machine learning, namely, loopy belief propagation (BP).
    In this paper, we show that BP can be used to train and decode complex parsing models.
    Our approach calls a simpler parser as a subroutine, so it still exploits the useful, well-studied combinatorial structure of the parsing problem.1
  
  
    We wish to make a dependency parse&#8217;s score depend on higher-order features, which consider arbitrary interactions among two or more edges in the parse (and perhaps also other latent variables such as part-of-speech tags or edge labels).
    Such features can help accuracy&#8212;as we show.
    Alas, they raise the polynomial runtime of projective parsing, and render non-projective parsing NP-hard.
    Hence we seek approximations.
    We will show how BP&#8217;s &#8220;mess