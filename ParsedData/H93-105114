ive or neg- ative, thereby allowing it to accumulate vidence both for and against a sense.
  The result of training a network until all examples axe classified correctly is that infrequent okens can acquire disproportionate importance.
  For example, the context Fine, Henderson said, aimiably [sic].
  Can 7/ou get hint on the liner clearly uses line in the phone sense.
  How- ever, the only non-stopwords that are infrequent in other senses are henderson and aimiably; and, due to its misspelling, the latter is conflated to aim.
  The net- work must raise the weight of henderson so that it is sufficient o give phone the largest output.
  As a result, henderson appears in Table 1, in spite of its infrequency in the training corpus.
  To determine a good topology for the network, various network topologies were explored: networks with from 0 to 100 hidden units arranged in a single hidden layer; networks with multiple layers of hidden units; and net- works with a single layer of hidden units in which the output u