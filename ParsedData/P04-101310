r to apply standard probability estimation methods, we use neural networks to induce finite representations of both these sequences, which we will denote h(d1,..., di&#8722;1) and l(yield(di,..., dm)), respectively.
    The neural network training methods we use try to find representations which preserve all the information about the sequences which are relevant to estimating the desired probabilities.
    Of the previous work on using neural networks for parsing natural language, by far the most empirically successful has been the work using Simple Synchrony Networks.
    Like other recurrent network architectures, SSNs compute a representation of an unbounded sequence by incrementally computing a representation of each prefix of the sequence.
    At each position i, representations from earlier in the sequence are combined with features of the new position i to produce a vector of real valued features which represent the prefix ending at i.
    This representation is called a hidden representation.
    It i