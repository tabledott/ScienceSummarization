ngular Value Decomposition to produce a rankreduced approximation of an original matrix of word and document frequencies.
    We applied this technique to all documents in the training corpus (as opposed to components), reduced the rank to 100, then calculated the projections of the component and document vectors described in the previous paragraph into the reduced space.
    Perplexity (Jelinek, 1997) is a standard way of evaluating the quality of a language model on a test text.
    We define a perplexity-based distance metric pc(q)1/|q|, where pc(q) is the probability assigned to q by an ngram language model trained on component c. The final distance metric, which we call EM, is based on expressing the probability of q as a wordlevel mixture model: p(q) = &#65533;|q |Ec dcpc(wi|hi), where q = wl ... w|q|, and pc(w|h) is the ngram probability of w following word sequence h in component c. It is straighforward to use the EM algorithm to find the set of weights &#65533;dc, Vc that maximizes the likelihood of 