determine which categories are crucial for achieving this performance gain.
    This was done by training models that use sister-head dependencies for all categories but one.
    Table 6 shows the change in LR and LP that was found for each individual category (again for TnT tags and perfect tags).
    The highest drop in performance (around 3%) is observed when the PP category is reverted to head-head dependencies.
    For S and for the coordinated categories (CS, CNP, etc.
    ), a drop in performance of around 1% each is observed.
    A slight drop is observed also for VP (around 0.5%).
    Only minimal fluctuations in performance are observed when the other categories are removed (AP, AVP, and NP): there is a small effect (around 0.5%) if TnT tags are used, and almost no effect for perfect tags.
    We showed that splitting PPs to make Negra less flat does not improve parsing performance if testing is carried out on the collapsed categories.
    However, we observed that LR and LP are artificially inflate