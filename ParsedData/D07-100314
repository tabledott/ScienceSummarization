ion for modeling p(q | a) is that it is easier to model deletion of information (such as the part of the sen tence that answers the question) than insertion.
			Our QG doesnot model the real-world knowledge required to fill in an an swer; its job is to know what answers are likely to look like, syntactically.
			words.
			In our model, each question-word maps to exactly one answer-word.
			Let x : {1, ..., n} ? {1, ...,m} be a mapping from indices of words in qto indices of words in a.
			(It is for computational rea sons that we assume |x(i)| = 1; in general x couldrange over subsets of {1, ...,m}.)
			Because we de fine the correspondence in this direction, note that it is possible for multple question words to map to the same answer word.Why do we treat the alignmentX as a hidden vari able?
			In prior work, the alignment is assumed to be known given the sentences, but we aim to discoverit from data.
			Our guide in this learning is the struc ture inherent in the QG: the configurations betweenparent-child 