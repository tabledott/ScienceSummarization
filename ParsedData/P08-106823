5; Petrov et al., 2006; Titov and Henderson, 2007).
    These approaches have the advantage that the model is able to learn different usages for the hidden variables, depending on the target problem at hand.
    Crucially, however, these methods do not exploit unlabeled data when learning their representations.
    Wang et al. (2005) used distributional similarity scores to smooth a generative probability model for dependency parsing and obtained improvements in a Chinese parsing task.
    Our approach is similar to theirs in that the Brown algorithm produces clusters based on distributional similarity, and the clusterbased features can be viewed as being a kind of &#8220;backed-off&#8221; version of the baseline features.
    However, our work is focused on discriminative learning as opposed to generative models.
    Semi-supervised phrase structure parsing has been previously explored by McClosky et al. (2006), who applied a reranked parser to a large unsupervised corpus in order to obtain additional traini