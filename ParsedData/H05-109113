n example avector with a position for each dependency path fea ture is infeasible, due to the high dimensionality ofthe feature space.
			Here we can exploit dual learning algorithms that process examples only via computing their dot-products, such as the Support Vec tor Machines (SVMs) (Vapnik, 1998; Cristianiniand Shawe-Taylor, 2000).
			These dot-products be tween feature vectors can be efficiently computed through a kernel function, without iterating over allthe corresponding features.
			Given the kernel func tion, the SVM learner tries to find a hyperplane that separates positive from negative examples and at thesame time maximizes the separation (margin) be tween them.
			This type of max-margin separator hasbeen shown both theoretically and empirically to re sist overfitting and to provide good generalization performance on unseen examples.
			Computing the dot-product (i.e. kernel) between two relation examples amounts to calculating the 727 number of common features of the type illustrated in Table 