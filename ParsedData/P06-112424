 once (on average) and better on other words, while HPYCV is reversed and performs better than MKN on words that occurred only once or twice and worse on other words.
  
  
    We have described using a hierarchical PitmanYor process as a language model and shown that it gives performance superior to state-of-the-art methods.
    In addition, we have shown that the state-of-the-art method of interpolated KneserNey can be interpreted as approximate inference in the hierarchical Pitman-Yor language model.
    In the future we plan to study in more detail the differences between our model and the variants of Kneser-Ney, to consider other approximate inference schemes, and to test the model on larger data sets and on speech recognition.
    The hierarchical Pitman-Yor language model is a fully Bayesian model, thus we can also reap other benefits of the paradigm, including having a coherent probabilistic model, ease of improvements by building in prior knowledge, and ease in using as part of more complex models; w