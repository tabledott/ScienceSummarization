 perform well on language tasks such as search.
    Furthermore, an underlying domain of objects and a valuation function must be provided, as with any logic, leaving open the question of how we might learn the meaning of language using such a model, rather than just use it.
    Distributional Models Distributional models of semantics, on the other hand, dismiss the interaction between syntactically linked words and are solely concerned with lexical semantics.
    Word meaning is obtained empirically by examining the contexts1 in which a word appears, and equating the meaning of a word with the distribution of contexts it shares.
    The intuition is that context of use is what we appeal to in learning the meaning of a word, and that words that frequently have the same sort of context in common are likely to be semantically related.
    For instance, beer and sherry are both drinks, alcoholic, and often cause a hangover.
    We expect these facts to be reflected in a sufficiently large corpus: the words &#821