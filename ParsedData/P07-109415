this corpus have at least two possible tags, with the average number of tags per token being 2.3.
    We varied the values of the hyperparameters &#945; and Q and evaluated overall tagging accuracy.
    For comparison with our Bayesian HMM (BHMM) in this and following sections, we also present results from the Viterbi decoding of an HMM trained using MLE by running EM to convergence (MLHMM).
    Where direct comparison is possible, we list the scores reported by Smith and Eisner (2005) for their conditional random field model trained using contrastive estimation (CRF/CE).2 For all experiments, we ran our Gibbs sampling algorithm for 20,000 iterations over the entire data set.
    The algorithm was initialized with a random tag assignment and a temperature of 2, and the temperature was gradually decreased to .08.
    Since our inference procedure is stochastic, our reported results are an average over 5 independent runs.
    Results from our model for a range of hyperparameters are presented in Table 1.
    Wi