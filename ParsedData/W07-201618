 ? the systems could use only the an notated data provided and nothing else.
			ii) Open ?where systems could use PropBank data from Sec tions 02-21, as well as any other resource for training their labelers.
			3.1 Data.
			We selected 50 verbs from the 65 in the lexical sam ple task for the SRL task.
			The partitioning into train and test set was done in the same fashion as for the lexical sample task.
			Since PropBank does not tag any noun predicates, none of the 35 nouns from the lexical sample task were part of this data.
			3.2 Results.
			For each system, we calculated the precision, re call, and F-measure for both role label sets.
			Scores were calculated using the srl-eval.pl script from the CoNLL-2005 scoring package (Carreras and Ma`rquez, 2005).
			Only two teams chose to perform the SRL subtask.
			The performance of these two teams is shown in Table 5 and Table 6.
			91 System Type Precision Recall F UBC-UPC Open 85.31 82.08 83.66?0.5 UBC-UPC Closed 85.31 82.08 83.66?0.5 RTV Closed 81.58 70.1