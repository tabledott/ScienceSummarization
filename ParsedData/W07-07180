
  (Meta-) Evaluation of Machine Translation
  
    j schroeder ed ac uk Abstract This paper evaluates the translation quality of machine translation systems for 8 language pairs: translating French, German, Spanish, and Czech to English and back.
    We carried out an extensive human evaluation which allowed us not only to rank the different MT systems, but also to perform higher-level analysis of the evaluation process.
    We measured timing and intraand inter-annotator agreement for three types of subjective evaluation.
    We measured the correlation of automatic evaluation metrics with human judgments.
    This meta-evaluation reveals surprising facts about the most commonly used methodologies.
  
  
    This paper presents the results for the shared translation task of the 2007 ACL Workshop on Statistical Machine Translation.
    The goals of this paper are twofold: First, we evaluate the shared task entries in order to determine which systems produce translations with the highest quality.
    Second, 