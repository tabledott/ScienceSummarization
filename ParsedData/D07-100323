f documents returned byparticipating teams.
			Our task is the same as Pun yakanok et al (2004) and Cui et al (2005), where we search for single-sentence answers to factoid questions.
			We follow a similar setup to Shen and Klakow (2006) by automatically selecting answer candidate sentences and then comparing against a human-judged gold standard.
			We used the questions in TREC 8?12 for training and set aside TREC 13 questions for development(84 questions) and testing (100 questions).
			To gen erate the candidate answer set for development and testing, we automatically selected sentences from each question?s document pool that contains one ormore non-stopwords from the question.
			For gen erating the training candidate set, in addtion to thesentences that contain non-stopwords from the ques tion, we also added sentences that contain correct tributions; ? is initialized to be 0.1.
			8We thank the organizers and NIST for making the dataset publicly available.
			27 answer pattern.
			Manual judgement was p