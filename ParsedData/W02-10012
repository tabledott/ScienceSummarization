 paper describes variants of the perceptron algorithm for tagging problems.
			The al gorithms rely on Viterbi decoding of trainingexamples, combined with simple additive updates.
			We describe theory justifying the algorithm through a modi cation of the proof of convergence of the perceptron algorithm for classi cation problems.
			We give experimental results on part-of-speech tagging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger (a11.9% relative reduction in error for POS tag ging, a 5.1% relative reduction in error for NP chunking).
			Although we concentrate on taggingproblems in this paper, the theoretical frame work and algorithm described in section 3 ofthis paper should be applicable to a wide va riety of models where Viterbi-style algorithmscan be used for decoding: examples are Proba bilistic Context-Free Grammars, or ME models for parsing.
			See (Collins and Duy 2001; Collinsand Duy 2002; Collins 2002) for other applica tions of the v