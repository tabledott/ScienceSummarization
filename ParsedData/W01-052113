e parameters.
    Results for different values of 0 are shown in Table 4.
    The complete parsing model derived from the WSJ training set has 735,850 parameters in a total of nine distributions: three levels of backoff for each of the three distributions Ph, P, and P,,,,.
    The lexical bigrams are contained in the most specific distribution for P,,,,.
    Removing all these parameters reduces the total model size by 43%.
    The results show a gradual degradation as more parameters are pruned.
    The ten lexical bigrams with the highest scores for the pruning metric are shown in Table 5 for WSJ and Table 6.
    The pruning metric of equation 3 has been normalized by corpus size to allow comparison between WSJ and Brown.
    The only overlap between the two sets is for pairs of unknown word tokens.
    The WSJ bigrams are almost all specific to finance, are all word pairs that are likely to appear immediately adjacent to one another, and are all children of the base NP syntactic category.
    The Brown big