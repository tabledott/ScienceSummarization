ding the effort provide one or two examples of literal and nonliteral usages for each target verb to each annotator would almost certainly improve inter-annotator agreement.
    Table 1 lists the total number of target sentences, plus the manually evaluated literal and nonliteral counts, for each target word.
    It also provides the feedback set sizes for each target word.
    The totals across all words are given at the bottom of the table.
    The algorithms were evaluated based on how accurately they clustered the hand-annotated sentences.
    Sentences that were attracted to neither cluster or were equally attracted to both were put in the opposite set from their label, making a failure to cluster a sentence an incorrect clustering.
    Evaluation results were recorded as recall, precision, and f-score values.
    Literal recall is defined as (correct literals in literal cluster /total correct literals).
    Literal precision is defined as (correct literals in literal cluster / size of literal cluster).
