f newswire documents annotated with coreference chains.
    Although we did not participate in either MUC-6 or MUC-7, we were able to obtain the training and test corpora for both years from the MUC organizers for research purposes.'
    To our knowledge, these are the only publicly available annotated corpora for coreference resolution.
    For MUC-6, 30 dry-run documents annotated with coreference information were used as the training documents for our coreference engine.
    There are also 30 annotated training documents from MUC-7.
    The total size of the 30 training documents is close to 12,400 words for MUC-6 and 19,000 words for MUC-7.
    There are altogether 20,910 (48,872) training examples used for MUC-6 (MUC-7), of which only 6.5% (4.4%) are positive examples in MUC-6 (MUC-7).2 After training a separate classifier for each year, we tested the performance of each classifier on its corresponding test corpus.
    For MUC-6, the C5 pruning confidence is set at 20% and the minimum number of instances