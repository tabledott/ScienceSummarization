tionally, language objects are characterized by a feature vector.
    These feature vectors can be interpreted as points in a multidimensional space.
    The clustering uses a distance metric, e.g. the cosine of the angle between two such vectors.
    As in NLP there are often several thousand features, of which only a few correlate with each other at a time &#8211; think about the number of different words as opposed to the number of words occurring in a sentence &#8211; dimensionality reduction techniques can greatly reduce complexity without considerably losing accuracy.
    An alternative representation that does not deal with dimensions in space is the graph representation.
    A graph represents objects (as nodes) and their relations (as edges).
    In NLP, there are a variety of structures that can be naturally represented as graphs, e.g. lexical-semantic word nets, dependency trees, co-occurrence graphs and hyperlinked documents, just to name a few.
    Clustering graphs is a somewhat different task t