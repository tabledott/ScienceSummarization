y being assigned negative weight in future training.
    If desired, several iterations of this procedure may be performed.
  
  
    One of the advantages of CRFs and maximum entropy models in general is that they easily afford the use of arbitrary features of the input.
    One can encode local spelling features, layout features such as positions of line breaks, as well as external lexicon features, all in one framework.
    We study all these features in our research paper extraction problem, evaluate their individual contributions, and give some guidelines for selecting good features.
  
  
    Here we also briefly describe a HMM model we used in our experiments.
    We relax the independence assumption made in standard HMM and allow Markov dependencies among observations, e.g., P(otlst, ot&#8722;1).
    We can vary Markov orders in state transition and observation transitions.
    In our experiments, a model with second order state transitions and first order observation transitions performs the best.
  