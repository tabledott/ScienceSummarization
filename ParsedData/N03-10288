e, fi[y, y&#57741;] = f(y, y&#57741;, x, i), F(y, x) = Ei f(yi&#8722;1, yi, x, i), and let &#57738; denote component-wise matrix product.
    Then where &#57739;i and &#57740;i the forward and backward state-cost vectors defined by Therefore, we can use a forward pass to compute the &#57739;i and a backward bass to compute the &#57740;i and accumulate the feature expectations.
    To avoid overfitting, we penalize the likelihood with a spherical Gaussian weight prior (Chen and Rosenfeld, 1999): optimization algorithms when many correlated features are involved.
    Concurrently with the present work, Wallach (2002) tested conjugate gradient and second-order methods for CRF training, showing significant training speed advantages over iterative scaling on a small shallow parsing problem.
    Our work shows that preconditioned conjugate-gradient (CG) (Shewchuk, 1994) or limited-memory quasi-Newton (L-BFGS) (Nocedal and Wright, 1999) perform comparably on very large problems (around 3.8 million features).
    We 