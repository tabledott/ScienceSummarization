1;, the judges must determine if the instance (federal prosecutors, Y announced the arrest of X, Terry Nichols) is correct.
    The judges were asked to consider the following two criteria for their decision: Judges found that annotation decisions can range from trivial to difficult.
    The differences often were in the instances for which one of the judges fails to see the right context under which the inference could hold.
    To minimize disagreements, the judges went through an extensive round of training.
    To that end, the 1000 instances (x, pj, y) were split into DEV and TEST sets, 500 in each.
    The two judges trained themselves by annotating DEV together.
    The TEST set was then annotated separately to verify the inter-annotator agreement and to verify whether the task is well-defined.
    The kappa statistic (Siegel and Castellan Jr. 1988) was x = 0.72.
    For the 70 disagreements between the judges, a third judge acted as an adjudicator.
    We compare our ISP algorithms to the following ba