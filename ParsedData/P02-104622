it follows that the true precision of the final classifier exceeds B if the measured precision of every accepted rule exceeds B.
    Moreover, observe that recall can be written as: Nt Qt N` where N` is the number of instances whose true label is t. If Qt &gt; B, then recall is bounded below by NtB/N`, which grows as Nt grows.
    Hence we have proven the following theorem.
    Intuitively, the Yarowsky algorithm increases recall while holding precision above a threshold that represents the desired precision of the final classifier.
    The empirical behavior of the algorithm, as shown in figure 6, is in accordance with this analysis.
    We have seen, then, that the Yarowsky algorithm, like the co-training algorithm, can be justified on the basis of an independence assumption, precision independence.
    It is important to note, however, that the Yarowsky algorithm is not a special case of co-training.
    Precision independence and view independence are distinct assumptions; neither implies the other.2
  
 