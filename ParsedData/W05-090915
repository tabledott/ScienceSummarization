ng of the Association of Computational Linguistics (2005)* The Chinese data set consists of 920 sentences, while the Arabic data set consists of 664 sentences* Each sentence has four reference translations* Furthermore, for 7 systems on the Chinese data and 6 on the Arabic data, every sentence translation has been assessed by two separate human judges and assigned an Adequacy and a Fluency Score* Each such score ranges from one to five (with one being the poorest grade and five the highest)* For this paper, we computed a Combined Score for each translation by averaging the adequacy and fluency scores of the two judges for that translation* We also computed an average System Score for each translation system by averaging the Combined Score for all the translations produced by that system* (Note that although we refer to these data sets as the &amp;quot;Chinese&amp;quot; and the &amp;quot;Arabic&amp;quot; In this paper, we are interested in evaluating METEOR as a metric that can evaluate translations on a sente