ghly accurate, efficient and ro bust CCG parsers which can be used directly for this task (Clark and Curran, 2004b; Hockenmaier, 2003).The existing CCG parsers deliver predicate argu ment structures, but not semantic representations that can be used for inference.
			The present paper seeks to extend one of these wide coverage parsers by using it to build logical forms suitable for use invarious NLP applications that require semantic in terpretation.We show how to construct first-order represen tations from CCG derivations using the ?-calculus, and demonstrate that semantic representations can be produced for over 97% of the sentences in unseen WSJ text.
			The only other deep parser we are aware of to achieve such levels of robustness for the WSJ is Kaplan et al (2004).
			The use of the ?-calculusis integral to our method.
			However, first-order rep resentations are simply used as a proof-of-concept; we could have used DRSs (Kamp and Reyle, 1993)or some other representation more tailored to the ap plicatio