s a probability distribution over strings P(s) that attempts to reflect the frequency with which each string s occurs as a sentence in natural text.
    Language models are used in speech recognition to resolve acoustically ambiguous utterances.
    For example, if we have that PO takes two) &gt;&gt; P(it takes too), then we know ceieris paribus to prefer the former transcription over the latter.
    While smoothing is a central issue in language modeling, the literature lacks a definitive comparison between the many existing techniques.
    Previous studies (Nadas, 1984; Katz, 1987; Church and Gale, 1991; MacKay and Peto, 1995) only compare a small number of methods (typically two) on a single corpus and using a single training data size.
    As a result, it is currently difficult for a researcher to intelligently choose between smoothing schemes.
    In this work, we carry out an extensive empirical comparison of the most widely used smoothing techniques, including those described by Jelinek and Mercer (198