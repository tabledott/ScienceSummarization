tate distribution for this transition matrix assigns a probability to each (n - 1)-gram, which we denote S(w7-1).
    We say that an n-gram language model is consistent if, for each string w7-1, the probability that the model assigns to win-1 is S(win-1).
    Sequential maximum likelihood estimation does not, in general, lead to a consistent model, although for large values of T, the model will be very nearly consistent.
    Maximum likelihood estimation of the parameters of a consistent n-gram language model is an interesting topic, but is beyond the scope of this paper.
    The vocabulary of English is very large and so, even for small values of n, the number of parameters in an n-gram model is enormous.
    The IBM Tangora speech recognition system has a vocabulary of about 20,000 words and employs a 3-gram language model with over eight trillion parameters (Averbuch et al. 1987).
    We can illustrate the problems attendant to parameter estimation for a 3-gram language model with the data in Table 1.
    