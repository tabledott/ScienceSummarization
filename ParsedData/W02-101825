ge T. The IBM Model 4 (Brown et al., 1993), for example, converges to the word alignments shown in Figure 1.b and learns the translation probabilities shown in Figure 1.a.2 Since in the IBM model one cannot link a Target word to more than a Source word, the training procedure yields unintuitive translation probabilities.
    (Note that another good word-for-word model is one that assigns high probability to p(xb) and p(zb) and low probability to p(xc).)
    In this paper, we describe a translation model that assumes that lexical correspondences can be established not only at the word level, but at the phrase level as well.
    In constrast with many previous approaches (Brown et al., 1993; Och et al., 1999; Yamada and Knight, 2001), our model does not try to capture how Source sentences can be mapped into Target sentences, but rather how Source and Target sentences can be generated simultaneously.
    In other words, in the style of Melamed (2001), we estimate a joint probability model that can be easily marg