d by BLEU.
    In-domain Systran scores on this metric are lower than all statistical systems, even the ones that have much worse human scores.
    Surprisingly, this effect is much less obvious for out-of-domain test data.
    For instance, for out-ofdomain English-French, Systran has the best BLEU and manual scores.
    Our suspicion is that BLEU is very sensitive to jargon, to selecting exactly the right words, and not synonyms that human judges may appreciate as equally good.
    This is can not be the only explanation, since the discrepancy still holds, for instance, for out-of-domain French-English, where Systran receives among the best adequacy and fluency scores, but a worse BLEU score than all but one statistical system.
    This data set of manual judgements should provide a fruitful resource for research on better automatic scoring methods.
    So, who won the competition?
    The best answer to this is: many research labs have very competitive systems whose performance is hard to tell apart.
    T