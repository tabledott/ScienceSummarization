n tokens from 11,294 different types.
    10,512 patterns appeared just once and these were effectively discarded since our learning algorithm only considers patterns which occur at least twice (see Section 3.3).
    The document-centric approach benefits from a large corpus containing a mixture of relevant and irrelevant documents.
    We provided this using a subset of the Reuters Corpus Volume I (Rose et al., 2002) which, like the MUC-6 corpus, consists of newswire texts.
    3000 documents relevant to the management succession task (identified using document metadata) and 3000 irrelevant documents were used to produce the supplementary corpus.
    This supplementary corpus yielded 126,942 pattern tokens and 79,473 types with 14,576 of these appearing more than once.
    Adding the supplementary corpus to the data set used by the document-centric approach led to an improvement of around 15% on the document filtering task and over 70% for sentence filtering.
    It was not used for the semantic similarity a