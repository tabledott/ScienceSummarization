tem.
    The simplest instance of this problem can be found in the realm of language modeling, using perplexity-based selection methods.
    The sentences in the general-domain corpus are scored by their perplexity score according to an in-domain language model, and then sorted, with only the lowest ones being retained.
    This has been done for language modeling, including by Gao et al (2002), and more recently by Moore and Lewis (2010).
    The ranking of the sentences in a general-domain corpus according to in-domain perplexity has also been applied to machine translation by both Yasuda et al (2008), and Foster et al (2010).
    We test this approach, with the difference that we simply use the source side perplexity rather than computing the geometric mean of the perplexities over both sides of the corpus.
    We also reduce the size of the training corpus far more aggressively than Yasuda et al&#8217;s 50%.
    Foster et al (2010) do not mention what percentage of the corpus they select for their IR-base