    Unlike other methods discussed so far, voted perceptron training (Collins, 2002) attempts to minimize the difference between the global feature vector for a training instance and the same feature vector for the best-scoring labeling of that instance according to the current model.
    More precisely, for each training instance the method computes a weight update in which &#710;yk is the Viterbi path Like the familiar perceptron algorithm, this algorithm repeatedly sweeps over the training instances, updating the weight vector as it considers each instance.
    Instead of taking just the final weight vector, the voted perceptron algorithm takes the average of the &#57738;t.
    Collins (2002) reported and we confirmed that this averaging reduces overfitting considerably.
  
  
    Figure 1 shows the base NPs in an example sentence.
    Following Ramshaw and Marcus (1995), the input to the NP consists of the words in a sentence annotated automatically with part-of-speech (POS) tags.
    The task is to label