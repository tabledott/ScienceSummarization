 and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting.
    We extend this normal form to null productions and give the first extensive empirical comparison of simple and normal form ITGs, for posterior decoding under our likelihood models.
    Additionally, we show how to deal with training instances where the gold alignments are outside of the hypothesis class by instead optimizing the likelihood of a set of minimum-loss alignments.
    Perhaps the greatest advantage of ITG models is that they straightforwardly permit blockstructured alignments (i.e. phrases), which general matchings cannot efficiently do.
    The need for block alignments is especially acute in ChineseEnglish data, where oracle AERs drop from 10.2 without blocks to around 1.2 with them.
    Indeed, blocks are the primary reason for gold alignments being outside the space of one-to-one ITG alignments.
    We show that placing linear potential functions on many-to-one blocks can substantially improve performanc