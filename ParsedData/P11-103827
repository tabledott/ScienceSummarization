that the F-score decreases monotonically.
    Thus, we use a smaller threshold, i.e. td = 1.
    Second, there are differences between the two corpora, with dependencies from the Blog corpus producing slightly lower precision but higher recall, compared with the NYT corpus.
    The lower precision for the Blog corpus appears to be due to the text not being as clean as NYT, introducing parser errors.
    Nevertheless, the difference in F-score between the two corpora is insignificant.
    Third, we obtain the best results, especially in terms of precision, for wd = 0.5, i.e. with expanded dependencies, but penalised relative to nonexpanded dependencies.
    Overall, the best F-score is 71.2%, with a precision of 61.1% and recall of 85.3%, obtained over the Blog corpus with td = 1 and wd = 0.5.
    Clearly there is significant room for immprovements in these results.
    We leave the improvement of ill-formed word detection for future work, and perform evaluation of candidate selection for Twitter assuming perf