balizations produced by our algorithm from the central tree in Figure 7.
    Here, we can see that the lowestscoring sentence is both grammatical and concise.
    Table 4 also illustrates that entropy-based scoring does not always correlate with the quality of the generated sentence.
    For example, the fifth sentence in Table 4&#8212; Palestinians fired antitank missile at a bulldozer to build a new embankment in the area&#8212;is not a well-formed sentence; however, our language model gave it a better score than its well-formed alternatives, the second and the third sentences (see Section 4 for further discussion).
    Despite these shortcomings, we preferred entropy-based scoring to symbolic linearization.
    In the next section, we motivate our choice.
    3.3.1 Statistical versus Symbolic Linearization.
    In the previous version of the system (Barzilay, McKeown, and Elhadad 1999), we performed linearization of a fusion dependency structure using the language generator FUF/SURGE (Elhadad and Robin 199