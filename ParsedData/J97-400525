 the ERF weights in Table 4 are not the best weights for our example grammar.
    Consider the alternative model MK given in Figure 9, defining probability distribution 11*.
    An alternative model, M*.
    These weights are proper, in the sense that weights for rules with the same left-hand side sum to one.
    The reader can verify that 0* sums to Z = 3+3`n and that q* is: In short, in the AV case, the ERF weights do not yield the best weights.
    This means that the ERF method does not converge to the correct weights as the corpus size increases.
    If there are genuine dependencies in the grammar, the ERF method converges systematically to the wrong weights.
    Fortunately, there are methods that do converge to the right weights.
    These are methods that have been developed for random fields.
  
  
    A random field defines a probability distribution over a set of labeled graphs SZ called configurations.
    In our case, the configurations are the dags generated by the grammar, i.e., C2 = L(G).
   