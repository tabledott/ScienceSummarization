r limit is.
    We can check, what the consequences of less manual annotation of results would have been: With half the number of manual judgements, we can distinguish about 40% of the systems, 10% less.
    The test set included 2000 sentences from the Europarl corpus, but also 1064 sentences out-ofdomain test data.
    Since the inclusion of out-ofdomain test data was a very late decision, the participants were not informed of this.
    So, this was a surprise element due to practical reasons, not malice.
    All systems (except for Systran, which was not tuned to Europarl) did considerably worse on outof-domain training data.
    This is demonstrated by average scores over all systems, in terms of BLEU, fluency and adequacy, as displayed in Figure 5.
    The manual scores are averages over the raw unnormalized scores.
    It is well know that language pairs such as EnglishGerman pose more challenges to machine translation systems than language pairs such as FrenchEnglish.
    Different sentence structure a