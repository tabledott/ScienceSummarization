he average error rate varies with k for the seven similarity metrics introduced above.
    As previously mentioned, a steeper slope indicates a better similarity ranking.
    All the curves have a generally upward trend but always lie far below backoff (51% error rate).
    They meet at k = 1000 because Sf JD:x:1(n) is always the set of all nouns.
    We see that the functions fall into four groups: (1) the L2 norm; (2) Kendall's T; (3) the confusion probability and the cosine metric; and (4) the L1 norm, Jensen-Shannon divergence, and Jaccard's coefficient.
    We can account for the similar performance of various metrics by analyzing how they incorporate information from the intersection of the supports of q and r. (Recall that we are using q and r for the conditional verb cooccurrrence probabilities of two nouns n and m.) Consider the following supports (illustrated in Figure 3): We can rewrite the similarity functions from Section 2 in terms of these sets, making use of the identities E vEVqWqr q(v) + EvE