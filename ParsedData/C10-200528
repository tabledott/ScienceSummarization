ke available how their classification was built.
			One exception is TwitterSentiment (Go et al., 2009), for instance, which considers tweets with good emoticons as positive examples and tweets with bad emoticons as negative examples for the training data, and builds a classifier using unigrams and bigrams as features.
			We showed in Section 4 that our approach works better than theirs for this problem, obtaining lower error rates.
	
	
			We have presented an effective and robust sen timent detection approach for Twitter messages, which uses biased and noisy labels as input to build its models.
			This performance is due to the fact that: (1) our approach creates a more abstract representation of these messages, instead of usinga raw word representation of them as some previous approaches; and (2) although noisy and bi ased, the data sources provide labels of reasonablequality and, since they have different bias, com bining them also brought some benefits.
			The main limitation of our approach is the cases 