In our coreference resolution MLN, Y includes Head and known groundings of Type, Number and Gender, Z includes InClust and unknown groundings of Type, Number, Gender, and X includes IsPrn, Appo and PredNom.
    (For simplicity, from now on we drop X from the formula.)
    With Z, the optimization problem is no longer convex.
    However, we can still find a local optimum using gradient descent, with the gradient being where ni is the number of true groundings of the ith clause.
    We extended PSCG for unsupervised learning.
    The gradient is the difference of two expectations, each of which can be approximated using samples generated by MC-SAT.
    The (i, j)th entry of and the step size can be computed accordingly.
    Since our problem is no longer convex, the negative diagonal Hessian may contain zero or negative entries, so we first took the absolute values of the diagonal and added 1, then used the inverse as the preconditioner.
    We also adjusted A more conservatively than Lowd &amp; Domingos (2007