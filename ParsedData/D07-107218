 to use variational inference (Bleiand Jordan, 2005), which provides a fast determin istic alternative to sampling, hence avoiding issues of diagnosing convergence and aggregating samples.Furthermore, our variational inference algorithm establishes a strong link with past work on PCFG refinement and induction, which has traditionally em ployed the EM algorithm.
			In EM, the E-step involves a dynamic program that exploits the Markov structure of the parse tree, and the M-step involves computing ratios based onexpected counts extracted from the E-step.
			Our vari ational algorithm resembles the EM algorithm in form, but the ratios in the M-step are replaced withweights that reflect the uncertainty in parameter es 692 ??Bz ?Tz ?Ez z ? z1 z2 z3 T Parameters Trees Figure 4: We approximate the true posterior p overparameters ? and latent parse trees z using a structured mean-field distribution q, in which the distri bution over parameters are completely factorized but the distribution over parse trees is unconstr