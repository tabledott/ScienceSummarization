lized form of ExpectationMaximization, to find HMM parameters which (at least locally) maximize the likelihood function Ld.
    Recently there is increasing interest in Bayesian methods in computational linguistics, and the primary goal of this paper is to compare the performance of various Bayesian estimators with each other and with EM.
    A Bayesian approach uses Bayes theorem to factorize the posterior distribution P(0  |d) into the Priors can be useful because they can express preferences for certain types of models.
    To take an example from our POS-tagging application, most words belong to relatively few parts-of-speech (e.g., most words belong to a single POS, and while there are some words which are both nouns and verbs, very few are prepositions and adjectives as well).
    One might express this using a prior which prefers HMMs in which the state-to-word emissions are sparse, i.e., each state emits few words.
    An appropriate Dirichlet prior can express this preference.
    While it is possibl