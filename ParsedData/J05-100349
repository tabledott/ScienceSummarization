;&#65533;, k*, S*&#222;, the values W&#254;k and Wk remain unchanged for many features and do not need to be recalculated.
    In fact, only A naive algorithm for the boosting loss function. features which co-occur with k* on some example must be updated.
    The algorithm in Figure 4 recalculates the values of A k and A&#8212;k only for those features which co-occur with the selected feature k*.
    To achieve this, the algorithm relies on a second pair of indices.
    For all i, 2 &lt; j &lt; ni, we define Figure 4 An improved algorithm for the boosting loss function.
    In contrast, the naive algorithm requires a pass over the entire training set, which requires the following number of steps: The relative efficiency of the two algorithms depends on the value of C/T at each iteration.
    In the worst case, when every feature chosen appears on every training example, then C/T = 1, and the two algorithms essentially have the same running time.
    However in sparse feature spaces there is reason to believe 