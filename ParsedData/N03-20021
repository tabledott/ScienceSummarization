ognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.
  
  
    The art of statistical language modeling (LM) is to create probability models over words and sentences that tradeoff statistical prediction with parameter variance.
    The field is both diverse and intricate (Rosenfeld, 2000; Chen and Goodman, 1998; Jelinek, 1997; Ney et al., 1994), with many different forms of LMs including maximumentropy, whole-sentence, adaptive and cache-based, to name a small few.
    Many models are simply smoothed conditional probability distributions for a word given its preceding history, typically the two preceding words.
    In this work, we introduce two new methods for language modeling: factored language model (FLM) and generalized parallel backoff (GPB).
    An FLM considers a word as a bundle of features, and GPB is a technique that generalized backoff to arbitrary conditional probability tables.
    While these techniques can be consider