l K at the previous round of feature selection.
    In initial experiments we found ExpLoss to give similar, perhaps slightly better, accuracy than LogLoss.
    This section describes further experiments investigating various aspects of the boosting algorithm: the effect of the &amp; and N parameters, learning curves, the choice of the Si,j weights, and efficiency issues.
    5.4.1 The Effect of the a and N Parameters.
    Figure 5 shows the learning curve on development data for the optimal value of &amp; (0.0025).
    The accuracy shown is the performance relative to the baseline method of using the probability from the generative model alone in ranking parses, where the measure in equation (21) is used to measure performance.
    For example, a score of 101.5 indicates a 1.5% increase in this score.
    The learning curve is initially steep, eventually flattening off, but reaching its peak value after a large number (90,386) of rounds of feature selection.
    Table 2 indicates how the peak performance var