el aspect of this paper is that we present a formal objective and a training algorithm for combining two generic models.
  
  
    We have described an efficient and fully unsupervised method of producing state-of-the-art word alignments.
    By training two simple sequence-based models to agree, we achieve substantial error reductions over standard models.
    Our jointly trained HMM models reduce AER by 29% over test-time intersected GIZA++ model 4 alignments and also increase our robustness to varying initialization regimens.
    While AER is only a weak indicator of final translation quality in many current translation systems, we hope that more accurate alignments can eventually lead to improvements in the end-to-end translation process.
    Acknowledgments We thank the anonymous reviewers for their comments.
  

