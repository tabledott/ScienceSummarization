 to outperform a number of state-of-the-art unsupervised and transfer-based baselines.
    The method is simple; for a given training set, the learner ignores all lexical identities and only observes features over other characteristics, e.g., part-of-speech tags, orthographic features, direction of syntactic attachment, etc.
    The learner builds a model from an annotated source language data set, after which the model is used to directly make target language predictions.
    There are three basic assumptions that drive this approach.
    First, that high-level tasks, such as syntactic parsing, can be learned reliably using coarse-grained statistics, such as part-of-speech tags, in place of fine-grained statistics such as lexical word identities.
    Second, that the parameters of features over coarsegrained statistics are in some sense language independent, e.g., a feature that indicates that adjectives modify their closest noun is useful in all languages.
    Third, that these coarse-grained statistics are