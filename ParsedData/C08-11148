corresponds to a feature in the feature vectors that we will generate.
			Since a typical input set of word pairs yields millions of pat terns, we need to use feature selection, to reduce the number of patterns to a manageable quantity.
			For each pattern, we count the number of input word pairs that generated the pattern.
			For example, ?* X cut * Y *?
			is generated by both mason:stone and carpenter:wood.
			We then sort the patterns in descending order of the number of word pairs that generated them.
			If there are N input word pairs (and thus N feature vectors, including both the training and testing sets), then we select the topkN patterns and drop the remainder.
			In the fol lowing experiments, k is set to 20.
			The algorithm is not sensitive to the precise value of k.The reasoning behind the feature selection al gorithm is that shared patterns make more useful features than rare patterns.
			The number of features (kN ) depends on the number of word pairs (N ), because, if we have more feature ve