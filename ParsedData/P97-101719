k richard bryan more highly than richard brian.
    A bigram model would prefer orren hatch over olin hatch.
    Other errors are due to unigram training problems, or more rarely, incorrect or brittle phonetic models.
    For example, &amp;quot;Long&amp;quot; occurs much more often than &amp;quot;Ron&amp;quot; in newspaper text, and our word selection does not exclude phrases like &amp;quot;Long Island.&amp;quot; So we get long wyden instead of ron wyden.
    Rare errors are due to incorrect or brittle phonetic models.
    Still the machine's performance is impressive.
    When word separators ( &#8226; ) are removed from the katakana phrases, rendering the task exceedingly difficult for people, the machine's performance is unchanged.
    When we use OCR.
    7% of katakana tokens are mis-recognized, affecting 50% of test strings, but accuracy only drops from 64% to 52%.
  
  
    We have presented a method for automatic backtransliteration which, while far from perfect, is highly competitive.
    It also ach