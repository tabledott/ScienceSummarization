
  Collecting Highly Parallel Data for Paraphrase Evaluation
  
    A lack of standard datasets and evaluation has prevented the field of paraphrasmaking the kind of rapid progress enjoyed by the machine translation community over the last 15 years.
    We address both problems by presenting a novel data collection framework that produces highly parallel text data relatively inexpensively and on a large scale.
    The highly parallel nature of this data allows us to use simple n-gram comparisons to measure both the semantic adequacy and lexical dissimilarity of paraphrase candidates.
    In addition to being simple and efficient to compute, experiments show that these metrics correlate highly with human judgments.
  
  
    Machine paraphrasing has many applications for natural language processing tasks, including machine translation (MT), MT evaluation, summary evaluation, question answering, and natural language generation.
    However, a lack of standard datasets and automatic evaluation metrics has impede