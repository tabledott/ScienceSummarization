 where Any positive conditional distribution p(Y |X) that obeys the Markov property can be written in the form (1) for appropriate choice of feature functions and weight vector (Hammersley and Clifford, 1971).
    The most probable label sequence for input sequence x is because Z&#57738;(x) does not depend on y. F(y, x) decomposes into a sum of terms for consecutive pairs of labels, so the most likely y can be found with the Viterbi algorithm.
    We train a CRF by maximizing the log-likelihood of a given training set T = {(xk, yk)}Nk=1, which we assume fixed for the rest of this section: To perform this optimization, we seek the zero of the gradient In words, the maximum of the training data likelihood is reached when the empirical average of the global feature vector equals its model expectation.
    The expectation Ep,,(Y |.
    )F(Y , x) can be computed efficiently using a variant of the forward-backward algorithm.
    For a given x, define the transition matrix for position i as Let f be any local featur