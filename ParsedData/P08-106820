er parsers were then trained with respect to these projective trees.
    The development and test sets were not projectivized, so our secondorder parser is guaranteed to make errors in test sentences containing non-projective dependencies.
    To overcome this, McDonald and Pereira (2006) use a two-stage approximate decoding process in which the output of their second-order parser is &#8220;deprojectivized&#8221; via greedy search.
    For simplicity, we did not implement a deprojectivization stage on top of our second-order parser, but we conjecture that such techniques may yield some additional performance gains; we leave this to future work.
    Table 4 gives accuracy results on the PDT 1.0 test set for our unlabeled parsers.
    As in the English experiments, there are clear trends in the results: parsers using cluster-based features outperform parsers using baseline features, and secondorder parsers outperform first-order parsers.
    Both of the comparisons between cluster-based and baseline features in