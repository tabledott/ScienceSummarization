ause negative examples from each iteration only lead to incor rect derivations and it is always good to includethem.
			To further increase the number of negative examples, every positive example for a pro duction is also included as a negative example for all the other productions having the same LHS.
			After a specified number of MAX ITR iterations, 917 (ANSWER?
			answer(RIVER), [1..9]) (RIVER?
			TRAVERSE(STATE), [1..9]) (TRAVERSE?traverse, [1..7]) Which1 rivers2 run3 through4 the5 states6 bordering7 (STATE?
			STATEID, [8..9]) (STATEID?
			stateid texas, [8..9]) Texas8 ?9 Figure 5: An incorrect semantic derivation of the NL sentence ?Which rivers run through the states bordering Texas??
			which gives the incorrect MR answer(traverse(stateid(texas))).
			the trained classifiers from the last iteration are returned.
			Testing involves using these classifiers to generate the most probable derivation of a test sentence as described in the subsection 2.2, and returning its MR. The MRL grammar may contain p