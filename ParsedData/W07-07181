we analyze the evaluation measures themselves in order to try to determine &#8220;best practices&#8221; when evaluating machine translation research.
    Previous ACL Workshops on Machine Translation were more limited in scope (Koehn and Monz, 2005; Koehn and Monz, 2006).
    The 2005 workshop evaluated translation quality only in terms of Bleu score.
    The 2006 workshop additionally included a limited manual evaluation in the style of NIST machine translation evaluation workshop.
    Here we apply eleven different automatic evaluation metrics, and conduct three different types of manual evaluation.
    Beyond examining the quality of translations produced by various systems, we were interested in examining the following questions about evaluation methodologies: How consistent are people when they judge translation quality?
    To what extent do they agree with other annotators?
    Can we improve human evaluation?
    Which automatic evaluation metrics correlate most strongly with human judgments of transl