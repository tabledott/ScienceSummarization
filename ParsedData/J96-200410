ient of agreement (Siegel and Castellan 1988) as a measure of reliability!
    The kappa coefficient (K) measures pairwise agreement among a set of coders making category judgments, correcting for expected chance agreement: where P(A) is the proportion of times that the coders agree and P(E) is the proportion of times that we would expect them to agree by chance, calculated along the lines of the intuitive argument presented above.
    (For complete instructions on how to calculate K, see Siegel and Castellan [1988].)
    When there is no agreement other than that which would be expected by chance, K is zero.
    When there is total agreement, K is one.
    It is possible, and sometimes useful, to test whether or not K is significantly different from chance, but more importantly, interpretation of the scale of agreement is possible.
    Krippendorff (1980) discusses what constitutes an acceptable level of agreement, while giving the caveat that it depends entirely on what one intends to do with the, coding.
 