   Individual systems and system combinations are ranked based on how frequently they were judged to be better than or equal to any other system.
    The results of this are reported in Section 4.
    Appendix A provides detailed tables that contain pairwise comparisons between systems.
    We were interested in determining the inter- and intra-annotator agreement for the ranking task, since a reasonable degree of agreement must exist to support our process as a valid evaluation setup.
    To ensure we had enough data to measure agreement, we purposely designed the sampling of source segments shown to annotators so that items were likely to be repeated, both within an annotator&#8217;s assigned tasks and across annotators.
    We did so by assigning an annotator a batch of 20 screens (each with three ranking sets; see 3.1) that were to be completed in full before generating new screens for that annotator.
    Within each batch, the source segments for nine of the 20 screens (45%) were chosen from a small pool