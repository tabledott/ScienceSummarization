ng et al, 2006), wherein a lower order n-gram is used in a hypothesis-generation phase, then later the K-best of these hypotheses are re-scored using a large-scale distributed language model.
			The resulting translation performance was shown to improve appreciably over the hypothesis deemed best by the first-stage system.
			The amount of data used was 3 billion words.
			More recently, a large-scale distributed language model has been proposed in the contexts of speech recognition and machine translation (Emami et al, 2007).
			The underlying architecture is similar to(Zhang et al, 2006).
			The difference is that they integrate the distributed language model into their machine translation decoder.
			However, they don?t re port details of the integration or the efficiency of the approach.
			The largest amount of data used in the experiments is 4 billion words.
			Both approaches differ from ours in that they store corpora in suffix arrays, one sub-corpus per worker,and serve raw counts.
			This implies th