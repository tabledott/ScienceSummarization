   As E. T. Jaynes, a more recent pioneer of maximum entropy, put it (Jaynes 1990): ... the fact that a certain probability distribution maximizes entropy subject to certain constraints representing our incomplete information, is the fundamental property which justifies use of that distribution for inference; it agrees with everything that is known, but carefully avoids assuming anything that is not known.
    It is a transcription into mathematics of an ancient principle of wisdom ...
  
  
    We consider a random process that produces an output value y, a member of a finite set Y.
    For the translation example just considered, the process generates a translation of the word in, and the output y can be any word in the set {dans, en, a, au cours de, pendant}.
    In generating y, the process may be influenced by some contextual information x, a member of a finite set X.
    In the present example, this information could include the words in the English sentence surrounding in.
    Our task is to construct 