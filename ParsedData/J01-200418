alculated in closed form through matrix inversion.
    They are limited, therefore, to grammars where the nonterminal set is small enough to permit inversion.
    String prefix probabilities can be straightforwardly used to compute conditional word probabilities by definition: Stolcke and Segal (1994) and Jurafsky et al. (1995) used these basic ideas to estimate bigram probabilities from hand-written PCFGs, which were then used in language models.
    Interpolating the observed bigram probabilities with these calculated bigrams led, in both cases, to improvements in word error rate over using the observed bigrams alone, demonstrating that there is some benefit to using these syntactic language models to generalize beyond observed n-grams.
    Another approach that uses syntactic structure for language modeling has been to use a shift-reduce parser to &amp;quot;surface&amp;quot; c-commanding phrasal headwords or part-of-speech (POS) tags from arbitrarily far back in the prefix string, for use in a trigram-like