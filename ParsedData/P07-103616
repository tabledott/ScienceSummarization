arameters to drift too far from the supervised model.
    The parameter re-estimation in line 9, uses a similar intuition, but instead of weighting data instances, we introduced a smoothing parameter -y which controls the convex combination of models induced by the labeled and the unlabeled data.
    Unlike the technique mentioned above which focuses on naive Bayes, our method allows us to weight linear models generated by different learning algorithms.
    Another way to look the algorithm is from the self-training perspective (McClosky et al., 2006).
    Similarly to self-training, we use the current model to generate new training examples from the unlaTop-K-Inference, we use beamsearch to find the Kbest solution according to Eq.
    (1). beled set.
    However, there are two important differences.
    One is that in self-training, once an unlabeled sample was labeled, it is never labeled again.
    In our case all the samples are relabeled in each iteration.
    In self-training it is often the case that o