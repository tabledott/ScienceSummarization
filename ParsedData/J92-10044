t concern itself not only with coverage but also, and perhaps more importantly, with overgeneralizat ion.
    In many existing systems, the ability to parse as many sentences as possible is often achieved at the expense of accepting inappropriate word strings as legitimate sentences.
    This had not been viewed as a major concern in the past, since systems were typically presented only with well-formed text strings, as opposed to errorful recognizer outputs.
    The constraints can be much more effective if they are embedded in a probabilistic framework.
    The use of probabilities in a language model can lead to a substantially reduced perplexity3 for the recognizer.
    If the natural language component's computational and memory requirements are not excessive, and if it is organized in such a way that it can easily predict a set of next-word candidates, then it can be incorporated into the active search process of the recognizer, dynamically predicting possible words to follow a hypothesized word sequenc