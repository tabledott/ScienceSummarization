odels like probabilistic context-free grammars are inadequate.
    This work illustrates that existing decision-tree technology can be used to construct and estimate models which selectively choose elements of the context which contribute to disambiguation decisions, and which have few enough parameters to be trained using existing resources.
    I begin by describing decision-tree modeling, showing that decision-tree models are equivalent to interpolated n-gram models.
    Then I briefly describe the training and parsing procedures used in SPATTER.
    Finally, I present some results of experiments comparing SPATTER with a grammarian's rulebased statistical parser, along with more recent results showing SPATTER applied to the Wall Street Journal domain.
  
  
    Much of the work in this paper depends on replacing human decision-making skills with automatic decision-making algorithms.
    The decisions under consideration involve identifying constituents and constituent labels in natural language sentences.
