performance of the cotraining approach decreases because noisy training examples may be selected from the remaining unlabeled set.
    Finally, the performance of the approach does not change any more, because the algorithm runs out of all possible examples in the unlabeled set.
    Fortunately, the proposed approach performs well with a wide range of iteration numbers.
    We can also see that the two component classifier has similar trends with the cotraining approach.
    It is encouraging that the component Chinese classifier alone can perform better than the best baseline when the iteration number is set between 40 and 70.
    Figure 4 shows how the growth size at each iteration (p positive and n negative confident examples) influences the accuracy of the proposed co-training approach.
    In the above experiments, we set p=n, which is considered as a balanced growth.
    When p differs very much from n, the growth is considered as an imbalanced growth.
    Balanced growth of (2, 2), (5, 5), (10, 10) and