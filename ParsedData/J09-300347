ter, we vary the number of rounds of boosting.
    For TiMBL, we vary the value for k (the number of neighbors) and the distance metric (overlap or modified value difference metric [MVDM]).
    For Ripper, we vary whether negative tests are disallowed for nominal (-!n) and set (-!s) valued attributes and how much to simplify the hypothesis (-S).
    For SVM, we experiment with linear, polynomial, and radial basis function kernels.
    Table 10 gives the settings selected for the neutral&#8211;polar classification experiments for the different learning algorithms.
    Table 11.
    For each algorithm, we give the results for the two baseline classifiers, followed by the results for the classifier trained using all the neutral&#8211;polar features.
    The results shown in bold are significantly better than both baselines (two-sided t-test, p &#8804; 0.05) for the given algorithm.
    Working together, how well do the neutral&#8211;polar features perform?
    For BoosTexter, TiMBL, and Ripper, the classifiers t