ese kinds of applications provide a good test case for an approach like ours.1 The first task is to identify fields from citations (McCallum et al., 2000) .
    The data originally included 500 labeled references, and was later extended with 5,000 unannotated citations collected from papers found on the Internet (Grenager et al., 2005).
    Given a citation, the task is to extract the each open bracket.
    The correct assignment was shown in (a).
    While the predicted label assignment (b) is generally coherent, some constraints are violated.
    Most obviously, punctuation marks are ignored as cues for state transitions.
    The constraint &#8220;Fields cannot end with stop words (such as &#8220;the&#8221;)&#8221; may be also good. fields that appear in the given reference.
    See Fig.
    1.
    There are 13 possible fields including author, title, location, etc.
    To gain an insight to how the constraints can guide semi-supervised learning, assume that the sentence shown in Figure 1 appears in the unl