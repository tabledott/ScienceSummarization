ng model was that we sought to discover fundamental differences in behavior.
    Because we have a binary decision task, Ef,k(n, vi) simply counts the number of k nearest neighbors to n that make the right decision.
    If we have two functions f and g such that Ef,k(n,vi) &gt; Eg,k(n, v1), then the k most similar words according to f are on the whole better predictors than the k most similar words according to g; hence, f induces an inherently better similarity ranking for distance-weighted averaging.
    The difficulty with using the full model (Equation (1)) for comparison purposes is that fundamental differences can be obscured by issues of weighting.
    For example, suppose the probability estimate E (2 &#8212; Li (q,r)) &#8226; r(v) (suitably normalized) performed poorly.
    We would not be able to tell whether the cause was an inherent deficiency in the L1 norm or just a poor choice of weight function &#8212; perhaps (2 &#8212; Li (q, r))2 would have yielded better estimates.
    Figure 2 shows how t