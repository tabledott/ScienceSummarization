 a significant correlation with the judgments, with the coefficient ranging from .480 to .578 for AltaVista counts and from .473 to .595 for the Google counts.
    Table 11 also provides the correlations between plausibility judgments and counts re-created using class-based smoothing, which we will discuss in Section 3.3.
    An important question is how well humans agree when judging the plausibility of adjective-noun, noun-noun, and verb-noun bigrams.
    Intersubject agreement gives an upper bound for the task and allows us to interpret how well our Web-based method performs in relation to humans.
    To calculate intersubject agreement we used leaveone-out resampling.
    This technique is a special case of n-fold cross-validation (Weiss and Kulikowski 1991) and has been previously used for measuring how well humans agree in judging semantic similarity (Resnik 1999, 2000).
    For each subject group, we divided the set of the subjects&#8217; responses with size n into a set of size n &#8722; 1 (i.e., the 