 to the 12 American grade levels.
    The language models we use are simple: they are based on unigrams and assume that the probability of a token is independent of the surrounding tokens, given the grade language model.
    A unigram language model is defined by a list of types (words) and their individual probabilities.
    Although this is a weak model, it can be trained from less data than more complex models, and turns out to give good accuracy for our problem.
    We define a generative model for a text passage T in which we assume T was created by a hypothetical author using the following algorithm: The probability of T given model Gi is therefore: where C(w) is the count of the type w in T. Our goal is to find the most likely grade language model given the text T, or equivalently, the model Gi that maximizes L(Gi T) = logP(Gi T).
    We derive L(Gi  |T) from (1) via Bayes&#8217; Rule, which is: However, we first make two further assumptions: Substituting (1) into (2), simplifying, and taking logarithm