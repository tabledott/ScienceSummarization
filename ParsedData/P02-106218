l zeros.
    The algorithm then makes a pass over the training set, at each training example storing a parameter vector for .
    The parameter vector is only modified when a mistake is made on an example.
    In this case the update is very simple, involving adding the difference of the offending examples&#8217; representations ( in the figure).
    See (Cristianini and Shawe-Taylor 2000) chapter 2 for discussion of the perceptron algorithm, and theory justifying this method for setting the parameters.
    In the most basic form of the perceptron, the parameter values are taken as the final parameter settings, and the output on a new test example with for is simply the highest .
    Input: A set of candidates for , A sequence of parameter vectors for Initialization: Set for ( stores the number of votes for ) scoring candidate under these parameter values, i.e., where .
    (Freund &amp; Schapire 1999) describe a refinement of the perceptron, the voted perceptron.
    The training phase is identical to that i