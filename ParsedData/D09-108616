ansferred from one language to another.
    In this section, we evaluate the extent of this direct projection on a small handannotated corpus.
    In &#167;5, we will use a QG generative model to learn dependency parsers from bitext when there are no annotations in the target language.
    Finally, in &#167;6,we show how QG features can augment a target-language parser trained on a small set of labeled trees.
    For syntactic annotation projection to work at all, we must hypothesize, or observe, that at least some syntactic structures are preserved in translation.
    Hwa et al. (2005) have called this intuition the Direct Correspondence Assumption (DCA, with slight notational changes): Given a pair of sentences w and w' that are translations of each other with syntactic structure t and t', if nodes x' and y' of t' are aligned with nodes x and y of t, respectively, and if syntactic relationship R(x', y') holds in t', then R(x, y) holds in t. The validity of this assumption clearly depends on the node-to-node