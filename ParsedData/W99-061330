coverage (it is seen on every example) but a low, baseline precision.
    When this feature type was included, CoBoost chose this default feature at an early iteration, thereby giving non-abstaining pseudo-labels for all examples, with eventual convergence to the two classifiers agreeing by assigning the same label to almost all examples.
    Again, this deserves further investigation.
    Finally, we would like to note that it is possible to devise similar algorithms based with other objective functions than the one given in Equ.
    (7), such as the likelihood function used in maximum-entropy problems and other generalized additive models (Lafferty 99).
    We are currently exploring such algorithms.
  
  
    The Expectation Maximization (EM) algorithm (Dempster, Laird and Rubin 77) is a common approach for unsupervised training; in this section we describe its application to the named entity problem.
    A generative model was applied (similar to naive Bayes) with the three labels as hidden vanables on un