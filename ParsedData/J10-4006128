er-rank matrix of the same size, and they have been shown to be effective in many semantic tasks, probably because they provide a beneficial form of smoothing of the dimensions.
    See Turney and Pantel (2010) for references and discussion.
    We can apply SVD (or similar methods) to any of the tensorderived matrices we used for the tasks herein.
    An interesting alternative is to smooth the source tensor directly by a tensor decomposition technique.
    In this section, we present (very preliminary) evidence that tensor decomposition can improve performance, and it is at least as good in this respect as matrix-based SVD.
    This is the only experiment in which we operate on the tensor directly, rather than on the matrices derived from it, paving the way to a more active role for the underlying tensor in the DM approach to semantics.
    The (truncated) Tucker decomposition of a tensor can be seen as a higher-order generalization of SVD.
    Given a tensor X of dimensionality I1 x I2 x I3, its n-rank Rn 