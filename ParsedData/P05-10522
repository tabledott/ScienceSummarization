structure, can in principle capture additional generalizations and thus lead to the identification of additional instances of relations.
    However, a general problem in Natural Language Processing is that as the processing gets deeper, it becomes less accurate.
    For instance, the current accuracy of tokenization, chunking and sentence parsing for English is about 99%, 92%, and 90% respectively.
    Algorithms based solely on deeper representations inevitably suffer from the errors in computing these representations.
    On the other hand, low level processing such as tokenization will be more accurate, and may also contain useful information missed by deep processing of text.
    Systems based on a single level of representation are forced to choose between shallower representations, which will have fewer errors, and deeper representations, which may be more general.
    Based on these observations, Zhao et al. (2004) proposed a discriminative model to combine information from different syntactic sources