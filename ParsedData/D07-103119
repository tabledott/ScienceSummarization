; eventually the distribution of state sequences converges to the desired posterior.
			Each iteration of the Gibbs sampler is much faster than the Forward-Backward algorithm (both take time linear in the length of the string, but for an HMM with s hidden states, each iteration of the Gibbs sampler takes O(s) time while each iteration of the Forward-Backward algorithm takes O(s2) time), so we ran 50,000 iterations of all samplers (which takes roughly the same elapsed time as 1,000 Forward-Backward iterations).
			As can be seen from Table 1, the posterior state sequences we obtained are not particularly good.Further, when we examined how the posterior like lihoods varied with increasing iterations of Gibbs sampling, it became apparent that the likelihood was still increasing after 50,000 iterations.
			Moreover,when comparing posterior likelihoods from different runs with the same prior parameters but differ ent random number seeds, none of the likelihoods crossed, which one would expect if the samplers had c