to which it is measured, it is important that the text be representative of that for which the language model is intended.
    Because perplexity is subject to sampling error, making fine distinctions between language models may require that the perplexity be measured with respect to a large sample.
    In an n-gram language model, we treat two histories as equivalent if they end in the same n - 1 words, i.e., we assume that for k &gt; n, Pr (wk w1k-1) is equal to Pr (wk I wifcin1+1).
    For a vocabulary of size V, a 1-gram model has V - 1 independent parameters, one for each word minus one for the constraint that all of the probabilities add up to 1.
    A 2-gram model has V(V - 1) independent parameters of the form Pr (102 I wi ) and V - 1 of the form Pr (w) for a total of V2 - 1 independent parameters.
    In general, an n-gram model has V&amp;quot; - 1 independent parameters: V&amp;quot;-1(V - 1) of the form Pr (wn I wr1), which we call the order-n parameters, plus the 17n-1-1 parameters of an (n - 1)-gr