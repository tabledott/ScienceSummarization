 brevity we refer the reader to (Artstein and Poesio, 2005) and (Hripcsak and Rothschild, 2004) for formulation and discussion of n and Frel The two metrics are based on different assumptions about the nature of the annotation task.
    Frel is founded on the premise that the task is to recognise and label spec sentences from within a background population, and does not explicitly model agreement on nspec instances.
    It ranges from 0 (no agreement) to 1 (no disagreement).
    Conversely, n gives explicit credit for agreement on both spec and nspec instances.
    The observed agreement is then corrected for &#8216;chance agreement&#8217;, yielding a metric that ranges between &#8722;1 and 1.
    Given our definition of hedge classification and assessing the manner in which the annotation was carried out, we suggest that the founding assumption of Frel 1 fits the nature of the task better than that of n. Following initial agreement calculation, the instances of disagreement were examined.
    It turned out t