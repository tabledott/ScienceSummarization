 initial grammar consists of all possible CNF rules over five nonterminals and the terminals a and b (135 rules), with random rule probabilities.
    As shown in Figure 1, with an unbracketed training set W the cross-entropy estimate H(W, GB) remains almost unchanged after 40 iterations (from 1.57 to 1.44) and no useful solution is found.
    In contrast, with a fully bracketed version C of the same training set, the cross-entropy estimate H(W,GB) decreases rapidly (1.57 initially, 0.88 after 21 iterations).
    Similarly, the cross-entropy estimate H(C, GB) of the bracketed text with respect to the grammar improves rapidly (2.85 initially, 0.89 after 21 iterations).
    The inferred grammar models correctly the palindrome language.
    Its high probability rules (p &gt; 0.1, pip' &gt; 30 for any excluded rule p') are which is a close to optimal CNF CFG for the palindrome language.
    The results on this grammar are quite sensitive to the size and statistics of the training corpus and the initial rule probab