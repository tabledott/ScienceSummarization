to IBM&#8217;s Model 1, in that they both take into account only the word types that participate in a given link.
    IBM Model 1 uses P(f|e), the probability of f being generated by e, while our model uses P(l|e, f), the probability of a link existing between e and f. In this experiment, we set Model 1 translation probabilities according to our initial 02 alignment, sampling as we described in Section 4.2.
    We then use the M=1 P(fj|eaj) to evaluate candidate alignments in a search that is otherwise identical to our algorithm.
    We ran Model 1 refinement for three iterations and recorded the best results that it achieved.
    It is clear from Table 4 that refining our initial 02 alignment using IBM&#8217;s Model 1 is less effective than using our model in the same manner.
    In fact, the Model 1 refinement receives a lower score than our initial alignment.
  
  
    When viewed with no features, our probability model is most similar to the explicit noise model defined in (Melamed, 2000).
    In fact, Me