in our notation, p(y).
    The original Metropolis algorithm is also a special case of the Metropolis-Hastings algorithm, in which the proposal probability is symmetric, that is, g(x,y) g(y, x).
    The acceptance function then reduces to min(1,71-(y)/ir(x)), which is min(1,q(y)/q(x)) in our notation.
    I mention this only to point out that it is a different special case.
    Our proposal probability is not symmetric, but rather independent of the previous configuration, and though our acceptance function reduces to a form (4) that is similar to the original Metropolis acceptance function, it is not the same: in general, 49(y) / 0(x) q(y)/q(x). distribution diverges from the initial context-free approximation, the more features will be necessary to &amp;quot;correct&amp;quot; it, and the more random sampling will be called on.
    A second issue is incomplete data.
    The approach described here assumes complete data (a parsed training corpus).
    Fortunately, an extension of the method to handle incomple