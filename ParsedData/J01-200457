 improved more than the accuracy in this case (as was also seen in Figure 5), seems to indicate that this additional information is causing the distribution to become more peaked, so that fewer analyses are making it into the beam.
    Reduction in average precision/recall error, number of rule expansions, and perplexity as conditioning increases.
    Table 4 compares the perplexity of our model with Chelba and Jelinek (1998a, 1998b) on the same training and testing corpora.
    We built an interpolated trigram model to serve as a baseline (as they did), and also interpolated our model's perplexity with the trigram, using the same mixing coefficient as they did in their trials (taking 36 percent of the estimate from the trigram).'
    The trigram model was also trained on Sections 00-20 of the C&amp;J corpus.
    Trigrams and bigrams were binned by the total count of the conditioning words in the training corpus, and maximum likelihood mixing coefficients were calculated for each bin, to mix the trigram with 