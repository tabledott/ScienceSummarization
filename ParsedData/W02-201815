or purposes of comparison.
    Finally, it should be noted that in the current implementation, we have not applied any of the possible optimizations that appear in the literature (Lafferty and Suhm, 1996; Wu and Khudanpur, 2000; Lafferty et al., 2001) to speed up normalization of the probability distribution q.
    These improvements take advantage of a model&#8217;s structure to simplify the evaluation of the denominator in (1).
    The particular data sets examined here are unstructured, and such optimizations are unlikely to give any improvement.
    However, when these optimizations are appropriate, they will give a proportional speed-up to all of the algorithms.
    Thus, the use of such optimizations is independent of the choice of parameter estimation method.
    To compare the algorithms described in &#167;2, we applied the implementation outlined in the previous section to four training data sets (described in Table 1) drawn from the domain of natural language processing.
    The &#8216;rules&#8217; 