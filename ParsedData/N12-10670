
  Better Evaluation for Grammatical Error Correction
  
    We present a novel method for evaluating grammatical error correction.
    The core of method, which we call is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the goldstandard annotation.
    This optimal edit seis subsequently scored using mea- We test our on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction.
  
  
    Progress in natural language processing (NLP) research is driven and measured by automatic evaluation methods.
    Automatic evaluation allows fast and inexpensive feedback during development, and objective and reproducible evaluation during testing time.
    Grammatical error correction is an important NLP task with useful applications for second language learning.
    Evaluation for error correction is typically done by computing F1