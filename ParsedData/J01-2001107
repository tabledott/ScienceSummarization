an always make an arbitrarily small compression of a given set of data, if we are allowed to make the grammar arbitrarily complex, to match and, potentially, to overfit the data, and it is untenable because generative grammar offers no explicit notion of how well a grammar must match the training data.
    MDL's insight is that it is possible to make explicit the trade-off between complexity of the analysis and snugness of fit to the data-corpus in question.
    The first tool in that computational trade-off is the use of a probabilistic model to compress the data, using stock tools of classical information theory.
    These notions were rejected as irrelevant by early workers in early generative grammar (Goldsmith 2001).
    Notions of probabilistic grammar due to Solomonoff (1995) were not integrated into that framework, and the possibility of using them to quantify the goodness of fit of a grammar to a corpus was not exploited.
    It seems to me that it is in this context that we can best understand the w