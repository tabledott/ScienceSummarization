formation is by considering features from bothmodels as part of one large log-linear model.
			However, by including more and less informative features in one model, we may transfer too much ex planatory power to the more specific features.
			Toovercome this problem, Smith et al (2006) demon strated that using ensembles of separately trainedmodels and combining them in a logarithmic opin ion pool (LOP) leads to better parameter values.
			This approach was used as the second way in which 14 we combined our models.
			An ensemble of log-linearmodels was combined using a multiplicative con stant ? which we train manually using held out data.
			t? ? M? m=1 ?mhm(swc, tw) + ?
			( N?
			n=1 ?nhn(sw, tw) )Typically, the two models would need to be normalised before being combined, but here the multi plicative constant fulfils this ro?le by balancing theirseparate contributions.
			This is the first work suggesting the application of LOPs to decoding in ma chine translation.
			In the future more sophisticated tra