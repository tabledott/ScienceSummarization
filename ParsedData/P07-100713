;i, the EM algorithm provides the following iterative steps: where Equation (4) represents the expectation Estep, Equation (5) represents the maximization Mstep, and N represents the number of instances in DU.
    Note that the probabilities bpL(&#969;i|xk) and bpL(&#969;i) in Equation (4) will stay the same throughout the iterations for each particular instance xk and class &#969;i.
    The new a posteriori probabilities bp(s)(&#969;i|xk) at step s in Equation (4) are simply the a posteriori probabilities in the conditions of the labeled data, bpL(&#969;i|xk), weighted by the ratio of the new priors bp(s)(&#969;i) to the old priors bpL(&#969;i).
    The denominator in Equation (4) is simply a normalizing factor.
    The a posteriori bp(s)(&#969;i|xk) and a priori probabilities bp(s)(&#969;i) are re-estimated sequentially during each iterations for each new instance xk and each class &#969;i, until the convergence of the estimated probabilities bp(s)(&#969;i), which will be our estimated sense priors.
    Thi