n and Marcus 91; Briscoe and Carroll 93) conditioned probabilities on non-terminal labels and part of speech tags alone.
    The SPATTER parser (Magerman 95; Jelinek et al. 94) does use lexical information, and recovers labeled constituents in Wall Street Journal text with above 84% accuracy &#8212; as far as we know the best published results on this task.
    This paper describes a new parser which is much simpler than SPATTER, yet performs at least as well when trained and tested on the same Wall Street Journal data.
    The method uses lexical information directly by modeling head-modifier' relations between pairs of words.
    In this way it is similar to 'By 'modifier' we mean the linguistic notion of either an argument or adjunct.
    Link grammars (Lafferty et al. 92), and dependency grammars in general.
  
  
    The aim of a parser is to take a tagged sentence as input (for example Figure 1(a)) and produce a phrase-structure tree as output (Figure 1(b)).
    A statistical approach to this problem co