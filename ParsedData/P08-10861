guage model assigns a probability P(w) to any given string of words wm1 = w1, ..., wm.
    In the case of n-gram language models this is done by factoring the probability: do not differ in the last n &#8722; 1 words, one problem ngram language models suffer from is that the training data is too sparse to reliably estimate all conditional probabilities P(wi|wi&#8722;1 Class-based n-gram models are intended to help overcome this data sparsity problem by grouping words into equivalence classes rather than treating them as distinct words and thus reducing the number of parameters of the model (Brown et al., 1990).
    They have often been shown to improve the performance of speech recognition systems when combined with word-based language models (Martin et al., 1998; Whittaker and Woodland, 2001).
    However, in the area of statistical machine translation, especially in the context of large training corpora, fewer experiments with class-based n-gram models have been performed with mixed success (Raab, 2006).
   