 of Wall Street Journal material.'
    We smoothed these estimates according to class membership for proper names and numbers, and according to an extended version of the enhanced GoodTuring method (Church and Gale, 1991) for the remaining words.
    The latter smoothing operation not only optimally regresses the probabilities of seen ngrams but also assigns a non-zero probability to all unseen n-grams which depends on how likely their component m-grams (m &lt; n, i.e., words and bigrams) are.
    The resulting conditional probabilities are converted to log-likelihoods for reasons of numerical accuracy and used to estimate the overall probability P(S) of any English sentence S according to a Markov assumption, i.e., Because both equations would assign lower and lower probabilities to longer sentences and we need to compare sentences of different lengths, a heuristic strictly increasing function of sentence length, f(/) 0.5/, is added to the log-likelihood estimates.
  
  
    Our first goal was to integrate t