 divided into four sets; language mod els are trained for each set separately4 . For eachtraining data size, we report the size of the result ing language model, the fraction of 5-grams from the test data that is present in the language model, and the BLEU score (Papineni et al, 2002) obtainedby the machine translation system.
			For smaller train ing sizes, we have also computed test-set perplexityusing Kneser-Ney Smoothing, and report it for com parison.
			7.1 Data Sets.
			We compiled four language model training data sets, listed in order of increasing size: 3One additional round for the sentence end marker.
			4Experience has shown that using multiple, separately trained language models as feature functions in Eq (1) yields better results than using a single model trained on all data.
			863 1e+07 1e+08 1e+09 1e+10 1e+11 1e+12 10 100 1000 10000 100000 1e+06 0.1 1 10 100 1000 N um be r o f n -g ra m s Ap pr ox . L M s ize in G B LM training data size in million tokens x1.8/x2 x1.8/x2 x1.8/x2 x1.6/x2 targ