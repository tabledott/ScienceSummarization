hine translation system may be very different from the domain of the system&#8217;s training data.
    For the WMT 2007 shared task, the challenge was to use a large amount of out-of-domain training data Proceedings of the Second Workshop on Statistical Machine Translation, pages 224&#8211;227, Prague, June 2007. c&#65533;2007 Association for Computational Linguistics (about 40 million words) combined with a much smaller amount of in-domain training data (about 1 million words) to optimize translation performance on that particular domain.
    We carried out these experiments on French&#8211;English.
    The first baseline system is trained only on the outof-domain Europarl corpus, which has the following corpus statistics: The second baseline system is trained only on the in-domain NewsCommentary corpus.
    This corpus is much smaller: French English Sentences 42,884 Words 1,198,041 1,018,503 To make use of all the training data, the straightforward way is to simply concatenate the two training corpora and 