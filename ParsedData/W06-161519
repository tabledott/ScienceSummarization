es of the three models with varying amounts of WSJ training data.
    With one hundred sentences of training data, structural correspondence learning gives a 19.1% relative reduction in error over the supervised baseline, and it consistently outperforms both baseline models.
    Figure 5(b) gives results for 40,000 sentences, and Figure 5(c) shows corresponding significance tests, with p &lt; 0.05 being significant.
    We use a McNemar paired test for labeling disagreements (Gillick and Cox, 1989).
    Even when we use all the WSJ training data available, the SCL model significantly improves accuracy over both the supervised and ASO baselines.
    The second column of Figure 5(b) gives unknown word accuracies on the biomedical data.
    Of thirteen thousand test instances, approximately three thousand were unknown.
    For unknown words, SCL gives a relative reduction in error of 19.5% over Ratnaparkhi (1996), even with 40,000 sentences of source domain training data.
    In this section we give results for 