
  Dependency Parsing by Belief Propagation
  
    We formulate dependency parsing as a graphical model with the novel ingredient of global constraints.
    We show how to apply loopy belief propagation (BP), a simple and tool for and inference.
    As a parsing algorithm, BP is both asymptotically and empirically efficient.
    Even with second-order features or latent variables, which would make exact parsing considerslower or NP-hard, BP needs only with a small constant factor.
    Furthermore, such features significantly improve parse accuracy over exact first-order methods.
    Incorporating additional features would increase the runtime additively rather than multiplicatively.
  
  
    Computational linguists worry constantly about runtime.
    Sometimes we oversimplify our models, trading linguistic nuance for fast dynamic programming.
    Alternatively, we write down a better but intractable model and then use approximations.
    The CL community has often approximated using heavy pruning or rerankin