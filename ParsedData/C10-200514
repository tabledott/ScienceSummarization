 source is biased towards thesewords.
			This is probably the reason why this website outputs such fewer number of tweets com pared to the other websites (see Table 1) as well as why its data has the smallest entropy among the sources (see Table 2).
			The quality of the data and its individual bias have certainly impact in the combination of labels.
			However, there is other important aspect that oneneeds to consider: different bias between the labelers.
			For instance, if labelers a and b make similar decisions, we expect that combining their labels would not bring much improvement.
			There fore, the diversity of labelers is a key element incombining them (Polikar, 2006).
			One way to mea sure this is by calculating the agreement between the labels produced by the labelers.
			We use the kappa coefficient (Cohen, 1960) to measure thedegree of agreement between two sources.
			Ta ble 3 presents the coefficients for each par of data source.
			All the coefficients are between 0.4 and0.6, which represents 