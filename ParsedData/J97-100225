ment boundaries or nonboundaries.
    Thus for both classification and segmentation, the basic question is what level of agreement coders reach under the reliability tests.
    It has been argued elsewhere (Carletta 1996) that since the amount of agreement one would expect by chance depends on the number and relative frequencies of the categories under test, reliability for category classifications should be measured using the kappa coefficient.'
    Even with a good yardstick, however, care is needed to determine from such figures whether or not the exhibited agreement is acceptable, as Krippendorff (1980) explains.
    Reliability in essence measures the amount of noise in the data; whether or not that will interfere with results depends on where the noise is and the strength of the relationship being measured.
    As a result, Krippendorff warns against taking overall reliability figures too seriously, in favor of always calculating reliability with respect to the particular hypothesis under test.
    Usin