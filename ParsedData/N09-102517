ill fill it.
    On the other hand, splitting at a period is a safe bet, and frees the model to use rules that dig deeper into NP and VP trees when constructing a top-level S. Table 5 shows weights for generated English nonterminals: SBAR-C nodes are rewarded and commas are punished.
    The combined effect of all weights is subtle.
    To interpret them further, it helps to look at gross changes in the system&#8217;s behavior.
    For example, a major error in the baseline system is to move &#8220;X said&#8221; or &#8220;X asked&#8221; from the beginning of the Chinese input to the middle or end of the English translation.
    The error occurs with many speaking verbs, and each time, we trace it to a different rule.
    The problematic rules can even be non-lexical, e.g.
    : It is therefore difficult to come up with a straightforward feature to address the problem.
    However, when we apply MIRA with the features already listed, these translation errors all disappear, as demonstrated by examples 4&#8211;5