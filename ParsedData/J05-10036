ees that were not included in the original model.
    The baseline model achieved 88.2% F-measure on this task.
    The new model achieves 89.75% Fmeasure, a 13% relative decrease in F-measure error.
    Although the experiments in this article are on natural language parsing, the approach should be applicable to many other natural language processing (NLP) problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.
    See Collins (2002a) for an application of the boosting approach to named entity recognition, and Walker, Rambow, and Rogati (2001) for the application of boosting techniques for ranking in the context of natural language generation.
    The article also introduces a new, more efficient algorithm for the boosting approach which takes advantage of the sparse nature of the feature space in the parsing data.
    Other NLP tasks are likely to have similar characteristics in terms of sparsity.
    Experiments show an eff