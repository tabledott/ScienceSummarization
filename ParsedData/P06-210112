    Most systems should be evaluated and preferably trained on less harsh metrics.
  
  
    To balance the advantages of direct loss minimization, continuous risk minimization, and convex optimization, deterministic annealing attempts the solution of increasingly difficult optimization problems (Rose, 1998).
    Adding a scale hyperparameter &#947; to equation (1), we have the following family of distributions: When &#947; = 0, all yi,k are equally likely, giving the uniform distribution; when &#947; = 1, we recover the model in equation (1); and as &#947; &#8212;* oc, we approach the winner-take-all Viterbi function that assigns probability 1 to the top-scoring analysis.
    For a fixed &#947;, deterministic annealing solves 2An alternative would be to artificially add yz (e.g., the reference translation(s)) to the hypothesis set during training.
    We then increase &#947; according to some schedule and optimize &#952; again.
    When &#947; is low, the smooth objective might allow us to pass over local mi