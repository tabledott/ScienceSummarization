the last word in the n-gram is corrupted.
  
  
    We evaluate the hypothesis that one can take an existing, near state-of-the-art, supervised NLP system, and improve its accuracy by including word representations as word features.
    This technique for turning a supervised approach into a semi-supervised one is general and task-agnostic.
    However, we wish to find out if certain word representations are preferable for certain tasks.
    Lin and Wu (2009) finds that the representations that are good for NER are poor for search query classification, and vice-versa.
    We apply clustering and distributed representations to NER and chunking, which allows us to compare our semi-supervised models to those of Ando and Zhang (2005) and Suzuki and Isozaki (2008).
    Chunking is a syntactic sequence labeling task.
    We follow the conditions in the CoNLL-2000 shared task (Sang &amp; Buchholz, 2000).
    The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker.
    In