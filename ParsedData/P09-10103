cy is modeled in a log-linear fashion, allowing us to incorporate features of both the instruction text and the environment.
    We employ a policy gradient algorithm to estimate the parameters of this model.
    We evaluate our method on two distinct applications: Windows troubleshooting guides and puzzle game tutorials.
    The key findings of our experiments are twofold.
    First, models trained only with simple reward signals achieve surprisingly high results, coming within 11% of a fully supervised method in the Windows domain.
    Second, augmenting unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain.
    These results indicate the power of learning from this new form of automated supervision.
  
  
    Grounded Language Acquisition Our work fits into a broader class of approaches that aim to learn language from a situated context (Mooney, 2008a; Mooney, 2008b; Fleischman and Roy, 2005; Yu and Ballard, 2004; Siskind, 200