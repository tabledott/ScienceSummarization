diagram in Figure 2.
    Each conversation C is a sequence of acts a, and each act produces a post, represented by a bag of words shown using the W plates.
    The number of acts available to the model is fixed; we experimented with between 5 and 40.
    Starting with a random assignment of acts, we train our conversation model using EM, with forwardbackward providing act distributions during the expectation step.
    The model structure in Figure 2 is sadly no. some pasta bake, but coffee and pasta bake is not a contender for tea and toast... . yum!
    Ground beef tacos?
    We &#8217;re grilling out.
    Turkey dogs for me, a Bubba Burger for my dh, and combo for the kids. ha!
    They gotcha!
    You had to think about Arby&#8217;s to write that tweet.
    Arby&#8217;s is conducting a psychlogical study.
    Of roast beef.
    Rumbly tummy soon to be tamed by Dominos for lunch!
    Nom nom nom! similar to previous HMMs for supervised dialogue act recognition (Stolcke et al., 2000), but our model is traine