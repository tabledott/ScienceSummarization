allowing the analysis of subjective translation quality measures for different systems, the judgments gathered during the manual evaluation may be used to evaluate how well the automatic evaluation metrics serve as a surrogate to the manual evaluation processes.
    NIST began running a &#8220;Metrics for MAchine TRanslation&#8221; challenge (MetricsMATR), and presented their findings at a workshop at AMTA (Przybocki et al., 2008).
    This year we conducted a joint MetricsMATR and WMT workshop, with NIST running the shared evaluation task and analyzing the results.
    In this year&#8217;s shared evaluation task 14 different research groups submitted a total of 26 different automatic metrics for evaluation: (Dobrinkat et al., 2010) ? indicates a constrained win, no other constrained system is statistically better.
    For all pairwise comparisons between systems, please check the appendix.
    System combinations are listed in the order of how often their translations were ranked higher than or equal to any 