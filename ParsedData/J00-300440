ining file.
    The four models were tested on the 10 randomly-chosen 1,000-word Rocling segments used before.
    The results for the individual test files, in terms of error rate per thousand words, are shown in Figure 9 and summarized in Table 6.
    Larger training sets generally give smaller error, which is only to be expected&#8212;although the results for some individual test files flatten out and show no further improvement with larger training files, and in some cases more training data actually increases the number of errors.
    Overall, the error rate is reduced by about 25% for each doubling of the training data.
    We have experimented with compression models of different orders on the PH corpus.
    Generally speaking, compression of text improves as model order increases, up to a point determined by the logarithm of the size of the training text.
    Typically, little compression is gained by going beyond order 5 models.
    For segmentation, we observe many errors when a model of order 1 is 