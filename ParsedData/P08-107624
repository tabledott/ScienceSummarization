emi and JESS-CM have an important common feature.
    That is, both methods discriminatively combine models trained by using unlabeled data in order to create informative feature representation for discriminative learning.
    Unlike self/co-training approaches (Blum and Mitchell, 1998), which use estimated labels as &#8216;correct labels&#8217;, this approach automatically judges the reliability of additional features obtained from unlabeled data in terms of discriminative training.
    Ando and Zhang (2007) have also pointed out that this methodology seems to be one key to achieving higher performance in NLP applications.
    There is an approach that combines individually and independently trained joint PMs into a discriminative model (Li and McCallum, 2005).
    There is an essential difference between this method and JESSCM.
    We categorize their approach as an &#8216;indirect approach&#8217; since the outputs of the target task, y, are not considered during the unlabeled data incorporation.
    Note t