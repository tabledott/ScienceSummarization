nd for the current set (although the score could be decreased by a smarter mapping from the HTER range to the Effort range).
    We argue that since the HTER metric is considered a good approximation for the effort required in post-editing, effort-like scores derived from the HTER score provide a good way to compute oracle scores in a deterministic manner.
    Note that again the oracle based on (H)BLEU gives a worse MAE score at 0.61, which support the interpretation that the (H)BLEU metric provides a lower correlation to post-editing effort compared to (H)TER.
    Overall, we consider the MAE values for these HTER and (H)BLEU-based oracles to indicate high error margins.
    Most notably the performance of the best system gets the same MAE score as the (H)BLEU oracle, at 0.61 MAE.
    We take this to mean that the scoring task is more difficult compared to the ranking task, since even oracle-based solutions get high error scores.
    When looking back at the goals that we identified for this shared-task, mo