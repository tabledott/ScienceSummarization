ke weeks or months to finish.
    This is a big problem because developers of machine translation systems need to monitor the effect of daily changes to their systems in order to weed out bad ideas from good ideas.
    We believe that MT progress stems from evaluation and that there is a logjam of fruitful research ideas waiting to be released from 1So we call our method the bilingual evaluation understudy, BLEU. the evaluation bottleneck.
    Developers would benefit from an inexpensive automatic evaluation that is quick, language-independent, and correlates highly with human evaluation.
    We propose such an evaluation method in this paper.
    How does one measure translation performance?
    The closer a machine translation is to a professional human translation, the better it is.
    This is the central idea behind our proposal.
    To judge the quality of a machine translation, one measures its closeness to one or more reference human translations according to a numerical metric.
    Thus, our MT evalu