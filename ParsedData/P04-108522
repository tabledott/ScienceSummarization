tain a more general one.
    Such a model is shown in Figure 1(b), a Bayesian network model that is well-understood and that has precisely defined semantics.
    To this Bayesian network representation, we apply maximum entropy modeling to define a probability distribution at each node ( ) dependent on the observation variable and the five contextual tags used in the four pragmatic dependencies.8 For notational simplicity, the contextual tags representing these pragmatic dependencies are represented here as a vector ( ,, and so on).
    Given feature functions Again, the only role of the denominator is to ensure that sums to 1, and need not be computed when searching for the most probable tags.
    Note that in our case, the structure of the Bayesian network is known and need not be inferred, since AP identification is performed before the actual agreement and disagreement classification.
    Since tag sequences are known during training, the inference of a model for sequence labels is no more difficult than 