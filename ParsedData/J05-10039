nd Lafferty 1997; Papineni, Roukos, and Ward 1997, 1998) require several full passes over the training set for each round of feature selection, suggesting that at least for the parsing data, the improved boosting algorithm is several orders of magnitude more efficient.1 In section 6.4 we discuss our approach in comparison to these earlier methods for feature selection, as well as the more recent work of McCallum (2003); Zhou et al. (2003); and Riezler and Vasserman (2004).
    The remainder of this article is structured as follows.
    Section 2 reviews historybased models for NLP and highlights the perceived shortcomings of history-based models which motivate the reranking approaches described in the remainder of the article.
    Section 3 describes previous work (Friedman, Hastie, and Tibshirani 2000; Duffy and Helmbold 1999; Mason, Bartlett, and Baxter 1999; Lebanon and Lafferty 2001; Collins, Schapire, and Singer 2002) that derives connections between boosting and maximum-entropy models for the simpler ca