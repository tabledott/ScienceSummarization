
    While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments (Papineni et al., 2002; Doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgments; correlation studies have been made of individual components (Bangalore et al., 2000), but not of systems.
    In this paper we present an empirical study of how well various corpus-based metrics agree with human judgments, when evaluating several NLG systems that generate sentences which describe changes in the wind (for weather forecasts).
    These systems do not perform content determination (they are limited to microplanning and realisation), so our study does not address corpus-based evaluation of content determination.
  
  
    NLG systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998).
    NLG evaluations have tended to be of the intrinsic type (Sparck Jones and Galliers, 1