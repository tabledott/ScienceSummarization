ve than in the ExpLoss case.
    The procedure for finding the optimal value BestWt(k, &#175;a) must be applied for each feature which co-occurs with the chosen feature k*.
    For example, the iterative scaling procedure described above must be applied for a number of features.
    For each feature, this will involve recalculation of the distribution {P(xi,1 1 si),P(xi,2 I si), ...,P(xi,ni I si)I for each example i on which the feature occurs.13 It takes only one feature that is seen on all training examples for the algorithm to involve recalculation of P(xi,j I si) for the entire training set.
    This contrasts with the simple updates in the improved boosting algorithm (W+k = W+k + D and Wk = Wk + D).
    In fact in the parsing experiments, we were forced to give up on the LogLoss feature selection methods because of their inefficiency (see section 6.4 for more discussion about efficiency).
    &#732;hk &#732;pk(&#175;a').
    Note, however, that approximate methods for finding the best feature and updatin