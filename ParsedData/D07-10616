; Strube and Ponzetto, 2006) have reported stronger results on some tasks than have been achieved by existing measures based on shallower lexical resources.
			The results of our algorithm are competitivewith some Wikipedia algorithms while using only Word Net 2.1 as the underlying lexical resource.
			The approach presented here is generalizable to construction from any underlying semantic resource.PageRank is the most well-known example of a random walk Markov chain?see (Berkhin, 2005) for a sur vey.
			It uses the local hyperlink structure of the web to define a graph which it walks to aggregate popularityinformation for different pages.
			Recent work has ap plied random walks to NLP tasks such as PP attachment(Toutanova et al, 2004), word sense disambiguation (Mi halcea, 2005; Tarau et al, 2005), and query expansion (Collins-Thompson and Callan, 2005).
			However, to ourknowledge, the literature in NLP has only considered us ing one stationary distribution per specially-constructedgraph as a probability 