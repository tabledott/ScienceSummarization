 MC-SAT, like most existing relational inference algorithms, grounds all predicates and clauses, thus requiring memory and time exponential in the predicate and clause arities.
    We developed a general method for producing a &#8220;lazy&#8221; version of relational inference algorithms (Poon &amp; Domingos, 2008), which carries exactly the same inference steps as the original algorithm, but only maintains a small subset of &#8220;active&#8221; predicates/clauses, grounding more as needed.
    We showed that Lazy-MC-SAT, the lazy version of MC-SAT, reduced memory and time by orders of magnitude in several domains.
    We use Lazy-MC-SAT in this paper.
    Supervised learning for Markov logic maximizes the conditional log-likelihood L(x, y) = log P(Y = yJX = x), where Y represents the non-evidence predicates, X the evidence predicates, and x, y their values in the training data.
    For simplicity, from now on we omit X, whose values are fixed and always conditioned on.
    The optimization problem is convex 