t they agreed that A &gt; B, A = B, or A &lt; B. Intra-annotator agreement was computed similarly, but we gathered items that were annotated on multiple occasions by a single annotator.
    Table 4 gives K values for inter-annotator and intra-annotator agreement.
    These give an indication of how often different judges agree, and how often single judges are consistent for repeated judgments, respectively.
    The interpretation of Kappa varies, but according to Landis and Koch (1977), 0 &#8722; .2 is slight, .2 &#8722; .4 is fair, .4 &#8722; .6 is moderate, .6 &#8722;.8 is substantial and the rest almost perfect.
    Based on these interpretations the agreement for yes/no judgments is moderate for inter-annotator agreement and substantial for intra-annotator agreement, but the inter-annotator agreement for sentence level ranking is only fair.
    We analyzed two possible strategies for improving inter-annotator agreement on the ranking task: First, we tried discarding initial judgments to give tors&#8217; i