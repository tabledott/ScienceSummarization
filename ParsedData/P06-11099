 in our corpus, there would be no difference in probability between the five parse trees in figure 4.
    However, if we also have a different sentence where JJ NNS (heavy losses) appears in a different context, e.g. in Heavy losses were reported, its covering subtree gets a relatively higher frequency and the parse tree where heavy losses occurs as a constituent gets a higher total probability.
  
  
    ML-DOP (Bod 2000) extends DOP with a maximum likelihood reestimation technique based on the expectation-maximization (EM) algorithm (Dempster et al. 1977) which is known to be statistically consistent (Shao 1999).
    ML-DOP reestimates DOP's subtree probabilities in an iterative way until the changes become negligible.
    The following exposition of ML-DOP is heavily based on previous work by Bod (2000) and Magerman (1993).
    It is important to realize that there is an implicit assumption in DOP that all possible derivations of a parse tree contribute equally to the total probability of the parse tree.
 