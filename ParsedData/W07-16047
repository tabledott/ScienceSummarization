n shown to perform well in combining heterogeneous forms of evidence, as in word sense disambiguation (Ratnaparkhi, 1998).
    It also has the desirable property of handling interactions among features without having to rely on the assumption of feature independence, as in a Naive Bayesian model.
    Our ME model was trained on 7 million &#8220;events&#8221; consisting of an outcome (the preposition that appeared in the training text) and its associated context (the set of feature-value pairs that accompanied it).
    These 7 million prepositions and their contexts were extracted from the MetaMetrics corpus of 1100 and 1200 Lexile text (11th and 12th grade) and newspaper text from the San Jose Mercury News.
    The sentences were then POS-tagged (Ratnaparkhi, 1998) and then chunked into noun phrases and verb phrases by a heuristic chunker.
    The maximum entropy model was trained with 25 contextual features.
    Some of the features represented the words and tags found at specific locations adjacent to the p