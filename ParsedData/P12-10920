
	Improving Word Representations via Global Context and Multiple Word Prototypes
		Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems.
		However, most of these models arebuilt with only local context and one represen tation per word.
		This is problematic becausewords are often polysemous and global con text can also provide useful information for learning word meanings.
		We present a new neural network architecture which 1) learnsword embeddings that better capture the se mantics of words by incorporating both local and global document context, and 2) accountsfor homonymy and polysemy by learning mul tiple embeddings per word.
		We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate ourmodel on it, showing that our model outper forms competitive baselines and other neural language models.
		1
	
	
			Vector-space models (VSM) represent word meanings with vectors that capt