originally was that company names were not separated from their post-modifier (such as Corp).
    Besides this, it suggests that there is not a great deal of difference between an annotator just learning the task, and one who has had a great deal of experience with it.
    We have also evaluated how well the suggestion feature of the annotation tool performs.
    In particular, we want to determine how useful named entities are in determining the correct bracketing.
    We ran the tool over the original corpus, following NE-based suggestions where possible.
    We find that when evaluated against our annotations, the Fscore is 50.71%.
    We need to look closer at the precision and recall though, as they are quite different.
    The precision of 93.84% is quite high.
    However, there are many brackets where the entities do not help at all, and so the recall of this method was only 34.74%.
    This suggests that a NE feature may help to identify the correct bracketing in one third of cases.
  
  
    Having 