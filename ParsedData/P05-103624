esired.
    We would like to come up with a better utility function than a simple weighting parameter for our supervised version.
    The unsupervised version probably can also be further improved.
    We achieved much success using syntactic labels to constrain compressions, and there are surely other constraints that can be added.
    However, more training data is always the easiest cure to statistical problems.
    If we can find much larger quantities of training data we could allow for much richer rule paradigms that relate compressed to original sentences.
    One example of a rule we would like to automatically discover would allow us to compress all of our design goals or In the limit such rules blur the distinction between compression and paraphrase.
  
  
    This work was supported by NSF grant IIS0112435.
    We would like to thank Kevin Knight and Daniel Marcu for their clarification and test sentences, and Mark Johnson for his comments.
  

