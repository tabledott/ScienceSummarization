ext, but they tend to get stuck in local maxima (Charniak, 1993).
    Even when they avoid local maxima (e.g., through clever initialization) they typically deviate from human ideas of what the &#8220;right&#8221; structure is (Merialdo, 1994).
    One strategy is to incorporate domain knowledge into the model&#8217;s structure.
    Instead of blind HMMs or PCFGs, one could use models whose features are crafted to pay attention to a range of domainspecific linguistic cues.
    Log-linear models can be so crafted and have already achieved excellent performance when trained on annotated data, where they are known as &#8220;maximum entropy&#8221; models (Ratnaparkhi et al., 1994; Rosenfeld, 1994).
    Our goal is to learn log-linear models from unannotated data.
    Since the forward-backward and inside-outside algorithms are instances of Expectation-Maximization (EM) (Dempster et al., 1977), a natural approach is to construct EM algorithms that handle log-linear models.
    Riezler (1999) did so, then resorted 