e parametric, i.e., they learn a numerical weight (e.g., a probability) associated with each feature, where the set of features is fixed before learning begins.
    Such procedures can be used to learn features or structural units by embedding them in a &#8220;propose-and-prune&#8221; algorithm: a feature proposal component proposes potentially useful features (e.g., combinations of the currently most useful features), which are then fed to a parametric learner that estimates their weights.
    After estimating feature weights and pruning &#8220;useless&#8221; low-weight features, the cycle repeats.
    While such algorithms can achieve impressive results (Stolcke and Omohundro, 1994), their effectiveness depends on how well the feature proposal step relates to the overall learning objective, and it can take considerable insight and experimentation to devise good feature proposals.
    One of the main reasons for the recent interest in nonparametric Bayesian inference is that it offers a systematic framework 