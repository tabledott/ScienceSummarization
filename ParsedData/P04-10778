ing to the results reported in that paper, ROUGE-L, ROUGE-W, and ROUGE-S also outperformed NIST.
  
  
    Using objective functions to automatically evaluate machine translation quality is not new.
    Su et al. (1992) proposed a method based on measuring edit distance (Levenshtein 1966) between candidate and reference translations.
    Akiba et al. (2001) extended the idea to accommodate multiple references.
    Nie&#223;en et al. (2000) calculated the lengthnormalized edit distance, called word error rate (WER), between a candidate and multiple reference translations.
    Leusch et al. (2003) proposed a related measure called position-independent word error rate (PER) that did not consider word position, i.e. using bag-of-words instead.
    Instead of error measures, we can also use accuracy measures that compute similarity between candidate and reference translations in proportion to the number of common words between them as suggested by Melamed (1995).
    An n-gram co-occurrence measure, BLEU, proposed