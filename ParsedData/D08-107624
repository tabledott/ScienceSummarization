 surface.
    A large variety of different search strategies for MERT are investigated in (Cer et al., 2008), which provides many fruitful insights into the optimization process.
    In (Duh and Kirchhoff, 2008), MERT is used to boost the BLEU score on N-best re-ranking tasks.
    The incorporation of a large number of sparse feature functions is described in (Watanabe et al., 2007).
    The paper investigates a perceptron-like online large-margin training for statistical machine translation.
    The described approach is reported to yield significant improvements on top of a baseline system which employs a small number of feature functions whose weights are optimized under the MERT criterion.
    A study which is complementary to the upper bound on the size of envelopes derived in Section 4 is provided in (Elizalde and Woods, 2006) which shows that the number of inference functions of any graphical model as, for instance, Bayesian networks and Markov random fields is polynomial in the size of the model if th