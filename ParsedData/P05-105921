for pairs of sentences that are less than 16 words, the ITG alignment space has a good coverage over all possibilities.
    Hence, it&#8217;s reasonable to see a better chance of improving the alignment result for sentences less than 16 words.
  
  
    We presented the formal description of a Stochastic Lexicalized Inversion Transduction Grammar with its EM training procedure, and proposed specially designed pruning and smoothing techniques.
    The experiments on a parallel corpus of Chinese and English showed that lexicalization helped for aligning sentences of up to 15 words on both sides.
    The pruning and the limitations of the bracketing grammar may be the reasons that the result on sentences of up to 25 words on both sides is not better than that of the unlexicalized ITG.
    Acknowledgments We are very grateful to Rebecca Hwa for assistance with the Chinese-English data, to Kevin Knight and Daniel Marcu for their feedback, and to the authors of GIZA.
    This work was partially supported by NSF ITR