four components of the model.
  We have assigned bit strings to the syntactic and seman- tic categories and to the rules manually.
  Our retention is that bit strings differing in the least significant bit posi- tions correspond to categories of non-terminals or rules that are similar.
  We also have assigned bitstrings for the words in the vocabulary (the lexical heads) using automatic lustering algorithms using the bigram mu- tual information clustering algorithm (see [4]).
  Given the bitsting of a history, we then designed a decision tree for modeling the probability that a rule will be used for rewriting a node in the parse tree.
  Since the grammar produces parses which may be more detailed than the Treebank, the decision tree was built using a training set constructed in the following man- ner.
  Using the grammar with the P-CFG model we de- termined the most likely parse that is consistent with the Treebank and considered the resulting sentence-tree pair as an event.
  Note that the grammar parse will