wn in Figure 3.
    In order to measure intra-annotator agreement 10% of the items were repeated and evaluated twice by each judge.
    In order to measure interannotator agreement 40% of the items were randomly drawn from a common pool that was shared across all annotators so that we would have items that were judged by multiple annotators.
  
  
    the two types of manual evaluation We measured pairwise agreement among annotators using the kappa coefficient (K) which is defined as where P(A) is the proportion of times that the annotators agree, and P(E) is the proportion of time that they would agree by chance.
    For inter-annotator agreement we calculated P(A) for the yes/no judgments by examining all items that were annotated by two or more annotators, and calculating the proportion of time they assigned identical scores to the same items.
    For the ranking tasks we calculated P(A) by examining all pairs of systems which had been judged by two or more judges, and calculated the proportion of time tha