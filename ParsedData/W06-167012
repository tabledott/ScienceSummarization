osed in recent years (McCallum et al., 2000; Lafferty et al., 2001; Collins, 2002; Altun et al., 2003).
    In this paper we apply perceptron trained HMMs originally proposed in (Collins, 2002).
    HMMs define a probabilistic model for observation/label sequences.
    The joint model of an observation/label sequence (x, y), is defined as: where yi is the ith label in the sequence and xi is the ith word.
    In the NLP literature, a common approach is to model the conditional distribution of label sequences given the label sequences.
    These models have several advantages over generative models, such as not requiring questionable independence assumptions, optimizing the conditional likelihood directly and employing richer feature representations.
    This task can be represented as learning a discriminant function F : X x Y &#8212;* IR, on a training data of observation/label sequences, where F is linear in a feature representation `b defined over the joint input/output space `b is a global feature represen