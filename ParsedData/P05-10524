h annotated entities.
    Using SVM as a classifier along with the full composite kernel produced the best performance on this task.
    This paper will also show a comparison of SVM and KNN (k-Nearest-Neighbors) under different kernel setups.
  
  
    Many machine learning algorithms involve only the dot product of vectors in a feature space, in which each vector represents an object in the object domain.
    Kernel methods (Muller et al., 2001) can be seen as a generalization of feature-based algorithms, in which the dot product is replaced by a kernel function (or kernel) &#936;(X,Y) between two vectors, or even between two objects.
    Mathematically, as long as &#936;(X,Y) is symmetric and the kernel matrix formed by &#936; is positive semi-definite, it forms a valid dot product in an implicit Hilbert space.
    In this implicit space, a kernel can be broken down into features, although the dimension of the feature space could be infinite.
    Normal feature-based learning can be implemented in kernel f