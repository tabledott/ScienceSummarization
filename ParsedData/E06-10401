ming an increasingly important topic in Natural Language Generation (NLG), as in other fields of computational linguistics.
    Some NLG researchers are impressed by the success of the BLEU evaluation metric (Papineni et al., 2002) in Machine Translation (MT), which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets.
    BLEU and related metrics work by comparing the output of an MT system to a set of reference (&#8216;gold standard&#8217;) translations, and in principle this kind of evaluation could be done with NLG systems as well.
    Indeed NLG researchers are already starting to use BLEU (Habash, 2004; Belz, 2005) in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate NLG systems.
    However, the use of such corpus-based evaluation metrics is only sensible if they are known to be correlated with the results of human-based evaluations.