rench-English systems, BBN combined the French-English and German-English systems, and Edinburgh submitted combinations for the French-English and GermanEnglish systems as well as a multi-source system combination which combined all systems which translated from any language pair into English for the News test set.
    The University of Saarland also produced a system combination over six commercial RBMT systems (Eisele et al., 2008).
    Saarland graciously provided the output of these systems, which we manually evaluated alongside all other entries.
    For more on the participating systems, please refer to the respective system descriptions in the proceedings of the workshop.
  
  
    As with last year&#8217;s workshop, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores.
    It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality.
    Therefore, rather than select an official automatic evaluation me