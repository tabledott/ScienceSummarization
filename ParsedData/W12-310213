how often single judges are consistent for repeated judgments, respectively.
    The exact interpretation of the kappa coefficient is difficult, but according to Landis and Koch (1977), 0 &#8722; 0.2 is slight, 0.2 &#8722; 0.4 is fair, 0.4 &#8722; 0.6 is moderate, 0.6 &#8722; 0.8 is substantial, and 0.8 &#8722; 1.0 is almost perfect.
    Based on these interpretations, the agreement for sentencelevel ranking is fair for inter-annotator and moderate for intra-annotator agreement.
    Consistent with previous years, intra-annotator agreement is higher than inter-annotator agreement, except for English&#8211; Czech.
    An important difference from last year is that the evaluations were not constrained only to workshop participants, but were made available to all Turkers.
    The workshop participants were trusted to complete the tasks in good faith, and we have multiple years of data establishing general levels of inter- and intra-annotator agreement.
    Their HITs were unpaid, and access was limited with the 