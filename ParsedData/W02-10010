
	Discriminative Training Methods For Hidden Markov Models: Theory And Experiments With Perceptron Algorithms
		We describe new algorithms for train ing tagging models, as an alternativeto maximum-entropy models or conditional random elds (CRFs).
		The al gorithms rely on Viterbi decoding oftraining examples, combined with simple additive updates.
		We describe the ory justifying the algorithms througha modi cation of the proof of conver gence of the perceptron algorithm forclassi cation problems.
		We give experimental results on part-of-speech tag ging and base noun phrase chunking, in both cases showing improvements over results for a maximum-entropy tagger.
	
	
			Maximum-entropy (ME) models are justi ably a very popular choice for tagging problems in Natural Language Processing: for example see (Ratnaparkhi 96) for their use on part-of-speech tagging, and (McCallum et al 2000) for their use on a FAQ segmentation task.
			ME models have the advantage of being quite exible in the features that can be incor