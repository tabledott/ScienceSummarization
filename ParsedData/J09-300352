ormance.
    In these experiments, six different feature sets for each algorithm show improvements in accuracy over the baseline, yet none of those improvements are significant.
    This suggests that achieving the highest performance for neutral&#8211;polar classification requires a wide variety of features working together in combination.
    We further test this result by evaluating the effect of removing the features that produced either no change or a drop in accuracy from the respective all-feature classifiers.
    For example, we train a TiMBL neutral&#8211;polar classifier using all the features except for those in the PRECEDED-POS, INTENSIFY, STRUCTURE, CURSENT-COUNTS, and TOPIC feature sets, and then compare the performance of this new classifier to the TiMBL, allfeature classifier.
    Although removing the non-performing features has little effect for BoosTexter, performance does drop for both TiMBL and Ripper.
    The primary source of this performance drop is a decrease in polar recall: 2% for T