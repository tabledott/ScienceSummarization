, one takes a large sample of the language, and calculates the negative log probability of the sample, normalized by its size.15 The lower the cross entropy (i.e., the higher the probability the model assigns to the sample), the better the model.
    Usually this is reported in terms of perplexity, which we will do as well.'
    Some of the trials discussed below will report results in terms of word and/or sentence error rate, which are obtained when the language model is embedded in a speech recognition system.
    Word error rate is the number of deletion, insertion, or substitution errors per 100 words.
    Sentence error rate is the number of sentences with one or more errors per 100 sentences.
    Statistical parsers are typically evaluated for accuracy at the constituent level, rather than simply whether or not the parse that the parser found is completely correct or not.
    A constituent for evaluation purposes consists of a label (e.g., NP) and a span (beginning and ending word positions).
    For ex