, TAG (Sarkar and Joshi, 2003), CCG (Hockenmaier and Steedman, 2002; Clark and Curran, 2004b), and variants of phrase-structure grammar (Briscoe et al., 2006), including the phrase-structure grammar implicit in the Penn Treebank (Collins, 2003; Charniak, 2000).
    Different parsers produce different output, for example phrase structure trees (Collins, 2003), dependency trees (Nivre and Scholz, 2004), grammatical relations (Briscoe et al., 2006), and formalismspecific dependencies (Clark and Curran, 2004b).
    This variety of formalisms and output creates a challenge for parser evaluation.
    The majority of parser evaluations have used test sets drawn from the same resource used to develop the parser.
    This allows the many parsers based on the Penn Treebank, for example, to be meaningfully compared.
    However, there are two drawbacks to this approach.
    First, parser evaluations using different resources cannot be compared; for example, the Parseval scores obtained by Penn Treebank parsers cannot be