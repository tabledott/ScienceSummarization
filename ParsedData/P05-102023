 to the three ACE test data sets.
    Table 5 shows the bestperforming resolver for each test set and scoring program combination.
    Interestingly, with respect to the MUC scorer, the best performance on the three data sets is achieved by the same resolver.
    The results with respect to B-CUBED are mixed, however.
    For each resolver shown in Table 5, we also compute the average rank of the partitions generated by the resolver for the corresponding test texts.6 Intuitively, a resolver that consistently produces good partitions (relative to other candidate partitions) would achieve a low average rank.
    Hence, we can infer from the fairly high rank associated with the top B-CUBED resolvers that they do not perform consistently better than their counterparts.
    Regarding our second question of why the same set of candidate partitions is scored differently by the two scoring programs, the reason can be attributed to two key algorithmic differences between these scorers.
    First, while the MUC scorer 