ons and content coverage as in the Document Understanding Conference (DUC) (Over and Yen, 2003) would require over 3,000 hours of human efforts.
    This is very expensive and difficult to conduct in a frequent basis.
    Therefore, how to evaluate summaries automatically has drawn a lot of attention in the summarization research community in recent years.
    For example, Saggion et al. (2002) proposed three content-based evaluation methods that measure similarity between summaries.
    These methods are: cosine similarity, unit overlap (i.e. unigram or bigram), and longest common subsequence.
    However, they did not show how the results of these automatic evaluation methods correlate to human judgments.
    Following the successful application of automatic evaluation methods, such as BLEU (Papineni et al., 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries.
    In this paper, we int