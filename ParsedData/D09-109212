fferent posterior distributions over topics for different documents in a given tuple), we can nevertheless divide topic assignment variables between languages and use them to estimate a Dirichlet-multinomial posterior distribution for each language in each tuple.
    For each tuple we can then calculate the JensenShannon divergence (the average of the KL divergences between each distribution and a mean distribution) between these distributions.
    Figure 4 shows the density of these divergences for different numbers of topics.
    As with the previous figure, there are a small number of documents that contain only one topic in all languages, and thus have zero divergence.
    These tend to be very short, formulaic parliamentary responses, however.
    The vast majority of divergences are relatively low (1.0 indicates no overlap in topics between languages in a given document tuple) indicating that, for each tuple, the model is not simply assigning all tokens in a particular language to a single topic.
    As