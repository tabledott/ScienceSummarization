rmally left to right, but occasionally right to left, e.g., Church (1988)).
    There are a few exceptions, such as Brill&#8217;s transformation-based learning (Brill, 1995), but most of the best known and most successful approaches of recent years have been unidirectional.
    Most sequence models can be seen as chaining together the scores or decisions from successive local models to form a global model for an entire sequence.
    Clearly the identity of a tag is correlated with both past and future tags&#8217; identities.
    However, in the unidirectional (causal) case, only one direction of influence is explicitly considered at each local point.
    For example, in a left-to-right first-order HMM, the current tag t0 is predicted based on the previous tag t_1 (and the current word).1 The backward interaction between t0 and the next tag t+1 shows up implicitly later, when t+1 is generated in turn.
    While unidirectional models are therefore able to capture both directions of influence, there are good rea