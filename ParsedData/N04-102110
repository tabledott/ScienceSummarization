eferences for selection and scoring.
    Here, using an 1000-best list, we obtain oracle translations with a relative human BLEU score of 88.5%.
    Based on the results of the oracle experiment, and in order to make rescoring computationally feasible for features requiring significant computation for each hypothesis, we used the top 1000 translation candidates for our experiments.
    The baseline system&#8217;s BLEU score is 31.6% on the test set (equivalent to the 1-best oracle in Table 1).
    This is the benchmark against which the contributions of the additional features described in the remainder of this paper are to be judged.
    As a precursor to developing the various syntactic features described in this report, the syntactic representations on which they are based needed to be computed.
    This involved part-of-speech tagging, chunking, and parsing both the Chinese and English side of our training, development, and test sets.
    Applying the part-of-speech tagger to the often ungrammatical MT ou