ce.
    Table 3 shows the result of our cross-linguistic evaluation on this data.
    Since the data sets are so small we decided to use the conditional entropy evaluation.
    Here DO refers to the distributional clustering algorithm where all words are clustered; D5 leaves all words with frequency at most 5 in a seperate cluster, DM uses morphological information as well, DF uses frequency information and DMF uses morphological and frequency information.
    We evaluated it for all words, and also for words with frequency at most 5.
    We can see that the use of morphological information consistently improves the results on the rare words by a substantial margin.
    In some cases, however, a simpler algorithm performs better when all the words are considered &#8212; notably in Slovene and Estonian.
    We have also evaluated this method by comparing the perplexity of a class-based language model derived from these classes.
    We constructed a class bigram model, using absolute interpolation with a single