introducing additional alignment steps at the clause and/or phrase levels, but testing this hypothesis would require access to robust parsing technology.
    The original version of the program did not consider the category of 2-2 alignments.
    Table 6 shows that the program was right on 10 of 15 actual 2-2 alignments.
    This was achieved at the cost of introducing 2 spurious 2-2 alignments.
    Thus in 12 tries, the program was right 10 times, wrong 2 times.
    This is significantly better than chance, since there is less than 1% chance of getting 10 or more heads out of 12 flips of a fair coin.
    Thus it is worthwhile to include the 2-2 alignment possibility.
    When we discussed the estimation of the model parameters, c and s2, we mentioned that it is possible to fit the parameters more accurately if we estimate different values for each language pair, but that doing so did not seem to increase performance by very much.
    In fact, we found exactly the same total number of errors, although the err