itrary features of the input, including the correspondence features described earlier.
    In particular, MIRA aims to learn weights so that the score of correct output, yt, for input xt is separated from the highest scoring incorrect outputs2, with a margin proportional to their Hamming losses.
    MIRA has been used successfully for both sequence analysis (McDonald et al., 2005a) and dependency parsing (McDonald et al., 2005b).
    As with any structured predictor, we need to factor the output space to make inference tractable.
    We use a first-order Markov factorization, allowing for an efficient Viterbi inference procedure.
  
  
    In section 2 we claimed that good representations should encode correspondences between words like &#8220;signal&#8221; from MEDLINE and &#8220;investment&#8221; from the WSJ.
    Recall that the rows of 0 are projections from the original feature space onto the real line.
    Here we examine word features under these projections.
    Figure 4 shows a row from the matrix 0.