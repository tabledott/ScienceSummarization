 in the training text, and hence to Pr (w2), we must have, in the limit, where H(w) is the entropy of the 1-gram word distribution and /(ci , c2) is the average mutual information of adjacent classes.
    Because L(7r) depends on 7r only through this average mutual information, the partition that maximizes L(7r) is, in the limit, also the partition that maximizes the average mutual information of adjacent classes.
    We know of no practical method for finding one of the partitions that maximize the average mutual information.
    Indeed, given such a partition, we know of no practical method for demonstrating that it does, in fact, maximize the average mutual information.
    We have, however, obtained interesting results using a greedy algorithm.
    Initially, we assign each word to a distinct class and compute the average mutual information between adjacent classes.
    We then merge that pair of classes for which the loss in average mutual information is least.
    After V &#8212; C of these merges, C cl