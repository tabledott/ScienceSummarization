rds).
			The resulting model produces the alignment in Figure 1(d).
			It has sorted out the the-le / the-les confusion, and is also able to guess to-de, which is not the most common translation for either word, but which is supported by the good Dice value on the following pair (make-faire).
			With all these features, we got a final AER of 10.7, broadly similar to the 8.9 or 9.7 AERs of unsymmetrized IBM Model 4 trained on the same data that the Dice counts were takenfrom.6 Of course, symmetrizing Model 4 by in tersecting alignments from both directions does yield an improved AER of 6.9, so, while ourmodel does do surprisingly well with cheaply ob tained count-based features, Model 4 does still outperform it so far.
			However, our model can4It is important to note that while our matching algo rithm has no first-order effects, the features can encode such effects in this way, or in better ways ? e.g. using as features posteriors from the HMM model in the style of Matusov et al (2004).
			5The number of such