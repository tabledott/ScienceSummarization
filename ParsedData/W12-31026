systems&#8217; models, for instance.
    We therefore categorize all commercial systems as unconstrained when evaluating the results.
  
  
    As with past workshops, we placed greater emphasis on the human evaluation than on the automatic evaluation metric scores.
    It is our contention that automatic measures are an imperfect substitute for human assessment of translation quality.
    Therefore, we define the manual evaluation to be primary, and distinct words (case-insensitive) is based on the provided tokenizer. use the human judgments to validate automatic metrics.
    Manual evaluation is time consuming, and it requires a large effort to conduct on the scale of our workshop.
    We distributed the workload across a number of people, beginning with shared-task participants and interested volunteers.
    This year, we also opened up the evaluation to non-expert annotators hired on Amazon Mechanical Turk (CallisonBurch, 2009).
    To ensure that the Turkers provided high quality annotations, we used con