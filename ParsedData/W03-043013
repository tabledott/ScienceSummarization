ewton&#8217;s method finds them all in about 12 quick iterations.
    There are two additional important modeling choices: (1) Because we expect our models to still require several thousands of features, we save time by adding many of the features with highest gain each round of induction rather than just one; (including a few redundant features is not harmful).
    (2) Because even models with a small select number of features can still severely overfit, we train the model with just a few BFGS iterations (not to convergence) before performing the next round of feature induction.
    Details are in (McCallum, 2003).
  
  
    Some general-purpose lexicons, such a surnames and location names, are widely available, however, many natural language tasks will benefit from more task-specific lexicons, such as lists of soccer teams, political parties, NGOs and English counties.
    Creating new lexicons entirely by hand is tedious and time consuming.
    Using a technique we call WebListing, we build lexicons automa