 hence a mostly likely sequence of lexical tags, corresponding to sequence of ambiguity classes.
    In the following, N can identified with the number of possible .tags, and W wil the set of all ambiguity classes.
    Applying an HMM consists of two tasks: estimating ti model parameters A, B and H from a training set; ar computing the most likely sequence of underlying sta transitions given new observations.
    Maximum likeliho( estimates (that is, estimates that maximize the probabili of the training set) can be found through application of z ternating expectation in a procedure known as the Baur Welch, or forward-backward, algorithm [Baum, 1972]. proceeds by recursively defining two sets of probabiliti( the forward probabilities where ot1(i) = ribi(51) for all i; and the backward prob bilities, where OT(j) = 1 for all j.
    The forward probabili oet(i) is the joint probability of the sequence up to tir t, {S1, S2, , St}, and the event that the Markov pr cess is in state i at time t. Similarly, the backwa