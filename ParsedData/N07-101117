  While in theory a metaclassifier can flexibly represent features, they do not explore features using the full flexibility of firstorder logic.
    Also, their method is neither errordriven nor rank-based.
    McCallum and Wellner (2003) use a conditional random field that factors into a product of pairwise decisions about pairs of nouns.
    These pairwise decisions are made collectively using relational inference; however, as pointed out in Milch et al. (2004), this model has limited representational power since it does not capture features of entities, only of pairs of mention.
    Milch et al. (2005) address these issues by constructing a generative probabilistic model, where noun clusters are sampled from a generative process.
    Our current work has similar representational flexibility as Milch et al. (2005) but is discriminatively trained.
  
  
    We have presented learning and inference procedures for coreference models using first-order features.
    By relying on sampling methods at training tim