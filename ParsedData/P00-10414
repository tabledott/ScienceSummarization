riment with.
    As we will discuss in this paper, we built several models of varying complexity;1 even the simplest one did reasonably well at summarization, whereas it would have been severely deficient at (traditional) translation.
  
  
    As in any language generation task, summarization can be conceptually modeled as consisting of two major sub-tasks: (1) content selection, and (2) surface realization.
    Parameters for statistical models of both of these tasks were estimated from a training corpus of approximately 25,000 1997 Reuters news-wire articles on politics, technology, health, sports and business.
    The target documents &#8211; the summaries &#8211; that the system needed to learn the translation mapping to, were the headlines accompanying the news stories.
    The documents were preprocessed before training: formatting and mark-up information, such as font changes and SGML/HTML tags, was removed; punctuation, except apostrophes, was also removed.
    Apart from these two steps, no other no