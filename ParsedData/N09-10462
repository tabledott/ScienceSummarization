perimentation (Koehn and Knight, 2003; Habash and Sadat, 2006; Chang et al., 2008).
    Recent work has shown that by combining a variety of segmentations of the input into a segmentation lattice and effectively marginalizing over many different segmentations, translations superior to those resulting from any single single segmentation of the input can be obtained (Xu et al., 2005; Dyer et al., 2008; DeNeefe et al., 2008).
    Unfortunately, this approach is difficult to utilize because it requires multiple segmenters that behave differently on the same input.
    In this paper, we describe a maximum entropy word segmentation model that is trained to assign high probability to possibly several segmentations of an input word.
    This model enables generation of diverse, accurate segmentation lattices from a single model that are appropriate for use in decoders that accept word lattices as input, such as Moses (Koehn et al., 2007).
    Since our model relies a small number of dense features, its parameters can