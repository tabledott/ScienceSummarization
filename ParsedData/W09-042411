 using software that IBM Model 1 probabilities into account (Moore, 2002).
    We filtered and de-duplcated the resulting parallel corpus.
    After discarding 630 thousand sentence pairs which had more than 100 words, our final corpus had 21.9 million sentence pairs with 587,867,024 English words and 714,137,609 French words.
    We distributed the corpus to the other WMT09 participants to use in addition to the Europarl v4 French-English parallel corpus (Koehn, 2005), which consists of approximately 1.4 million sentence pairs with 39 million English words and 44 million French words.
    Our translation model was trained on these corpora using the subsampling descried in Section 2.1.
    For language model training, we used the monolingual news and blog data that was assembled by the University of Edinburgh and distributed as part of WMT09.
    This data consisted of 21.2 million English sentences with half a billion words.
    We used SRILM to train a 5-gram language model using a vocabulary containing the