erved, the model &amp;quot;backs off' to a less-powerful model, as described below, in &#167;3.3.3 on p. 4.
    Throughout most of the model, we consider words to be ordered pairs (or two-element vectors), composed of word and word-feature, denoted (w, f).
    The word feature is a simple, deterministic computation performed on each word as it is added to or feature computation is an extremely small part of the implementation, at roughly ten lines of code.
    Also, most of the word features are used to distinguish types of numbers, which are language-independent.2 The rationale for having such features is clear: in Roman languages, capitalization gives good evidence of names.3 This section describes the model formally, discussing the transition probabilities to the wordstates, which &amp;quot;generate&amp;quot; the words of each name-class.
    As with most trained, probabilistic models, we looked up in the vocabulary.
    It produces one of the fourteen values in Table 3.1.
    These values are computed in 