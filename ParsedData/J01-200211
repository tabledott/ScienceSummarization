 a classifier is trained to predict the correct output class when given as input the outputs of the ensemble classifiers, and possibly additional information (Wolpert 1992; Breiman 1996b; Ting and Witten 1997a, 1997b).
    Stacking can lead to an arbiter effect.
    In this paper we compare voting and stacking approaches on the tagging problem.
    In the remainder of this section, we describe the combination methods we use in our experiments.
    We start with variations based on weighted voting.
    Then we go on to several types of stacked classifiers, which model the disagreement situations observed in the training data in more detail.
    The input to the second-stage classifier can be limited to the first-level outputs or can contain additional information from the original input pattern.
    We will consider a number of different second-level learners.
    Apart from using three well-known machine learning methods, memory-based learning, maximum entropy, and decision trees, we also introduce a new meth