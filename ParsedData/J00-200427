of relative offset parameters.
    The much smaller number of parameters allowed Dagan, Church, and Gale's model to be effectively trained on much smaller bitexts.
    Vogel, Ney, and Tillmann (1996) have shown how some additional assumptions can turn this model into a hidden Markov model, enabling even more efficient parameter estimation.
    It cannot be overemphasized that the word order correlation bias is just knowledge about the problem domain, which can be used to guide the search for the optimum model parameters.
    Translational equivalence can be empirically modeled for any pair of languages, but some models and model biases work better for some language pairs than for others.
    The word order correlation bias is most useful when it has high predictive power, i.e., when the distribution of alignments or offsets has low entropy.
    The entropy of this distribution is indeed relatively low for the language pair that both Brown and his colleagues and Dagan, Church, and Gale were working with&#8212;