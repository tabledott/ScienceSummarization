 posit appropriate syntactic relationships in the latent variable t, because&#8212;thanks to the structure of the QG model&#8212;that is the easiest way for it to exploit the extra information in t', w' to help predict w.4 At test time, t', w' are not made available, so we just use the trained model to find argmaxt p(t  |w), backing off from the conditioning on t', w' and summing over a.
    Below, we present the specific generative model (&#167;5.1) and some details of training (&#167;5.2).
    We will then compare three approaches (&#167;5.3): &#167;5.3.2 a straight EM baseline (which does not condition on t', w' at all) &#167;5.3.3 a &#8220;hard&#8221; projection baseline (which naively projects t', w' to derive direct supervision in the target language) &#167;5.3.4 our conditional EM approach above (which makes t', w' available to the learner for &#8220;soft&#8221; indirect supervision via QG) Our base models of target-language syntax are generative dependency models that have achieved state-of-the art re