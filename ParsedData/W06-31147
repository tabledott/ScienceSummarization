 0.5 0.4 &#8226;systran &#8226;upc-mr &#8226; &#8226;rali 0.3 &#8226;ntt 0.2 0.1 -0.0 -0.1 &#8226;systran &#8226;upc-mr -0.9 9 10 11 12 13 14 15 16 17 18 19 6 7 8 9 10 11 Fluency 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 &#8226;upv -0.5 &#8226;upv &#8226;systran &#8226;upc-mr &#8226; Fluency 0.4 0.3 0.2 0.1 -0.0 -0.1 -0.2 -0.3 -0.4 -0.5 -0.6 &#8226;systran &#8226;ntt
  
  
    was done by the participants.
    This revealed interesting clues about the properties of automatic and manual scoring.
    &#8226; We evaluated translation from English, in addition to into English.
    English was again paired with German, French, and Spanish.
    We dropped, however, one of the languages, Finnish, partly to keep the number of tracks manageable, partly because we assumed that it would be hard to find enough Finnish speakers for the manual evaluation.
  
  
    The evaluation framework for the shared task is similar to the one used in last year&#8217;s shared task.
    Training and testing is based on the Europarl corpus.
    F