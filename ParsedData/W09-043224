that a very tiny development set of only 200 parallel sentences is adequate enough to get comparable performance as a 2000-sentence set.
    Finally, we described how to reduce the time for training models from a synthetic corpus generated through Moses by 50% at least, by exploiting word-alignment information provided during decoding.
  

