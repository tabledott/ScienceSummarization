is case, the annotators had made different decisions on what part of speech this cover symbol represented.
  When this text is excluded from consideration, mean inter-annotator disagreement for the correcting task drops to 3.5%, with the median unchanged at 3.6%.
  Consistency, while desirable, tells us nothing about he validity of the annotators corrections.
  We therefore compared each annotators output not only with the output of each of the others, but also with a benchmark version of the eight texts.
  This benchmark version was derived from the tagged Brown Corpus by (1) mapping the original Brown Corpus tags onto the Penn Treebank tagset and (2) carefully hand- correcting the revised version in accordance with the tagging conventions in force at the time of the experiment.
  Accuracy was then computed as the rate of disagreement between each annotators results and the benchmark version.
  The mean accuracy was 5.4% for the tagging task (median 5.7%) and 4.0% for the correcting task (median 3.4%).
  Exc