o represent the training data.
    This table shows that there is little difference in the decision stump nodes selected from feature sets determined by the power divergence statistics versus the Dice Coefficient.
    This is to be expected since the top ranked bigrams for each measure are consistent, and the decision stump node is generally chosen from among those.
    However, there are differences between the feature sets selected by the power divergence statistics and the Dice Coefficient.
    These are reflected in the different sized trees that are learned based on these feature sets.
    The number of leaf nodes and the total number of nodes for each learned tree is shown in columns 3 and 6.
    The number of internal nodes is simply the difference between the total nodes and the leaf nodes.
    Each leaf node represents the end of a path through the decision tree that makes a sense distinction.
    Since a bigram feature can only appear once in the decision tree, the number of internal nodes represent