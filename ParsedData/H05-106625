arsing results for English us ing spanning tree algorithms.
			This shows that for projective data sets, training and testing with the Chu-Liu-Edmonds algorithm is worse than using the Eisner algorithm.
			This is notsurprising since the Eisner algorithm uses the a pri ori knowledge that all trees are projective.
	
	
			We presented a general framework for parsing dependency trees based on an equivalence to maximum spanning trees in directed graphs.
			This frame work provides natural and efficient mechanismsfor parsing both projective and non-projective languages through the use of the Eisner and Chu-Liu Edmonds algorithms.
			To learn these structures we used online large-margin learning (McDonald et al,2005) that empirically provides state-of-the-art per formance for Czech.A major advantage of our models is the ability to naturally model non-projective parses.
			Non projective parsing is commonly considered more difficult than projective parsing.
			However, under our framework, we show that the opposite 