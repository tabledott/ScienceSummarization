ient should be chosen based on the task (not on the observed differences between coder marginals).
    When the coefficient is used to assess reliability, a single-distribution coefficient like &#960; or &#945; should be used; this is indeed already the practice in CL, because Siegel and Castellan&#8217;s K is identical with (multi-)&#960;.
    It is also good practice to test Artstein and Poesio Inter-Coder Agreement for CL reliability with more than two coders, in order to reduce the likelihood of coders sharing a deviant reading of the annotation guidelines.
    We touched upon the matter of skewed data in Section 2.3 when we motivated the need for chance correction: If a disproportionate amount of the data falls under one category, then the expected agreement is very high, so in order to demonstrate high reliability an even higher observed agreement is needed.
    This leads to the so-called paradox that chance-corrected agreement may be low even though Ao is high (Cicchetti and Feinstein 1990; Feinstein 