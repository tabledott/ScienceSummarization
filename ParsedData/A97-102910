dless of capitalization, especially since it is almost never seen as &amp;quot;mr.&amp;quot;.
    The calculation of the above probabilities is straightforward, using events/sample-size: where c() represents the number of times the events occurred in the training data (the count).
    Ideally, we would have sufficient training (or at least one observation of!) every event whose conditional probability we wish to calculate.
    Also, ideally, we would have sufficient samples of that upon which each conditional probability is conditioned, e.g., for Pr(NC I NC 1, w_,), we would like to have seen sufficient numbers of NC_,, w1.
    Unfortunately, there is rarely enough training data to compute accurate probabilities when &amp;quot;decoding&amp;quot; on new data.
    3.
    3.
    3.1 Unknown Words The vocabulary of the system is built as it trains.
    Necessarily, then, the system knows about all words for which it stores bigram counts in order to compute the probabilities in Equations 3.1 &#8211; 3.3.
    The q