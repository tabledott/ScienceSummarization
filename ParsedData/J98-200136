 class, i.e., to understand where annotators agreed the most Computing the K coefficient of agreement. and where they disagreed the most.
    The confusion matrix does this to some extent, but only works for two annotators&#8212;and therefore, for example, we couldn't use it to measure agreement on classes between the two annotators and ourselves.
    We computed what we called per-class percentage of agreement for three coders (the two annotators and ourselves) by taking the proportion of pairwise agreements relative to the number of pairwise comparisons, as follows: whenever all three coders ascribe a description to the same class, we count six pairwise agreements out of six pairwise comparisons for that class (100%).
    If two coders ascribe a description to class 1 and the other coder to class 2, we count two agreements in four comparisons for class 1 (50%) and no agreement for class 2 (0%).
    The rates of agreement for each class thus obtained are presented in Table 7.
    The figures indicate better 