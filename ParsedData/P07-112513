ata, at least under limited dependence assumptions.
    The class priors can be estimated based on the relative distribution sizes derived from the current training sets: where |S |is the number of samples in training set S. If we assume feature independence, which as we will see for our task is not as gross an approximation as it may at first seem, we can simplify the classconditional likelihood in the well known manner: and then (estimate the likelihood for each feature: where f(x, S) is the number of samples in training set S in which feature x is present, and a is a universal smoothing constant, scaled by the class prior.
    This scaling is motivated by the principle that without knowledge of the true distribution of a particular feature it makes sense to include knowledge of the class distribution in the smoothing mechanism.
    Smoothing is particularly important in the early stages of the learning process when the amount of training data is severely limited resulting in unreliable frequency estimates.