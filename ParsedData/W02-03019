linear separating surface in the original feature space.
    Fortunately, such non-linear training does not increase the computational cost if the calculation of the kernel function is as cheap as the inner product.
    A polynomial function defined as (sxi &#183; xj + r)d is popular in applications of SVMs to NLPs (Kudo and Matsumoto, 2000; Yamada et al., 2000; Kudo and Matsumoto, 2001), because it has an intuitively sound interpretation that each dimension of the mapped space is a 3For many real-world problems where the samples may be inseparable, we allow the constraints are broken with some penalty.
    In the experiments, we use so-called 1-norm soft margin formulation described as: subject to yi(w &#183; xi + b) &#8805; 1 &#8722; ei, i = 1, &#183; &#183; &#183; , L, ei &#8805; 0, i = 1,&#183;&#183;&#183; , L. (weighted) conjunction of d features in the original sample.
    As described above, the standard SVM learning constructs a binary classifier.
    To make a named entity recognition system based on