
  Learning Semantic Correspondences with Less Supervision
  
    A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state.
    To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.
    We show that our model generalizes across three domains of increasing difficulty&#8212;Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.
  
  
    Recent work in learning semantics has focused on mapping sentences to meaning representations (e.g., some logical form) given aligned sentence/meaning pairs as training data (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Lu et al., 2008).
    However, this degree of supervision is unrealistic for modeling human language acquisition and can be 