the proportion of time they assigned identical scores to the same items.
    For the ranking tasks we calculated P(A) by examining all pairs of systems which had been judged by two or more judges, and calculated the proportion of time that they agreed that A &gt; B, A = B, or A &lt; B.
    For intra-annotator agreement we did similarly, but gathered items that were annotated on multiple occasions by a single annotator.
    Table 12 gives K values for inter-annotator agreement, and Table 13 gives K values for intraannotator agreement.
    These give an indication of how often different judges agree, and how often single judges are consistent for repeated judgments, re
  
  
    spectively.
    The interpretation of Kappa varies, but according to Landis and Koch (1977), 0&#8722;.2 is slight, .2 &#8722;.4 is fair, .4 &#8722;.6 is moderate, .6 &#8722;.8 is substantial and the rest almost perfect.
    The inter-annotator agreement for the sentence ranking task was fair, for the constituent ranking it was moderate 