 the context of question answering) in that it adds an external constraint, not present at the time of learning the classifiers and it evaluates the ability of our inference algorithm to cope with it.
    The results exhibit that our expectations are correct.
    In fact, we believe that in natural situations the number of constraints that can apply is even larger.
    Observing the algorithm performs on other, specific, forced decision tasks verifies that LP is reliable in these situations.
    As shown in the experiment, it even performs better than omniscience, which is given more information at learning time, but cannot adapt to the situation at decision time.
  
  
    We presented an linear programming based approach for global inference where decisions depend on the outcomes of several different but mutually dependent classifiers.
    Even in the presence of a fairly general constraint structure, deviating from the sequential nature typically studied, this approach can find the optimal solution efficie