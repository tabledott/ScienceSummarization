 version of the BNC.
    The noun basis was the 2000 most common context words, basis weights were the probability of context words given the target word divided by the overall probability of the context word.
    Intransitive verb functionvectors were trained using the procedure presented in &#167;4.
    Since the dataset only contains intransitive verbs and nouns, we used S = N. The cosine measure of vectors was used as a similarity metric.
    First Experiment Results In Table 3 we present the comparison of the selected models.
    Our categorical model performs significantly better than the existing second-place (Kintsch) and obtains a &#961; quasiidentical to the multiplicative model, indicating significant correlation with the annotator scores.
    There is not a large difference between the mean High score and mean Low score, but the distribution in Figure 6 shows that our model makes a non-negligible distinction between high similarity phrases and low similarity phrases, despite the absolute scores no