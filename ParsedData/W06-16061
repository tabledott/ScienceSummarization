utputs than phrase-based systems when evaluated on relatively small scale, domain specific corpora.
    And syntax-inspired formal models (Chiang, 2005), in spite of being trained on significantly less data, have shown promising results when compared on the same test sets with mature phrase-based systems.
    To our knowledge though, no previous research has demonstrated that a syntax-based statistical translation system could produce better results than a phrase-based system on a large-scale, well-established, open domain translation task.
    In this paper we present such a system.
    Our translation models rely upon and naturally exploit submodels (feature functions) that have been initially developed in phrase-based systems for choosing target translations of source language phrases, and use new, syntax-based translation and target language submodels for assembling target phrases into well-formed, grammatical outputs.
    After we introduce our models intuitively, we discuss their formal underpinning and