tortion model, in the style of IBM Model 3.
    We have tried many types of distortion models.
    We eventually settled for the model discussed here because it produces better translations during decoding.
    Since the number of factors involved in computing the probability of an alignment does not vary with the size of the Target phrases into which Source phrases are translated, this model is not predisposed to produce translations that are shorter than the Source sentences given as input.
    3 Training Training the models described in Section 2 is computationally challenging.
    Since there is an exponential number of alignments that can generate a sentence pair (E, F), it is clear that we cannot apply the 1.
    Determine high-frequency ngrams in the bilingual corpus.
    2.
    Initialize the t-distribution table.
    3.
    Apply EM training on the Viterbi alignments, while using smoothing.
    4.
    Generate conditional model probabilities.
    Figure 2: Training algorithm for the phrase-based join