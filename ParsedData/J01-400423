 per leaf node is set at 5.
    For MUC-7, the pruning confidence is 60% and the minimum number of instances is 2.
    The parameters are determined by performing 10-fold cross-validation on the whole training set for each MUC year.
    The possible pruning confidence values that we tried are 10%, 20%, 40%, 60%, 80%, and 100%, and for minimum instances, we tried 2, 5, 10, 15, and 20.
    Thus, a total of 30 (6 x 5) cross-validation runs were executed.
    One advantage of using a decision tree learning algorithm is that the resulting decision tree classifier can be interpreted by humans.
    The decision tree generated for MUC-6, shown in Figure 2, seems to encapsulate a reasonable rule of thumb that matches our intuitive linguistic notion of when two noun phrases can corefer.
    It is also interesting to note that only 8 out of the 12 available features in the training examples are actually used in the final decision tree built.
    MUC-6 has a standard set of 30 test documents, which is used by all systems