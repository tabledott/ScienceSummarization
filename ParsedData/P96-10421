ults.
    We also show that sample selection yields a significant reduction in the size of the model used by the tagger.
  
  
    Many corpus-based methods for natural language processing (NLP) are based on supervised training&#8212; acquiring information from a manually annotated corpus.
    Therefore, reducing annotation cost is an important research goal for statistical NLP.
    The ultimate reduction in annotation cost is achieved by unsupervised training methods, which do not require an annotated corpus at all (Kupiec, 1992; Merialdo, 1994; Elworthy, 1994).
    It has been shown, however, that some supervised training prior to the unsupervised phase is often beneficial.
    Indeed, fully unsupervised training may not be feasible for certain tasks.
    This paper investigates an approach for optimizing the supervised training (learning) phase, which reduces the annotation effort required to achieve a desired level of accuracy of the trained model.
    In this paper, we investigate and extend the committe