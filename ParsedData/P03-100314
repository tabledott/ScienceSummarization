e web, etc.)
    Table 2 summarizes results of different combinations of training and test sets: For the TREC 2002 corpus, the relatively low MRRs are due to the small answer coverage of the TREC 2002 patterns.
    For the KM corpus, the relatively low MRRs are explained by two factors: (i) for this corpus, each evaluation pattern consists of only one string &#8211; the original answer; (ii) the KM questions are more complex than TREC questions (What piece of furniture is associated with Modred, Percival, Gawain, Arthur, and Lancelot?).
    It is interesting to see that using only the TREC9-10 data as training (system A in Table 2), we are able to beat the baseline when testing on TREC 2002 questions; however, this is not true when testing on KM questions.
    This can be explained by the fact that the TREC9-10 training set is similar to the TREC 2002 test set while it is significantly different from the KM test set.
    We also notice that expanding the training to TREC910Web (System B) and then to Quiz-Zone