English.
  
  
    Similar to last year&#8217;s workshop we carried out an extensive manual and automatic evaluation of machine translation performance for translating from four European languages into English, and vice versa.
    This year we substantially increased the number of automatic evaluation metrics and were also able to nearly double the efforts of producing the human judgments.
    There were substantial differences in the results results of the human and automatic evaluations.
    We take the human judgments to be authoritative, and used them to evaluate the automatic metrics.
    We measured correlation using Spearman&#8217;s coefficient and found that three less frequently used metrics were stronger predictors of human judgments than Bleu.
    They were: semantic role overlap (newly introduced in this workshop) ParaEval-recall and Meteor.
    Although we do not claim that our observations are indisputably conclusive, they again indicate that the choice of automatic metric can have a significant