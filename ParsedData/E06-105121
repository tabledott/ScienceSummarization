 have obtained less accuracy than the traditional bag-of-words approach (e.g.
    (Koster and Seutter, 2003)).
    Shallow linguistics information seems to be more effective to model the local context of the entities.
    Finally, we obtained worse results performing dimensionality reduction either based on generic linguistic assumptions (e.g. by removing words from stop lists or with certain PoS tags) or using statistical methods (e.g. tf.idf weighting schema).
    This may be explained by the fact that, in tasks like entity recognition and relation extraction, useful clues are also provided by high frequency tokens, such as stop words or punctuation marks, and by the relative positions in which they appear.
  
  
    First of all, the obvious references for our work are the approaches evaluated on AImed and LLL challenge data sets.
    In (Bunescu and Mooney, 2005b), the authors present a generalized subsequence kernel that works with sparse sequences containing combinations of words and PoS tags.
    The b