tained at (Teh, 2006).
    We also relate interpolated Kneser-Ney to the hierarchical Pitman-Yor language model.
    Our training data D consists of the number of occurrences cuw&#183; of each word w after each context u of length exactly n &#8722; 1.
    This corresponds to observing word w drawn cuw&#183; times from Gu.
    Given the training data D, we are interested in the posterior distribution over the latent vectors As mentioned previously, the hierarchical Chinese restaurant process marginalizes out each Gu, replacing it with the seating arrangement in the corresponding restaurant, which we shall denote by Su.
    Let S = {Sv : all contexts v}.
    We are thus interested in the equivalent posterior over seating arrangements instead: The most important quantities we need for language modelling are the predictive probabilities: what is the probability of a test word w after a context u?
    This is given by where the first probability on the right is the predictive probability under a particular setting