
  ROUGE: A Package For Automatic Evaluation Of Summaries
  
    for Recall-Oriented Understudy for Gisting Evaluation.
    It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.
    The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.
    This paper introduces four different included in the summarization evaluation package and their evaluations.
    Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.
  
  
    Traditionally evaluation of summarization involves human judgments of different quality metrics, for example, coherence, conciseness, grammaticality, readability, and content (Mani, 2001).
    However, even simple manual evaluation of summaries on a large scale over a few linguistic quality questi