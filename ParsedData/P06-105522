y similar.
    For example, after 4 SM cycles, the Fl scores of the 4 trained grammars have a variance of only 0.024, which is tiny compared to the deviation of 0.43 obtained by Matsuzaki et al. (2005)).
    Furthermore, these grammars allocate splits to nonterminals with a variance of only 0.32, so they agree to within a single latent state.
    One of the original motivations for lexicalization of parsers is the fact that part-of-speech (POS) tags are usually far too general to encapsulate a word&#8217;s syntactic behavior.
    In the limit, each word may well have its own unique syntactic behavior, especially when, as in modern parsers, semantic selectional preferences are lumped in with traditional syntactic trends.
    However, in practice, and given limited data, the relationship between specific words and their syntactic contexts may be best modeled at a level more fine than POS tag but less fine than lexical identity.
    In our model, POS tags are split just like any other grammar symbol: the subsymb