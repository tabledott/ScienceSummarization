 is that there is still a tendency to pick a segmentation containing fewer words.
    That is, given a choice between segmenting a sequence abc into abc and ab, c, the former will always be picked so long as its cost does not exceed the summed costs of ab and c: while; it is possible for abc to be so costly as to preclude the larger grouping, this will certainly not usually be the case.
    In this way, the method reported on here will necessarily be similar to a greedy method, though of course not identical.
    As the reviewer also points out, this is a problem that is shared by, e.g., probabilistic context-free parsers, which tend to pick trees with fewer nodes.
    The question is how to normalize the probabilities in such a way that smaller groupings have a better shot at winning.
    This is an issue that we have not addressed at the current stage of our research.
    Classical metric multidimensional scaling of distance matrix, showing the two most significant dimensions.
    The percentage scores on t