e about 16000 constituents which contributed to precision and recall measurements, versus 151 million productions potentially contributing to entropy.
    Thus, we choose entropy as our measure of correctness for most experiments.
    When we did measure precision and recall, we used the metric as defined by Collins (1996).
    Note that the fact that entropy changes smoothly and monotonically is critical for the performance of the multiple parameter optimization algorithm.
    Furthermore, we may have to run quite a few iterations of that algorithm to get convergence, so the fact that entropy is smooth for relatively small numbers of sentences is a large help.
    Thus, the discovery that entropy is a good surrogate for precision and recall is non-trivial.
    The same kinds of observations could be extended to speech recognition to optimize multiple thresholds there (the typical modern speech system has quite a few thresholds), a topic for future research.
    Note that for some sentences, with too tight th