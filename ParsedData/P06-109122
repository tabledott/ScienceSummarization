nificantly outperform phrase-based models, the combination of wordbased and phrase-based features performs better than those features types taken separately.
    Additionally, swap-based re-ordering slightly improves performance over monotone decoding.
    For all experiments, the training BLEU score remains significantly lower than the maximum obtainable BLEU score shown in line and line .
    In this respect, there is significant room for improvements in terms of feature functions and alternative set generation.
    The word-based models perform surprisingly well, i.e. the model in line uses only three feature types: model features like in Section 2, distortion features, and target language m-gram features up to .
    Training speed varies depending on the feature types used: for the simplest model shown in line of Table 3, the training takes about hours, for the models usence) and the test set (lower graph; BLEU with four references) as a function of the training iteration for the model corresponding to li