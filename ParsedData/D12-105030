arse tree.
    However, the composed representations must be learned with a neural network.
    We evaluated nine models on the complementary tasks of phrase similarity and paraphrase detection.
    The former task simplifies the challenge of finding an adequate method of composition and places more emphasis on the representation, whereas the latter poses, in a sense, the ultimate challenge for composition models.
    It involves entire sentences exhibiting varied syntactic constructions and in the limit involves genuine natural language undertanding.
    Across both tasks our results deliver a consistent message: simple is best.
    Despite being in theory more expressive, the representations obtained by the neural language model and the third-order tensor cannot match the simple semantic space on the phrase similarity task.
    In this task syntax-oblivious composition models are superior to the more sophisticated recursive autoencoder.
    The latter performs better on the paraphrase detection task when it