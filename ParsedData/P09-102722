 (15, 15) examples and imbalanced growth of (1, 5), (5, 1) examples are compared in the figure.
    We can see that the performance of the cotraining approach with the balanced growth can be improved after a few iterations.
    And the performance of the co-training approach with large p and n will more quickly become unchanged, because the approach runs out of the limited examples in the unlabeled set more quickly.
    However, the performance of the co-training approaches with the two imbalanced growths is always going down quite rapidly, because the labeled unbalanced examples hurt the performance badly at each iteration.
    In the above experiments, all features (unigram + bigram) are used.
    As mentioned earlier, feature selection techniques are widely used for dimension reduction.
    In this section, we further conduct experiments to investigate the influences of feature selection techniques on the classification results.
    We use the simple but effective document frequency (DF) for feature select