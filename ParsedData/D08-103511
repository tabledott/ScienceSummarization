j,i = E{t:zt=j} mt,i, where mt,i is the count of word i in sentence t. Assuming that each xt &#8212; &#952;j, then the posterior distribution for &#952;j is Dirichlet with vector parameter nj +&#952;0 (Bernardo and Smith, 2000).
    The expected value of this distribution is the multinomial distribution &#710;&#952;j, where, In this equation, W indicates the number of words in the vocabulary.
    Having obtained an estimate for the language model &#710;&#952;j, the observed data likelihood for segment j is a product over each sentence in the segment, 2Our experiments will assume that the number of topics K is known.
    This is common practice for this task, as the desired number of segments may be determined by the user (Malioutov and Barzilay, 2006).
    By viewing the likelihood as a product over all terms in the vocabulary, we observe interesting connections with prior work on segmentation and information theory.
    In this section, we explain how our model generalizes the well-known method of Utiyama an