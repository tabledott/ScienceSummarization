.e., HMM hidden states) t given words w. This recent literature reports contradictory results about these Bayesian inference methods.
    Johnson (2007) compared two Bayesian inference algorithms, Variational Bayes and what we call here a point-wise collapsed Gibbs sampler, and found that Variational Bayes produced the best solution, and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM.
    On the other hand, Goldwater and Griffiths (2007) reported that the same kind of Gibbs sampler produced much better results than EM on their unsupervised POS tagging task.
    One of the primary motivations for this paper was to understand and resolve the difference in these results.
    We replicate the results of both papers and show that the difference in their results stems from differences in the sizes of the training data and numbers of states in their models.
    It turns out that the Gibbs sampler used in these earlier papers is not the only kind of sampler for HMMs.
    T