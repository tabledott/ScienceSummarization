 prepositions that we work with, the total number of insertions, deletions and substitutions that require sufficient training events and might need different score combinations is 168, making the problem much harder.
    To find out if it is possible to reduce the required amount of annotated preposition errors for a system that still covers more than one third of the preposition errors, we ran the same learning curve experiments but now only taking the four most frequent prepositions into account: to, of, in, for.
    In the CLC, these four prepositions account for 39.8% of preposition error flags.
    As in the previous experiments, however, we found that we are not able to outperform the baseline by using just 1% of annotated data.
  
  
    We have conducted a failure analysis on examples where the system produces a blatantly bad suggestion in order to see whether this decision could be attributed to the error-specific classifier or to the language model, or both, and what the underlying cause is.
    Thi