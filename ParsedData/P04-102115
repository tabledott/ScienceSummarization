e has been aligned into a sequence of transliteration pair tokens, we can calculate the probability of test set models to the token sequence.
    The cross-entropy Hp (W) of a model on data W is defined as number of aligned transliteration pair tokens in the data W. The perplexity PPp (W) of a model is the reciprocal of the average probability assigned by the model to each aligned pair in the test set W We have the perplexity reported in Table 2 on the aligned bilingual dictionary, a database of 119,364 aligned tokens.
    The NCM perplexity is computed using n-gram equivalents of eqn.
    (8) for E2C transliteration, while TM perplexity is based on those of eqn (9) which applies to both E2C and C2E.
    It is shown that TM consistently gives lower perplexity than NCM in open and closed tests.
    We have good reason to expect TM to provide better transliteration results which we expect to be confirmed later in the experiments.
    The Viterbi algorithm produces the best sequence by maximizing the overall pro