igated the application of discriminative methods to NLP tasks, with mixed results.
    Klein and Manning (2002) argue that these results show a pattern where discriminative probability models are inferior to generative probability models, but that improvements can be achieved by keeping a generative probability model and training according to a discriminative optimization criteria.
    We show how this approach can be applied to broad coverage natural language parsing.
    Our estimation and training methods successfully balance the conflicting requirements that the training method be both computationally tractable for large datasets and a good approximation to the theoretically optimal method.
    The parser which uses this approach outperforms both a generative model and a discriminative model, achieving state-of-the-art levels of performance (90.1% F-measure on constituents).
    To compare these different approaches, we use a neural network architecture called Simple Synchrony Networks (SSNs) (Lane and He