ng topics and the order in which these topics appear in texts from the same domain.
    For instance, a typical earthquake newspaper report contains information about the quake&#8217;s epicenter, how much it measured, the time it was felt, and whether there were any victims or damage.
    By encoding constraints on the ordering of these topics, content models have a pronounced advantage in modeling document structure because they can learn to represent how documents begin and end, but also how the discourse shifts from one topic to the next.
    Like LSA, the content models are lexicalized; however, unlike LSA, they are domain-specific, and would expectedly yield inferior performance on out-of-domain texts.
    Barzilay and Lee (2004) implemented content models using an HMM wherein states correspond to distinct topics (for instance, the epicenter of an earthquake or the number of victims), and state transitions represent the probability of changing from one topic to another, thereby capturing possible topic-p