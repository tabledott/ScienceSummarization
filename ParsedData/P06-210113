nima that could open up at higher &#947;.
    Figure 3 shows how the smoothing is gradually weakened to reach the risk objective (3) as &#947; &#8212;* 1 and approach the true error objective (2) as &#947; &#8212;* oc.
    Our risk minimization most resembles the work of Rao and Rose (2001), who trained an isolatedword speech recognition system for expected word-error rate.
    Deterministic annealing has also been used to tackle non-convex likelihood surfaces in unsupervised learning with EM (Ueda and Nakano, 1998; Smith and Eisner, 2004).
    Other work on &#8220;generalized probabilistic descent&#8221; minimizes a similar objective function but with &#947; held constant (Katagiri et al., 1998).
    Although the entropy is generally higher at lower values of &#947;, it varies as the optimization changes &#952;.
    In particular, a pure unregularized loglinear model such as (5) is really a function of &#947;&#183;&#952;, so the optimizer could exactly compensate for increased &#947; by decreasing the &#952;