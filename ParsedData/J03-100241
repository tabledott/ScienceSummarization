ility and the alignment parameters.
    In the E-step of the EM algorithm for the fertility-based alignment models, we use the Viterbi alignment and its neighborhood.
    Unless stated otherwise, no bilingual dictionary is used in training.
    Tables 4 and 5 compare the alignment quality achieved using various models and training schemes.
    In general, we observe that the refined models (Models 4, 5, and 6) yield significantly better results than the simple Model 1 or Dice coefficient.
    Typically, the best results are obtained with Model 6.
    This holds across a wide range of sizes for the training corpus, from an extremely small training corpus of only 500 sentences up to a training corpus of 1.5 million sentences.
    The improvement that results from using a larger training corpus is more significant, however, if more refined models are used.
    Interestingly, even on a tiny corpus of only 500 sentences, alignment error rates under 30% are achieved for all models, and the best models have error ra