
    Empirical studies suggest that only a small percentage of human translations violate these constraints (Cherry and Lin, 2006).
    Stochastic ITGs are parameterized like their PCFG counterparts (Wu, 1997); productions A &#8212;* X are assigned probability Pr(X |A).
    These parameters can be learned from sentence-aligned bitext using the EM algorithm.
    The expectation task of counting productions weighted by their probability is handled with dynamic programming, using the inside-outside algorithm extended to bitext (Zhang and Gildea, 2004).
  
  
    This paper introduces a phrasal ITG; in doing so, we combine ITG with the JPTM.
    ITG parsing algorithms consider every possible two-dimensional span of bitext, each corresponding to a bilingual phrase pair.
    Each multi-token span is analyzed in terms of how it could be built from smaller spans using a straight or inverted production, as is illustrated in Figures 2 (a) and (b).
    To extend ITG to a phrasal setting, we add a third option for span a