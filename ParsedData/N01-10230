
  Applying Co-Training Methods To Statistical Parsing
  
    We propose a novel Co-Training method for statistical parsing.
    The algorithm takes as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text.
    The algorithm iteratively labels the entire data set with parse trees.
    Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data.
  
  
    The current crop of statistical parsers share a similar training methodology.
    They train from the Penn Treebank (Marcus et al., 1993); a collection of 40,000 sentences that are labeled with corrected parse trees (approximately a million word tokens).
    In this paper, we explore methods for statistical parsing that can be used to combine small amounts of labeled data with unlim