y&#8216;, to the one that learns three classes; &#8216;between&#8216;, &#8216;modify&#8216; and &#8216;beyond&#8216;.
    Their model can also avoid the influence of the exceptional dependency relations.
    Using same training and test data, we can achieve accuracy of 89.29%.
    The difference is considerable.
    Kanayama et al. (2000) use an HPSG-based Japanese grammar to restrict the candidate dependencies.
    Their model uses at most three candidates restricted by the grammar as features; the nearest, the second nearest, and the farthest from the modifier.
    Thus, their model can take longer context into account, and disambiguate complex dependency relations.
    However, the features are still static, and dynamic features are not used in their model.
    We cannot directly compare their model with ours because they use a different corpus, EDR corpus, which is ten times as large as the corpus we used.
    Nevertheless, they reported an accuracy 88.55%, which is worse than our model.
    Haruno et al.