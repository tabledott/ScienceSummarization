ses, and STATEMENTS elicit BACKCHANNELS in 26% of all cases.
    We also investigated non-n-gram discourse models, based on various language modeling techniques known from speech recognition.
    One motivation for alternative models is that n-grams enforce a one-dimensional representation on DA sequences, whereas we saw above that the event space is really multidimensional (DA label and speaker labels).
    Another motivation is that n-grams fail to model long-distance dependencies, such as the fact that speakers may tend to repeat certain DAs or patterns throughout the conversation.
    The first alternative approach was a standard cache model (Kuhn and de Mori 1990), which boosts the probabilities of previously observed unigrams and bigrams, on the theory that tokens tend to repeat themselves over longer distances.
    However, this does not seem to be true for DA sequences in our corpus, as the cache model showed no improvement over the standard N-gram.
    This result is somewhat surprising since unigram