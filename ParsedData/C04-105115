ny-to null word mappings.
			Our a priori assumption was that the lower the AER for a corpus, the more likely it would be to yield learnable information about paraphrase alternations.
			We closely followed the evaluation standards established in Melamed (2001) and Och &amp; Ney (2000, 2003).
			Following Och &amp; Ney?s methodology, two annotators each created an initial annotation for each dataset, subcategorizing alignments as either SURE (necessary) or POSSIBLE (allowed, but not required).
			Differences were then highlighted and the annotators were asked to review these cases.
			Finally we combined the two annotations into a single gold standard in the following manner: if both annotators agreed that an alignment should be SURE, then the alignment was marked as sure in the gold-standard; otherwise the alignment was marked as POSSIBLE.
			To compute Precision, Recall, and Alignment Error Rate (AER) for the twin datasets, we used exactly the formulae listed in Och &amp; Ney (2003).
			Let A be the set of 