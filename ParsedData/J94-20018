nterpolation schemes are possible.
    For example, different coefficients can be used depending on the count of (t1, t2), with the intuition that relative frequencies can be trusted more when this count is high.
    Another possibilitity is to interpolate also with models of different orders, such as hrf (t3/t2) or hrf (t3).
    Smoothing can also be achieved with procedures other than interpolation.
    One example is the &amp;quot;backing-off&amp;quot; strategy proposed by Katz (1987).
    Using a triclass model M it is possible to compute the probability of any sequence of words W according to this model: where the sum is taken over all possible alignments.
    The Maximum Likelihood (ML) training finds the model M that maximizes the probability of the training text: where the product is taken over all the sentences W in the training text.
    This is the problem of training a hidden Markov model (it is hidden because the sequence of tags is hidden).
    A well-known solution to this problem is the Forwar