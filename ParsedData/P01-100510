able for free.
    Surely we cannot reasonably expect that the manual annotation of one billion words along with corresponding parse trees will occur any time soon (but see (Banko and Brill 2001) for a discussion that this might not be completely infeasible).
    Despite this pitfall, there are techniques one can use to try to obtain the benefits of considerably larger training corpora without incurring significant additional costs.
    In the sections that follow, we study two such solutions: active learning and unsupervised learning.
    Active learning involves intelligently selecting a portion of samples for annotation from a pool of as-yet unannotated training samples.
    Not all samples in a training set are equally useful.
    By concentrating human annotation efforts on the samples of greatest utility to the machine learning algorithm, it may be possible to attain better performance for a fixed annotation cost than if samples were chosen randomly for human annotation.
    Most active learning approac