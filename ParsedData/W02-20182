 order to accurately estimate the model&#8217;s parameters.
    While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are usually quite large, and frequently contain hundreds of thousands of free parameters.
    Estimation of such large models is not only expensive, but also, due to sparsely distributed features, sensitive to round-off errors.
    Thus, highly efficient, accurate, scalable methods are required for estimating the parameters of practical models.
    In this paper, we consider a number of algorithms for estimating the parameters of ME models, including Generalized Iterative Scaling and Improved Iterative Scaling, as well as general purpose optimization techniques such as gradient ascent, conjugate gradient, and variable metric methods.
    Surprisingly, the widely used iterative scaling algorithms perform quite poorly, and for all of the test problems, a limited memory variable metric algorithm outperformed the other ch