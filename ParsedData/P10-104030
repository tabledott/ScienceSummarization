nd that combining different word representations can improve accuracy further.
    Error analysis indicates that Brown clustering induces better representations for rare words than C&amp;W embeddings that have not received many training updates.
    Another contribution of our work is a default method for setting the scaling parameter for word embeddings.
    With this contribution, word embeddings can now be used off-the-shelf as word features, with no tuning.
    Future work should explore methods for inducing phrase representations, as well as techniques for increasing in accuracy by using word representations in compound features.
    Replicating our experiments You can visit http://metaoptimize.com/ projects/wordreprs/ to find: The word representations we induced, which you can download and use in your experiments; The code for inducing the word representations, which you can use to induce word representations on your own data; The NER and chunking system, with code for replicating our experiments.
  
  