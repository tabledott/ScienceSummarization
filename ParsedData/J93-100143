ple.
    It is important in this procedure to use two different samples of text.
    If we were to use the same sample for both testing and training, we would obtain an overly optimistic estimate of how well the code performs.
    The other codes in Table 5 make better use of context (h), and therefore, they achieve better compression.
    For example, Huffman coding on words (a unigram model) is more than twice as compact as Huffman coding on characters (2.1 vs. 5 bits/char.).
    The unigram model is also more than twice as good as Lempel-Ziv (2.1 vs. 4.43 bits/ char.
    ), demonstrating that compress, a popular UnixTM tool for compressing files, could be improved by a factor of two (when the files are in English).
    The trigram model, the method of choice in speech recognition, achieves 1.76 bits per character, outperforming the practical alternatives in Table 5, but falling half a bit shy of Shannon's estimate of human performance.'
    Someday parsers might help squeeze out some of this remaining half