luating alignment after trainng the Giza++ model.
    Although the overall AER of 11.58% is higher than the best bilingual MT systems (Och &amp; Ney, 2003), the training data is inherently noisy, having more in common with analogous corpora than conventional MT parallel corpora in that the paraphrases are not constrained by the source text structure.
    The identical word AER of 10.57% is unsurprising given that the domain is unrestricted and the alignment algorithm does not employ direct string matching to leverage word identity.5 The non-identical word AER of 20.88% may appear problematic in a system that aims to generate paraphrases; as we shall see, however, this turns out not to be the case.
    Ablation experiments, not described here, indicate that additional data will improve AER.
    Recent work in SMT has shown that simple phrase-based MT systems can outperform more sophisticated word-based systems (e.g.
    Koehn et al. 2003).
    Therefore, we adopt a phrasal decoder patterned closely after that 