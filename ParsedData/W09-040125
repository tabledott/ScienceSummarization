 consistent the automatic metrics were with the human judgments.
    The way that we calculated consistency was the following: for every pairwise comparison of two systems on a single sentence by a person, we counted the automatic metric as being consistent if the relative scores were the same (i.e. the metric assigned a higher score to the higher ranked system).
    We divided this by the total number of pairwise comparisons to get a percentage.
    Because the systems generally assign real numbers as scores, we excluded pairs that the human annotators ranked as ties.
  
  
    Table 7 shows the correlation of automatic metrics when they rank systems that are translating into English.
    Note that TERp, TER and wcd6p4er are error metrics, so a negative correlation is better for them.
    The strength of correlation varied for the different language pairs.
    The automatic metrics were able to rank the French-English systems reasonably well with correlation coefficients in the range of .8 and .9.
    In com