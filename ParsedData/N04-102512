 in at least one grade language model, we can perform regression with a Gaussian kernel (Hastie et al., 2001, p. 165) across all grade models to obtain a smoothed value for P(w|G).
    With training, we found the optimal kernel width to be 2.5 grade levels.
    If w does not occur in any grade model (an &#8216;out-of-vocabulary&#8217; type) we can back off to a traditional semantic variable.
    In this study, we used an estimate which is a function of type length: where w is a type, i is a grade index between 1 and 12, |w |is w&#8217;s length in characters, and C = -13, D = 10 based on statistics from the Web corpus.
    Feature selection is an important step in text classification: it can lessen the computational burden by reducing the number of features and increase accuracy by removing &#8216;noise&#8217; words having low predictive power.
    The first feature selection step for many text classifiers is to remove the most frequent types (&#8216;stopwords&#8217;).
    This must be considered carefully for