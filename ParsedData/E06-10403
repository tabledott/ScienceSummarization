996), involving subjects reading and rating texts; usually subjects are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts.
    In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison.
    This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today.
    Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005).
    In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts.
    As in other areas of NLP, the advantages of automatic corpusbased evaluation are tha