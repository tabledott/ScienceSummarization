 unlabeled data without difficulty, while labeled data are costly to obtain.
    Therefore, a natural question is whether we can use unlabeled data to build a more accurate classifier, given the same amount of labeled data.
    This problem is often referred to as semi-supervised learning.
    Although a number of semi-supervised methods have been proposed, their effectiveness on NLP tasks is not always clear.
    For example, co-training (Blum and Mitchell, 1998) automatically bootstraps labels, and such labels are not necessarily reliable (Pierce and Cardie, 2001).
    A related idea is to use Expectation Maximization (EM) to impute labels.
    Although useful under some circumstances, when a relatively large amount of labeled data is available, the procedure often degrades performance (e.g.
    Merialdo (1994)).
    A number of bootstrapping methods have been proposed for NLP tasks (e.g.
    Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)).
    But these typically assume a very small am