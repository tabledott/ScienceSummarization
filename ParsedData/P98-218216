esent in Wordnet for the two categories combined, our algorithm produced 106, or over 3 for every 5 valid terms produced.
    It is for this reason that we are billing our algorithm as something that could enhance existing broadcoverage resources with domain-specific lexical information.
  
  
    We have outlined an algorithm in this paper that, as it stands, could significantly speed up the task of building a semantic lexicon.
    We have also examined in detail the reasons why it works, and have shown it to work well for multiple corpora and multiple categories.
    The algorithm generates many words not included in broad coverage resources, such as Wordnet, and could be thought of as a Wordnet &amp;quot;enhancer&amp;quot; for domain-specific applications.
    More generally, the relative success of the algorithm demonstrates the potential benefit of narrowing corpus input to specific kinds of constructions, despite the danger of compounding sparse data problems.
    To this end, parsing is invaluable.
  
