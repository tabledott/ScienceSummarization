e sentence-level BLEU calculated in the context of previous 1-best translations as Chiang et al.
    (2008b; 2009).
    We ran MIRA for 30 iterations.
    We used the MegaM classifier and sampled as described in Section 4.2.
    As previously noted, we used BLEU+1 (Liang et al., 2006) for g. MegaM was easy to set up and ran fairly quickly, however any linear binary classifier that operates on real-valued features can be used, and in fact we obtained similar results using the support vector machine module of WEKA (Hall et al., 2009) as well as the Stanford classifier (Manning and Klein, 2003).
    We ran for up to 30 iterations and used the same k and stopping criterion as was used for MERT, though variability of sampling precluded list convergence.
    While MERT and MIRA use each iteration&#8217;s final weights as a starting point for hill-climbing the next iteration, the pairwise ranking approach has no explicit tie to previous iterations.
    To incorporate such stability into our process we interpolated t