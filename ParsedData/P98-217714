ers c/interp and c/bigram clearly outperform the. baseline, but the classifier does not outperform dbigraml even though it interpolates between the less specific evidence (the preposition counts) and more specific evidence (the bigram counts).
    This may be due to the errors in our extracted training data; supervised classifiers that train from clean data typically benefit greatly by combining less specific evidence with more specific evidence.
    Despite the errors in the training data, the performance of the unsupervised classifiers (81.9%) begins to approach the best performance of the comparable supervised classifiers (84.5%).
    (Our goal is to replicate the supervision of a treebank, but not a semantic dictionary, so we do not compare against (Stetina and Nagao, 1997).)
    Furthermore, we do not use the second noun n2, whereas the best supervised methods use this information.
    Our result shows that the information in imperfect but abundant data from unambiguous attachments, as shown in Tables 2 