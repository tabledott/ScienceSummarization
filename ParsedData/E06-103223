to the degree that it is currently believed to do.
    We have shown that Bleu&#8217;s rather coarse model of allowable variation in translation can mean that an improved Bleu score is not sufficient to reflect a genuine improvement in translation quality.
    We have further shown that it is not necessary to receive a higher Bleu score in order to be judged to have better translation quality by human subjects, as illustrated in the 2005 NIST Machine Translation Evaluation and our experiment manually evaluating Systran and SMT translations.
    What conclusions can we draw from this?
    Should we give up on using Bleu entirely?
    We think that the advantages of Bleu are still very strong; automatic evaluation metrics are inexpensive, and do allow many tasks to be performed that would otherwise be impossible.
    The important thing therefore is to recognize which uses of Bleu are appropriate and which uses are not.
    Appropriate uses for Bleu include tracking broad, incremental changes to a single system