th employ a weighted combination of all features.
    The decision-tree and logic-based approaches all attempt to find a combination of a relatively small set of features that accurately predict classification.
    After training on 1,200 examples, the symbolic structures learned for the line corpus are relatively large.
    Average sizes are 369 leaves for C4.5 decision trees, 742 literals for PR:ill,D LIST decision lists, 841 literals for PFoiL-DNF formulae, and 1197 literals for PFoiL-CNF formulae.
    However, many nodes or literals can test the same feature and the last two results include the total literal count for six separate DNF or CNF formulae (one for each sense).
    Therefore, each discrimination is clearly only testing a relatively small fraction of the 2,859 available features.
    Nearest neighbor bases its classifications on all features; however, it weights them all equally.
    Therefore, differential weighting is apparently necessary for high-performance on this problem.
    Alternative i