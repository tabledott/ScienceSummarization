ive set of training sentences.
    More precisely, let f denote the source sentences of a training corpus with given reference translations vidual sentences, i.e., EprS 1 , and let Cs&#65533;tes,1, ..., es,Kudenote a set of K candidate translations.
    Assuming that the corpusbased error count for some translations eS 1 is additively decomposable into the error counts of the indiIn (Och, 2003), it was shown that linear models can effectively be trained under the MERT criterion using a special line optimization algorithm.
    This line optimization determines for each feature function hm and sentence fs the exact error surface on a set of candidate translations Cs.
    The feature function weights are then adjusted by traversing the error surface combined over all sentences in the training corpus and moving the weights to a point where the resulting error reaches a minimum.
    Candidate translations in MERT are typically represented as N-best lists which contain the N most probable translation hypotheses.
  