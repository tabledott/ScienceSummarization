omthe second one, the TREC evaluation grants it a ?cor rect and well-supported?
			point, since the document ID matches the correct document ID?even though the latter answer does not entail the true answer.
			Our evaluation does not suffer from this problem.We report two standard evaluation measures commonly used in IR and QA research: mean av erage precision (MAP) and mean reciprocal rank (MRR).
			All results are produced using the standard trec eval program.
			5.2 Baseline Systems.
			We implemented two state-of-the-art answer-finding algorithms (Cui et al, 2005; Punyakanok et al, 2004) as strong baselines for comparison.
			Cui et al.
			(2005) is the answer-finding algorithm behindone of the best performing systems in TREC eval uations.
			It uses a mutual information-inspired scorecomputed over dependency trees and a single alignment between them.
			We found the method to be brit tle, often not finding a score for a testing instance because alignment was not possible.
			We extendedthe original algor