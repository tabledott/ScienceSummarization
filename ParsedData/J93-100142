 h and the code.
    In the special case of ASCII, where Pr(s h, ASCII) = 1/256, we can actually carry out the indicated sum, and find, not surprisingly, that ASCII requires 8 bits per character: In more difficult cases, cross entropy is estimated by a sampling procedure.
    Two independent samples of the source are collected: Si and 52.
    The first sample, Si, is used to fit the values of the parameters of the code, and second sample, S2, is used to test the fit.
    For example, to determine the value of 5 bits per character for the Huffman code in Table 5, we counted the number of times that each of the 256 ASCII characters appeared in Si, a sample of Ni characters selected from the Wall Street Journal text distributed by the ACL/DCI.
    These counts were used to determine Pr(s h, code) (or rather Pr(s I code), since the Huffman code doesn't depend on h).
    Then we collected a second sample, Sz, of N2 characters, and tested the fit with the formula: where 52 [i] is the ith character in the second sam