EM.
    CE was found to outperform EM on the task of focus in this paper, when applied to English data (Smith and Eisner, 2005b).
    Here we review the method briefly, show how it performs across languages, and demonstrate that it can be combined effectively with structural bias.
    Contrastive training defines for each example xi a class of presumably poor, but similar, instances called the &#8220;neighborhood,&#8221; N(xi), and seeks to maximize At this point we switch to a log-linear (rather than stochastic) parameterization of the same weighted grammar, for ease of numerical optimization.
    All this means is that O (specifically, pstop and pchild in Eq.
    1) is now a set of nonnegative weights rather than probabilities.
    Neighborhoods that can be expressed as finitestate lattices built from xi were shown to give significant improvements in dependency parser quality over EM.
    Performance of CE using two of those neighborhoods on the current model and datasets is shown in Table 2.9 0-mean diagon