 against each other for similar (top) and dissimilar (bottom) word pairs, using the MarkovLink (left) and MarkovGloss (right) model variants.
			zlingly#r#1,?
			and then on to other TokenPOS nodes used in its definition: ?in a manner or to a degree that dazzles the beholder.?Figure 1 demonstrates how two word-specific station ary distributions are more highly correlated if the words are related.
			In both model variants, random walks for related words are more likely to visit the same parts of the graph, and so assign higher probability to the same nodes.
			Figure 1 also shows that the MarkovGloss variantproduces distributions with a much wider range of proba bilities than the MarkovLink, which might be a source of difficulty in integrating the two model variants.
			Figure 2 shows the correlation between the stationary distributions produced by the two model variants for the same word.
			The log-log scale makes it possible to see the entire range of probabilities on the same axes, and shows that distribu