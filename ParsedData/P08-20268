s mentioned previously, NANC is a news corpus, quite like the original WSJ data.
    At 81.4% it gives us a one percent improvement over the original WSJ parser.
    The topmost line, is the C/J parser trained on Medline data.
    As can be seen, even just a thousand lines of Medline is already enough to drive our results to a new level and it continues to improve until about 150,000 sentences at which point performance is nearly flat.
    However, as 270,000 sentences is fractionally better than 150,000 sentences that is the number of self-training sentences we used for our results on the test set.
    Lastly, the middle jagged line is for an interesting idea that failed to work.
    We mention it in the hope that others might be able to succeed where we have failed.
    We reasoned that textbooks would be a particularly good bridging corpus.
    After all, they are written to introduce someone ignorant of a field to the ideas and terminology within it.
    Thus one might expect that the English of a Biology