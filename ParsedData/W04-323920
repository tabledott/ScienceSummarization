second approach, is also left to as an open question.
    We employed a McNemar&#8217;s paired test on the labeling disagreements.
    Underlined results indicate that there is a significant difference (p &lt; 0.01) against the baseline (bow).
    If there is a statistical difference (p &lt; 0.01) between Boosting and SVMs with the same feature representation (bow / dep / n-gram), better results are asterisked.
    In the previous section, we described the merits of our Boosting algorithm.
    We experimentally verified these merits from the results of the PHS task.
    As illustrated in section 4, our method can automatically select relevant and compact features from a number of feature candidates.
    In the PHS task, a total 1,793 features (rules) were selected, while the set sizes of distinct uni-gram, bi-gram and trigram appearing in the data were 4,211, 24,206, and 43,658 respectively.
    Even though all subtrees are used as feature candidates, Boosting selects a small and highly relevant subset of fea