the probability ( symbol given that the Markov process is i In part-of-speech tagging, we will model word order di pendency through an underlying Markov process that 01 erates in terms of lexical tags,' yet we will only be ab to observe the sets of tags, or ambiguity classes, that ai possible for individual words.
    The ambiguity class of eac word is the set of its permitted parts of speech, only or of which is correct in context.
    Given the parameters .4, Markov modeling allows us to compute ti most probable sequence of state transitions, and hence a mostly likely sequence of lexical tags, corresponding to of ambiguity classes.
    In the following, identified with the number of possible .tags, and W wil the set of all ambiguity classes.
    Applying an HMM consists of two tasks: estimating ti parameters A, a training set; ar computing the most likely sequence of underlying sta transitions given new observations.
    Maximum likeliho( estimates (that is, estimates that maximize the probabili of the trai