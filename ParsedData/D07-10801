tures for statistical machine translation trained ona large training corpus.
			In this framework, the prob lem of translation is regarded as a sequential labeling problem, in the same way as part-of-speech tagging, chunking or shallow parsing.
			However, the use of alarge number of features did not provide any signifi cant improvements over a conventional small feature set.
			Bangalore et al (2006) trained the lexical choice model by using Conditional Random Fields (CRF) realized on a WFST.
			Their modeling was reduced toMaximum Entropy Markov Model (MEMM) to han dle a large number of features which, in turn, faced the labeling bias problem (Lafferty et al, 2001).
			Tillmann and Zhang (2006) trained their feature set using an online discriminative algorithm.
			Since thedecoding is still expensive, their online training approach is approximated by enlarging a merged k best list one-by-one with a 1-best output.
			Lianget al (2006) introduced an averaged perceptron al gorithm, but employed only 1-best tra