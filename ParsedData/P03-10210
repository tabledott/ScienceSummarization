
  Minimum Error Rate Training In Statistical Machine Translation
  
    Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.
    A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text.
    In this paper, we analyze various training criteria which directly optimize translation quality.
    These training criteria make use of recently proposed automatic evaluation metrics.
    We describe a new algorithm for efficient training an unsmoothed error count.
    We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure.
  
  
    Many tasks in natural language processing have evaluation criteria that go beyond simply counting the number of wrong decisions the system makes.
    Some often used criteria are, for example, F-Measure for parsing, mean average precision for ranked 