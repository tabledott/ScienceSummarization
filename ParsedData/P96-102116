two kinds of productions allow words of either Chinese or English to go unmatched.
    The SBTG assigns a probability Pr(c, e, q) to all generable trees q and sentence-pairs.
    In principle it can be used as the translation channel model by normalizing with Pr(e) and integrating out Pr(q) to give Pr(cle) in Equation (2).
    In practice, a strong language model makes this unnecessary, so we can instead optimize the simpler Viterbi approximation To complete the picture we add a bigram model get&#8212;let = g(e.i 1e3_ Pr(e).
    1) for the English language model Offset, alignment, or distortion parameters are entirely eliminated.
    A large part of the implicit function of such parameters&#8212;to prevent alignments where too many frame arguments become separated&#8212;is rendered unnecessary by the BTG's structural constraints, which prohibit many such configurations altogether.
    Another part of the parameters' purpose is subsumed by the SBTG's probabilities all and a0, which can be set to prefer straigh