   Model selection (using the &#8220;step&#8221; procedure) discarded all but the IN and Brown ID explanatory variables.
    The final estimated model is shown in Table 9.
    It shows that the WSJ+NANC/WSJ parser becomes more likely to have a higher f-score than the BROWN/BROWN parser as the number of prepositions in the sentence increases, and that the BROWN/BROWN parser is more likely to have a higher f-score on Brown sections K, N, P, G and L (these are the general fiction, adventure and western fiction, romance and love story, letters and memories, and mystery sections of the Brown corpus, respectively).
    The three sections of BROWN not in this list are F, M, and R (popular lore, science fiction, and humor).
  
  
    We have demonstrated that rerankers and selftrained models can work well across domains.
    Models self-trained on WSJ appear to be better parsing models in general, the benefits of which are not limited to the WSJ domain.
    The WSJtrained reranker using out-of-domain LA Times parses 