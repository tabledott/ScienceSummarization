lores data set sub-sampling methods.
    Unlike our work, S&#248;gaard found that simply concatenating all the data led to degradation in performance.
    Cohen et al. (2011) explores the idea learning language specific mixture coefficients for models trained independently on the target language treebanks.
    However, their results show that this method often did not significantly outperform uniform mixing.
  
  
    Comparing unsupervised and parser projection systems is difficult as many publications use nonoverlapping sets of languages or different evaluation criteria.
    We compare to the following three systems that do not augment the treebanks and report results for some of the languages that we considered: Naseem et al. (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model.
    In addition to their original results, we also report results using the same part-of-speech tagset as the systems described in this paper (USR&#8224;).
    This 