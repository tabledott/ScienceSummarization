   Our concern in this paper is with two other problems with Model 1 that are not deeply structural, and can be addressed merely by changing how the parameters of Model 1 are estimated.
    The first of these nonstructural problems with Model 1, as standardly trained, is that rare words in the source language tend to act as &#8220;garbage collectors&#8221; (Brown et al., 1993b; Och and Ney, 2004), aligning to too many words in the target language.
    This problem is not unique to Model 1, but anecdotal examination of Model 1 alignments suggests that it may be worse for Model 1, perhaps because Model 1 lacks the fertility and distortion parameters that may tend to mitigate the problem in more complex models.
    The cause of the problem can be easily understood if we consider a situation in which the source sentence contains a rare word that only occurs once in our training data, plus a frequent word that has an infrequent translation in the target sentence.
    Suppose the frequent source word has the transl