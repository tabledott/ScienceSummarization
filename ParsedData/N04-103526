220;de.&#8221;
  
  
    The fundamental assumption underlying much recent work in statistical machine translation (Yamada and Knight, 2001; Eisner, 2003; Gildea, 2003) is that local transformations (primarily child-node re-orderings) of one-level parent-children substructures are an adequate model for parallel corpora.
    Our empirical results suggest that this may be too strong of an assumption.
    To explain the data in two parallel corpora, one English-French, and one English-Chinese, we are often forced to learn rules involving much larger tree fragments.
    The theory, algorithms, and transformation rules we learn automatically from data have several interesting aspects.
    1.
    Our rules provide a good, realistic indicator of the complexities inherent in translation.
    We believe that these rules can inspire subsequent developments of generative statistical models that are better at explaining parallel data than current ones.
    2.
    Our rules put at the fingertips of linguists a very rich s