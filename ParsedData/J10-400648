ty reduction techniques tested on language data by Turney (2007) and Van de Cruys (2009) can be applied to the DM tensors before matricization.
    We present a pilot study in this direction in Section 6.5.
  
  
    In order to make our proposal concrete, we experiment with three different DM models, corresponding to different ways to construct the underlying weighted tuple structure (Section 3.1).
    All models are based on the natural idea of extracting word&#8211;link&#8211;word tuples from a dependency parse of a corpus, but this is not a requirement for DM: The links could for example be based on frequent n-grams as in Turney (2006b) and Baroni et al. (2010), or even on very different kinds of relation, such as co-occurring within the same document.
    The current models are trained on the concatenation of (1) the Web-derived ukWaC corpus,2 about 1.915 billion tokens (here and subsequently, counting only strings that are entirely made of alphabetic characters); (2) a mid-2009 dump of the English Wikip