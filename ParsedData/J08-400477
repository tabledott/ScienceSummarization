g coefficient like K calculated on individual boundaries would underestimate the degree of agreement, suggesting low agreement even among coders whose segmentations are mostly similar.
    A weighted coefficient of agreement like &#945; might produce values more in keeping with intuition, but we are not aware of any attempts at measuring agreement on segmentation using weighted coefficients.
    We see two main options.
    We suspect that the methods proposed by Krippendorff (1995) for measuring agreement on unitizing (see Section 4.3.2, subsequently) may be appropriate for the purpose of measuring agreement on discourse segmentation.
    A second option would be to measure agreement not on individual boundaries but on windows spanning several units, as done in the methods proposed to evaluate the performance of topic detection algorithms such as Pk (Beeferman, Berger, and Lafferty 1999) or WINDOWDIFF (Pevzner and Hearst 2002) (which are, however, raw agreement scores not corrected for chance).
    4.3.2 Uni