efers short sentences to long ones with the same semantic content, which favors conciseness, but sometimes selects bad n-grams to avoid a longer (but clearer) rendition.
    This an interesting problem not encountered in otherwise similar speech recognition models.
    We are currently investigating solutions to all of these problems in a highly experimental setting.
  
  
    Statistical methods give us a way to address a wide variety of knowledge gaps in generation.
    They even make it possible to load non-traditional duties onto a generator, such as word sense disambiguation for machine translation.
    For example, bei in Japanese may mean either American or rice, and sha may mean shrine or company.
    If for some reason, the analysis of beisha fails to resolve these ambiguities, the generator can pass them along in the lattice it builds, e.g.
    : In this case, the statistical model has a strong preference for the American company, which is nearly always the correct translation.7 Furthermore, our two