slation evaluation task submissions, (Ranking and Adequacy), are described in Sections 3 through 5 while the submission to the tunable metrics task, (Tuning), is described in Section 6.
  
  
    Whereas previous versions of Meteor simply strip punctuation characters prior to scoring, version 1.3 includes a new text normalizer intended specifically for translation evaluation.
    The normalizer first replicates the behavior of the tokenizer distributed with the Moses toolkit (Hoang et al., 2007), including handling of non-breaking prefixes.
    After tokenization, we add several rules for normalization, intended to reduce meaning-equivalent punctuation styles to common forms.
    The following two rules are particularly helpful: Consider the behavior of the Moses tokenizer and Meteor normalizers given a reference translation containing the phrase &#8220;U.S.-based organization&#8221;: Of these, only the Meteor 1.3 normalization allows metrics to match all of the following stylizations: While intended for Mete