ingle-best source for all languages.
    This advantage does not come simply from having more data.
    In fact, if we randomly sampled from the multi-source data until the training set size was equivalent to the size of the English data, then the results still hold (and in fact go up slightly for some languages).
    This suggests that even better transfer models can be produced by separately weighting each of the sources depending on the target language &#8211; either weighting by hand, if we know the language group of the target language, or automatically, if we do not.
    As previously mentioned, the latter has been explored in both S&#248;gaard (2011) and Cohen et al. (2011).
  
  
    We presented a simple, yet effective approach for projecting parsers from languages with labeled training data to languages without any labeled training data.
    Central to our approach is the idea of delexicalizing the models, which combined with a standardized part-of-speech tagset allows us to directly transfer models