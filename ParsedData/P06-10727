at 6o &#171; 0; note also the importance of picking the right point on the curve to stop.
    See Table 3 for performance of models selected across smoothing, initialization, starting, and stopping choices, in all six languages. values in [&#8722;1, 0.2] (see Eq.
    2).
    The same initializers Off0) and smoothing conditions were tested.
    Performance of supervised model selection among models trained at different &#948; values is plotted in Fig.
    1.
    When a model is selected across all conditions (3 initializers x 6 smoothing values x 7 &#948;s) using annotated development data, performance is notably better than the EM baseline using the same selection procedure (see Table 3, second column).
  
  
    The central idea of this paper is to gradually change (anneal) the bias &#948;.
    Early in learning, local dependencies are emphasized by setting &#948; &#171; 0.
    Then &#948; is iteratively increased and training repeated, using the last learned model to initialize.
    This idea bears a strong