 has been used to measure pairwise agreement among coders making category judgments (Carletta, 1996; Krippendorf, 1980; Siegel and Castellan, 1988).
    Thus, the observed user/agent interactions are modeled as a coder, and the ideal interactions as an expert coder. where ti is the sum of the frequencies in column i of M, and T is the sum of the frequencies in M (ti + .
    .
    .
    + tn).
    P(A), the actual agreement between the data and the key, is always computed from the confusion matrix M: Given the confusion matrices in Tables 3 and 4, P(E) = 0.079 for both agents.8 For Agent A, P(A) = 0.795 and frc = 0.777, while for Agent B, P(A) = 0.59 and c=0.555, suggesting that Agent A is more successful than B in achieving the task goals.
    As shown in Figure 1, performance is also a function of a combination of cost measures.
    Intuitively, cost measures should be calculated on the basis of any user or agent dialogue behaviors that should be minimized.
    A wide range of cost measures have been used in