t position.
    An ordering is imposed on the set of frame slots, such that inheritance decisions for slots higher in the order are conditioned on the decisions for slots lower in the order.
    The probability P(Y I X) is then the product of all 23 statistical models to additional linguistic phenomena such as quantification and anaphora resolution. decision probabilities: The discourse model is trained from a corpus annotated with both pre-discourse and post-discourse semantic frames.
    Corresponding pairs of input and output (X, Y) vectors are computed from these annotations, which are then used to train the 23 statistical decision trees.
    The training procedure for estimating these decision tree models is similar to that used for training the semantic interpretation model.
    Searching the discourse model begins by selecting a meaning frame Mp from the history stack H, and combining it with each pre-discourse meaning Ms received from the semantic interpretation model.
    This process yields a set of