
  Contextual Dependencies In Unsupervised Word Segmentation
  
    Developing better methods for segmenting continuous text into words is important for improving the processing of Asian languages, and may shed light on how humans learn to segment speech.
    We propose two new Bayesian word segmentation methods that assume unigram and bigram models of word dependencies respectively.
    The bigram model greatly outperforms the unigram model (and previous probabilistic models), demonstrating the importance of such dependencies for word segmentation.
    We also show that previous probabilistic models rely crucially on suboptimal search procedures.
  
  
    Word segmentation, i.e., discovering word boundaries in continuous text or speech, is of interest for both practical and theoretical reasons.
    It is the first step of processing orthographies without explicit word boundaries, such as Chinese.
    It is also one of the key problems that human language learners must solve as they are learning language.
  