bility of each word given a tag (lexical probabilities).
    Using a tagged corpus to train the model is called &amp;quot;supervised training,&amp;quot; since a human has prepared the correct training data.
    We conducted supervised training to derive both a bi-tag and a tri-tag model based on a corpus from the University of Pennsylvania, which was created as part of the TREEBANK project (Santorini 1990) consisting of Wall Street Journal (WSJ) articles, texts from the Library of America, transcribed radio broadcasts, and transcribed dialogues.
    The full TREEBANK consists of approximately 4 million words of text.
    Of the 47 parts of speech, 36 are word tags, and 11 are punctuation tags.
    Of the word tags, 22 are tags for open class words and 14 for closed class words.
    Each word or punctuation mark has been tagged, as shown in the following example, where NNS is plural noun; VBD is past tense verb; RB is adverbial; VBN is past participle verb.
    A bi-tag model predicts the relative likelihood o