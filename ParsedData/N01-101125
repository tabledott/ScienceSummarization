 during learning than decision trees; as a practical matter this means that the decision list is less likely to be over&#8212;trained.
    However, we believe that fragmentation also reflects on the feature set used for learning.
    Ours consists of at most approximately 100 binary features.
    This results in a relatively small feature space that is not as likely to suffer from fragmentation as are larger spaces.
  
  
    There are a number of immediate extensions to this work.
    The first is to ease the requirement that bigrams be made up of two consecutive words.
    Rather, we will search for bigrams where the component words may be separated by other words in the text.
    The second is to eliminate the filtering step by which candidate bigrams are selected by a power divergence statistic.
    Instead, the decision tree learner would consider all possible bigrams.
    Despite increasing the danger of fragmentation, this is an interesting issue since the bigrams judged most informative by the decisio