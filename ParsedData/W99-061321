e into the reals (h : 2x -4 R), where the sign4 of h(x) is interpreted as the predicted label and the magnitude I h(x)I is the confidence in the prediction: large numbers for I h(x)I indicate high confidence in the prediction, and numbers close to zero indicate low confidence.
    The weak hypothesis can abstain from predicting the label of an instance x by setting h(x) = 0.
    The final strong hypothesis, denoted 1(x), is then the sign of a weighted sum of the weak hypotheses, 1(x) = sign (Vii atht(x)), where the weights at are determined during the run of the algorithm, as we describe below.
    Pseudo-code describing the generalized boosting algorithm of Schapire and Singer is given in Figure 1.
    Note that Zt is a normalization constant that ensures the distribution Dt+i sums to 1; it is a function of the weak hypothesis ht and the weight for that hypothesis at chosen at the tth round.
    The normalization factor plays an important role in the AdaBoost algorithm.
    Schapire and Singer show that the 