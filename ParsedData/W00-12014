persed with some well-defined distribution in the right hand sides of a (context-free) rewriting system.
    The BBN model is also of the lexicalized PCFG variety.
    In the BBN model, as with Model 2 of (Collins, 1997), modifying nonterminals are generated conditioning both on the parent P and its head child H. Unlike Model 2 of (Collins, 1997), they are also generated conditioning on the previously generated modifying nonterminal, or and there is no subcat frame or distance feature.
    While the BBN model does not perform at the level of Model 2 of (Collins, 1997) on Wall Street Journal text, it is also less language-dependent, eschewing the distance metric (which relied on specific features of the English Treebank) in favor of the &amp;quot;bigrams on nonterminals&amp;quot; model.
    This section briefly describes the top-level parameters used in the BBN parsing model.
    We use p to denote the unlexicalized nonterminal corresponding to P in (1), and similarly for i, ri and h. We now present the toplev