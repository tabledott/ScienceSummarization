s possible when the cost of creating annotated train ing data is dramatically reduced?
			What new tasks should we try to solve if we do not limit ourselves to reusing existing training and test sets?
			Can complex annotation be done by untrained annotators?
			Howcan we ensure high quality annotations from crowd sourced contributors?To begin addressing these questions, we orga nized an open-ended $100 shared task.
			Researchers were given $100 of credit on Amazon MechanicalTurk to spend on an annotation task of their choosing.
			They were required to write a short paper de scribing their experience, and to distribute the datathat they created.
			They were encouraged to ad dress the following questions: How did you conveythe task in terms that were simple enough for non experts to understand?
			Were non-experts as good as experts?
			What did you do to ensure quality?
			How quickly did the data get annotated?
			What is the cost per label?
			Researchers submitted a 1 page proposalto the workshop organi