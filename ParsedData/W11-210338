atistical machine translation system.
    We evaluated whether the system tuned on their metrics produced higher-quality output than the baseline system that was tuned to BLEU, as is typically done.
    In order to evaluate whether the quality was better, we conducted a manual evaluation, in the same fashion that we evaluate the different MT systems submitted to the shared translation task.
    We provide the participants with a fixed MT system for Urdu-English, along with a small parallel set to be used for tuning.
    Specifically, we provide developers with the following components: We provided the metrics developers with Omar Zaidan&#8217;s Z-MERT software (Zaidan, 2009), which implements Och (2003)&#8217;s minimum error rate training procedure.
    Z-MERT is designed to be modular with respect to the objective function, and allows BLEU to be easily replaced with other automatic evaluation metrics.
    Metric developers incorporated their metrics into Z-MERT by subclassing the EvaluationMetric.java abstra