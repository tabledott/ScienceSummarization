the language model trained on a subset of the generaldomain corpus was restricted to only cover those tokens found in the in-domain corpus, following Moore and Lewis (2010).
  
  
    The baseline results show that a translation system trained on the general-domain corpus outperforms a system trained on the in-domain corpus by over 3 BLEU points.
    However, this can be improved further.
    We used the three methods from Section 4 to identify the best-scoring sentences in the generaldomain corpus.
    We consider three methods for extracting domaintargeted parallel data from a general corpus: sourceside cross-entropy (Cross-Ent), source-side crossentropy difference (Moore-Lewis) from (Moore and Lewis, 2010), and bilingual cross-entropy difference (bML), which is novel.
    Regardless of method, the overall procedure is the same.
    Using the scoring method, We rank the individual sentences of the general-domain corpus, select only the top N. We used the top N = {35k, 70k, 150k} sentence pairs out of the 12