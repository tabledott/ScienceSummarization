ms to demonstrate the competitiveness of MaxEnt-based reordering: Our experiments were made on two Chinese-toEnglish translation tasks: NIST MT-05 (news domain) and IWSLT-04 (travel dialogue domain).
    NIST MT-05.
    In this task, the bilingual training data comes from the FBIS corpus with 7.06M Chinese words and 9.15M English words.
    The trigram language model training data consists of English texts mostly derived from the English side of the UN corpus (catalog number LDC2004E12), which totally contains 81M English words.
    For the efficiency of minimum error rate training, we built our development set using sentences of length at most 50 characters from the NIST MT-02 evaluation test data.
    IWSLT-04.
    For this task, our experiments were carried out on the small data track.
    Both the bilingual training data and the trigram language model training data are restricted to the supplied corpus, which contains 20k sentences, 179k Chinese words and 157k English words.
    We used the CSTAR 2003 tes