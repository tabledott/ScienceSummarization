arser uses a history-based feature model for predicting the next parsing action.
			Each feature of this model is an attribute of a token defined relative to the current stack S, input queue I, or partially built dependency graph G, where the attribute can be anyof the symbolic input attributes in the CoNLL for mat: FORM, LEMMA, CPOSTAG, POSTAG and FEATS (split into atomic attributes), as well as the DEPREL attribute of tokens in the graph G. The baseline feature model is depicted in figure 1, where rows denote tokens, columns denote attributes, and each cell containing a plus sign represents a model feature.7 This model is an extrapolation from many previous experiments on different languages and usually represents a good starting point for further optimization.
			The baseline model was tuned for each of the ten languages using both forward and backward feature 6In fact, for Arabic, which has about 10% sentences with non-projective dependencies, it was later found that, with anoptimized feature model, it is