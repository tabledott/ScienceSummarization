&#945; = 0.2 for the reconstruction error prevents overfitting and achieves the highest performance.
  
  
    Autoencoders are neural networks that learn a reduced dimensional representation of fixed-size inputs such as image patches or bag-of-word representations of text documents.
    They can be used to efficiently learn feature encodings which are useful for classification.
    Recently, Mirowski et al. (2010) learn dynamic autoencoders for documents in a bagof-words format which, like ours, combine supervised and reconstruction objectives.
    The idea of applying an autoencoder in a recursive setting was introduced by Pollack (1990).
    Pollack&#8217;s recursive auto-associative memories (RAAMs) are similar to ours in that they are a connectionst, feedforward model.
    However, RAAMs learn vector representations only for fixed recursive data structures, whereas our RAE builds this recursive data structure.
    More recently, (Voegtlin and Dominey, 2005) introduced a linear modification to RAAMs that 