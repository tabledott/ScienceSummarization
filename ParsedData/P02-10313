 systems evaluated for the Message Understanding Conference task, Miller et al. (1998) made use of an integrated syntactic and semantic model producing a full parse tree, and achieved results comparable to other systems that did not make use of a complete parse.
    As in the FrameNet case, the parser was not trained on the corpus for which semantic annotations were available, and the effect of better, or even perfect, parses could not be measured.
    One of the differences between the two semantic annotation projects is that the sentences chosen for annotation for Propbank are from the same Wall Street Journal corpus chosen for annotation for the original Penn Treebank project, and thus hand-checked syntactic parse trees are available for the entire dataset.
    In this paper, we compare the performance of a system based on goldstandard parses with one using automatically generated parser output.
    We also examine whether it is possible that the additional information contained in a full parse tree is neg