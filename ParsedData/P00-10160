
  Rule Writing Or Annotation: Cost-Efficient Resource Usage For Base Noun Phrase Chunking
  
    This paper presents a comprehensive empirical comparison between two approaches for developing a base noun phrase chunker: human rule writing and active learning using interactive realtime human annotation.
    Several novel variations on active learning are investigated, and underlying cost models for cross-modal machine learning comparison are presented and explored.
    Results show that it is more efficient and more successful by several measures to train a system using active learning annotation rather than hand-crafted rule writing at a comparable level of human labor investment.
  
  
    One of the primary problems that NLP researchers who work in new languages or new domains encounter is a lack of available annotated data.
    Collection of data is neither easy nor cheap.
    The construction of the Penn Treebank significantly improved performance for English systems dealing in the &amp;quot;traditional&