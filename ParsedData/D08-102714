annotations by averaging the numeric responses from each possible subset of n annotators and computing the interannotator agreement with respect to the gold scores reported in (Miller and Charles, 1991).
    Our results are displayed in Figure 2, with Resnik&#8217;s 0.958 correlation plotted as the horizontal line; we find that at 10 annotators we achieve a correlation of 0.952, well within the range of other studies of expert and non-expert annotations.
    This task replicates the recognizing textual entailment task originally proposed in the PASCAL Recognizing Textual Entailment task (Dagan et al., 2006); here for each question the annotator is presented with two sentences and given a binary choice of whether the second hypothesis sentence can be inferred from the first.
    For example, the hypothesis sentence &#8220;Oil prices drop&#8221; would constitute a true entailment from the text &#8220;Crude Oil Prices Slump&#8221;, but a false entailment from &#8220;The government announced last week that it pla