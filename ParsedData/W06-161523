on is an important and wellstudied area in natural language processing.
    Here we outline a few recent advances.
    Roark and Bacchiani (2003) use a Dirichlet prior on the multinomial parameters of a generative parsing model to combine a large amount of training data from a source corpus (WSJ), and small amount of training data from a target corpus (Brown).
    Aside from Florian et al. (2004), several authors have also given techniques for adapting classification to new domains.
    Chelba and Acero (2004) first train a classifier on the source data.
    Then they use maximum a posteriori estimation of the weights of a maximum entropy target domain classifier.
    The prior is Gaussian with mean equal to the weights of the source domain classifier.
    Daum&#180;e III and Marcu (2006) use an empirical Bayes model to estimate a latent variable model grouping instances into domain-specific or common across both domains.
    They also jointly estimate the parameters of the common classification model and the