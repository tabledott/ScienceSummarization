hitespace characters, and the English sentences have 40,629,997 words.
    Our main source for training our five-gram language model was the English Gigaword corpus, and we also included close to one million English sentences taken from LDC parallel texts: GALE Year 1 training data (excluding FOUO data), Sinorama, AsiaNet, and Hong Kong news.
    We restricted the Gigaword corpus to a subsample of 25 million sentences, because of memory constraints.
  
  
    In this section, we experiment with three types of segmenters &#8211; character-based, lexicon-based and feature-based &#8211; to explore what kind of characteristics are useful for segmentation for MT.
    The training data for the segmenter is two orders of magnitude smaller than for the MT system, it is not terribly well matched to it in terms of genre and variety, and the information an MT system learns about alignment of Chinese to English might be the basis for a task appropriate segmentation style for Chinese-English MT.
    A phrase-based MT syst