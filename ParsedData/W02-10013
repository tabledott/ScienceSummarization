oted perceptron to NLP problems.
			1
	
	
			2.1 HMM Taggers.
			In this section, as a motivating example, we de scribe a special case of the algorithm in thispaper: the algorithm applied to a trigram tag ger.
			In a trigram HMM tagger, each trigram 1The theorems in section 3, and the proofs in sec tion 5, apply directly to the work in these other papers.
			Association for Computational Linguistics.
			Language Processing (EMNLP), Philadelphia, July 2002, pp.
			1-8.
			Proceedings of the Conference on Empirical Methods in Natural of tags and each tag/word pair have associated parameters.
			We write the parameter associated with a trigram hx; y; zi as  x;y;z, and the param eter associated with a tag/word pair (t; w) as  t;w. A common approach is to take the param eters to be estimates of conditional probabilities:  x;y;z = logP (z j x; y),  t;w = logP (w j t).
			For convenience we will use w [1:n]as short hand for a sequence of words [w 1 ; w 2 : : : w n ], and t [1:n] as shorthand for a taq sequence [t 1