It seems intuitive that a model with a smaller number of parameters will require fewer samples for convergence, but this is not necessarily the case, and at present this intuition lacks a theoretical basis.
    Feature selection methods can probably be motivated either from a Bayesian perspective (through a prior favoring models with a smaller number of nonzero parameters) or from a frequentist/goodness-of-fit perspective (models with fewer parameters are less likely to fit the data by chance), but this requires additional research.
    The statistical justification for boosting approaches is quite different.
    Boosting algorithms were originally developed within the PAC framework (Valiant 1984) for machine learning, specifically to address questions regarding the equivalence of weak and strong learning.
    Freund and Schapire (1997) originally introduced AdaBoost and gave a first set of statistical guarantees for the algorithm.
    Schapire et al. (1998) gave a second set of guarantees based on the analys