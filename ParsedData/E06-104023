se will still worsen corpussimilarity scores.
    Canvassing expert opinion of text quality and averaging the results is also in a sense frequencybased, as results reflect what the majority of experts consider good variants.
    Expert opinions can vary considerably, as shown by the low correlation among experts in our study (and as seen in corpus studies, e.g.
    Reiter et al., 2005), and evaluations by a small number of experts may also be problematic, unless we have good reason to believe that expert opinions are highly correlated in the domain (which was certainly not the case in our weather forecast domain).
    Ultimately, such disagreement between experts suggests that (intrinsic) judgments of the text quality &#8212; whether by human or metric &#8212; really should be be backed up by (extrinsic) judgments of the effectiveness of a text in helping real users perform tasks or otherwise achieving its communicative goal.
  
  
    We plan to further investigate the performance of automatic evaluation mea