
  Integrating Graph-Based and Transition-Based Dependency Parsers
  
    Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference.
    In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model.
    By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.
  
  
    Syntactic dependency graphs have recently gained a wide interest in the natural language processing community and have been used for many problems ranging from machine translation (Ding and Palmer, 2004) to ontology construction (Snow et al., 2005).
    A dependency graph for a sentence represents each word and its syntactic dependents through labeled directed arcs, a