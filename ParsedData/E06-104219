e whole group into a single feature set.
  
  
    TroFi was evaluated on the 25 target words listed in Table 1.
    The target sets contain from 1 to 115 manually annotated sentences for each verb.
    The first round of annotations was done by the first annotator.
    The second annotator was given no instructions besides a few examples of literal and nonliteral usage (not covering all target verbs).
    The authors of this paper were the annotators.
    Our inter-annotator agreement on the annotations used as test data in the experiments in this paper is quite high. n (Cohen) and n (S&amp;C) on a random sample of 200 annotated examples annotated by two different annotators was found to be 0.77.
    As per ((Di Eugenio &amp; Glass, 2004), cf. refs therein), the standard assessment for n values is that tentative conclusions on agreement exists when .67 &#8804; n &lt; .8, and a definite conclusion on agreement exists when n &#8805; .8.
    In the case of a larger scale annotation effort, having the person lea