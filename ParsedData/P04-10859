lap of spurts.
    Note that with only the first feature listed in the table, the maximum entropy ranker matches exactly the performance of the baseline algorithm (79.8% accuracy).
    Regarding lexical features, we used a countbased feature selection algorithm to remove many first-word and last-word features that occur infrequently and that are typically uninformative for the task at hand.
    Remaining features essentially contained function words, in particular sentence-initial indicators of questions (&#8220;where&#8221;, &#8220;when&#8221;, and so on).
    Note that all features in Table 1 are &#8220;backwardlooking&#8221;, in the sense that they result from an analysis of context preceding B.
    For many of them, we built equivalent &#8220;forward-looking&#8221; features that pertain to the closest utterance of the potential speaker A that follows part B.
    The motivation for extracting these features is that speaker A is generally expected to react if he or she is addressed, and thus, to take the fl