ations between target items (the matrix rows) and their contexts (the matrix columns).
    This results in the development of ad hoc models that lose sight of the fact that different semantic spaces actually rely on the same kind of underlying distributional information.
    DM instead represents corpus-extracted co-occurrences as a third-order tensor, a ternary geometrical object that models distributional data in terms of word&#8211; link&#8211;word tuples.
    Matrices are then generated from the tensor in order to perform semantic tasks in the spaces they define.
    Crucially, these on-demand matrices are derived from the same underlying resource (the tensor) and correspond to different &#8220;views&#8221; of the same data, extracted once and for all from a corpus.
    DM is tested here on what we believe to be the most varied array of semantic tasks ever addressed by a single distributional model.
    In all cases, we compare the performance of several DM implementations to state-of-the-art results.
   