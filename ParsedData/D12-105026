bles 4 (accuracy) and 5 (F1) show our results on the test set with the best feature combinations for each model (shown in parentheses).
    Each row corresponds to a different type of composition and each column to a different word representation model.
    As can be seen, the distributional memory (DM) is the best performing representation for the additive composition model.
    The neural language model (NLM) gives best results for the recursive autoencoder (RAE), although the other two representations come close.
    And finally the simple distributional semantic space (SDS) works best with multiplication.
    Also note that the best performing models, namely DM with addition and SDS with multiplication, use a basic feature space consisting only of the cosine similarity of the composed sentence vectors, the length of the two sentences involved, and their unigram word overlap.
    Although our intention was to use the paraphrase detection task as a test-bed for evaluating compositional models rather than ac