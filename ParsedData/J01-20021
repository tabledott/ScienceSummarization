xploited to yield a higher accuracy than the best individual system.
    We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora.
    Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data.
    After comparison, their outputs are combined using several voting strategies and second-stage classifiers.
    All combination taggers outperform their best component.
    The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus.
  
  
    In all natural language processing (NLP) systems, we find one or more language models that are used to predict, classify, or interpret language-related observations.
    Because most real-world NLP tasks require something that approaches full language understanding in order to be perfect, but automatic systems only have access to limited (and often supe