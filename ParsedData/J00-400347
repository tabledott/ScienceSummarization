ways agree either on the classification of a definite description or on its anchor raises the question of how to evaluate the performance of our system.
    We tried two different approaches: evaluating the performance of the system by measuring its precision and recall against a standardized annotation based on majority voting (as done in MUC), and measuring the extent of the system's agreement with the rest of the annotators by means of the same metric used to measure agreement among the annotators themselves (the kappa statistic).
    We used the first form of evaluation to measure both the performance of the single heuristics and the performance of the system as a whole; the agreement measure was only used to measure the overall performance of the system.
    We discuss each of these in turn.&amp;quot; 5.1.1 Precision and Recall.
    Recall and precision are measures commonly used in Information Retrieval to evaluate a system's performance.
    Recall is the percentage of correct answers reported by the s