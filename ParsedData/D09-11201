onstraints like the binding theory, the i-within-i filter, and appositive constructions restrict reference by configuration.
    Semantic constraints like selectional compatibility (e.g. a spokesperson can announce things) and subsumption (e.g.
    Microsoft is a company) rule out many possible referents.
    Finally, discourse phenomena such as salience and centering theory are assumed to heavily influence reference preferences.
    As these varied factors have given rise to a multitude of weak features, recent work has focused on how best to learn to combine them using models over reference structures (Culotta et al., 2007; Denis and Baldridge, 2007; Klenner and Ailloud, 2007).
    In this work, we break from the standard view.
    Instead, we consider a vastly more modular system in which coreference is predicted from a deterministic function of a few rich features.
    In particular, we assume a three-step process.
    First, a selfcontained syntactic module carefully represents syntactic structures using