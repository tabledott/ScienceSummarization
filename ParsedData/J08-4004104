 is more towards usefulness.
    There is also a tradeoff between the sophistication of judgments and the availability of coders who can make such judgments.
    Consequently, annotation by experts is often the only practical way to get useful corpora for CL.
    Current practice achieves high reliability either by using professionals (Kilgarriff 1999) or through intensive training (Hovy et al. 2006; Carlson, Marcu, and Okurowski 2003); this means that results are not replicable across sites, and are therefore less reliable than annotation by naive coders adhering to written instructions.
    We feel that inter-annotator agreement studies should still be carried out, as they serve as an assurance that the results are replicable when the annotators are chosen from the same population as the original annotators.
    An important additional assurance should be provided in the form of an independent evaluation of the task for which the corpus is used (cf.
    Passonneau 2006).
    One of the goals of this article