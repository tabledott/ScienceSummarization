ated as a hidden variable during training, following the approach of Zettlemoyer &amp; Collins (2005, 2007).
    We present an algorithm that incrementally learns the parameters of this model while simultaneously exploring the space of possible grammars.
    The model is used to guide the process of grammar refinement during training as well as providing a metric for selecting the best analysis for each new sentence.
    We evaluate the approach on benchmark datasets from a natural language interface to a database of US Geography (Zelle &amp; Mooney, 1996).
    We show that accurate models can be learned for multiple languages with both the variable-free and lambdacalculus meaning representations introduced above.
    We also compare performance to previous methods (Kate &amp; Mooney, 2006; Wong &amp; Mooney, 2006, 2007; Zettlemoyer &amp; Collins, 2005, 2007; Lu et al., 2008), which are designed with either language- or representation- specific constraints that limit generalization, as discussed in more detai