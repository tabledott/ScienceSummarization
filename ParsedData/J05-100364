ds to oversmoothing (undertraining).
    The graph shows consistently lower performance than that for &amp; = 0.0025.
    The bottom graph shows that there is little difference in performance for &amp; = 0.001 versus &amp; = 0.0025.
    This is a measure of the number of updates to the W&#254;k and Wk variables required in making a pass over the entire training set.
    Thus it is a measure of the amount of computation that the naive algorithm for ExpLoss, presented in Figure 3, requires for each round of feature selection.
    Next, say the improved algorithm in Figure 4 selects feature k* on the t th round of feature selection.
    Then we define the following quantity: We are now in a position to compare the running times of the two algorithms.
    We define the following quantities: Here, Work(n) is the computation required for n rounds of feature selection, where a single unit of computation corresponds to a pass over the entire training set.
    Savings(n) tracks the relative efficiency of the two algor