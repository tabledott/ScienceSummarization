d Isahara (2001; hereafter U&amp;I).
    As in our work, Utiyama and Isahara propose a probabilistic framework based on maximizing the compactness of the language models induced for each segment.
    Their likelihood equation is identical to our equations 3-5.
    They then define the language models for each segment as &#65533;Bj,i = nj,iW1 , without rigorous justifiW+Ei nj,i cation.
    This form is equivalent to Laplacian smoothing (Manning and Sch&#168;utze, 1999), and is a special case of our equation 2, with B0 = 1.
    Thus, the language models in U&amp;I can be viewed as the expectation of the posterior distribution p(Bj|{xt : zt = j}, B0), in the special case that B0 = 1.
    Our approach generalizes U&amp;I and provides a Bayesian justification for the language models that they apply.
    The remainder of the paper further extends this work by marginalizing out the language model, and by adding cue phrases.
    We empirically demonstrate that these extensions substantially improve performance.
    O