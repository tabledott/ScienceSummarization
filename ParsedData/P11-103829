ur method outperforms the noisy channel method by about 7% and 12% in F-score over SMS and Twitter corpora, respectively.
    This can be explained as follows.
    First, the Cook and Stevenson (2009) method is typebased, so all token instances of a given ill-formed word will be normalised identically.
    In the Twitter data, however, the same word can be normalised differently depending on context, e.g. hw &#8220;how&#8221; in so hw many time remaining so I can calculate it? vs. hw &#8220;homework&#8221; in I need to finish my hw first.
    Second, the noisy channel method was developed specifically for SMS normalisation, in which clipping is the most prevalent form of lexical variation, while in the Twitter data, we commonly have instances of word lengthening for emphasis, such as moviiie &#8220;movie&#8221;.
    Having said this, our method is superior to the noisy channel method over both the SMS and Twitter data.
    The SMT approach is relatively stable on the two datasets, but well below the performan