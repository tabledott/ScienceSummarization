tely, as models become more complex, for example through the addition of new features or components, dynamic programming algorithms can quickly explode in terms of computational or implementational complexity.1 As a result, efficiency of inference is a critical bottleneck for many problems in statistical NLP.
    This paper introduces dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007) as a framework for deriving inference algorithms in NLP.
    Dual decomposition leverages the observation that complex inference problems can often be decomposed into efficiently solvable sub-problems.
    The approach leads to inference algorithms with the following properties: The approach is very general, and should be applicable to a wide range of problems in NLP.
    The connection to linear programming ensures that the algorithms provide a certificate of optimality when they recover the exact solution, and also opens up the possibility of methods that incrementally tighten the LP relaxation until it is ex