nderson, 2001) to estimate the parameters of the probability models.
    SSNs have the advantage that they avoid the need to impose hand-crafted independence assumptions on the learning process.
    Training an SSN simultaneously trains a finite representations of the unbounded parse history and a mapping from this history representation to the parameter estimates.
    The history representations are automatically tuned to optimize the parameter estimates.
    This avoids the problem that any choice of hand-crafted independence assumptions may bias our results towards one approach or another.
    The independence assumptions would have to be different for the generative and discriminative probability models, and even for the parsers which use the generative probability model, the same set of independence assumptions may be more appropriate for maximizing one training criteria over another.
    By inducing the history representations specifically to fit the chosen model and training criteria, we avoid having t