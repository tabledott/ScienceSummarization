e systems (Table 3) is at about the same level as their agreement with the benchmark tagging.
    A more detailed view of intertagger agreement is shown in Table 4, which lists the (groups of) patterns of (dis)agreement for the four data sets.
    It is interesting to see that although the general accuracy for WSJ is lower than for LOB, the intertagger agreement for WSJ is on average higher.
    It would seem that the less consistent tagging for WSJ makes it easier for all systems to fall into the same traps.
    This becomes even clearer when we examine the patterns of agreement and see, for example, that the number of tokens where all taggers agree on a wrong tag is practically doubled.
    The agreement pattern distribution enables us to determine levels of combination quality.
    Table 5 lists both the accuracies of several ideal combiners (`)/0) and the error reduction in relation to the best base tagger for the data set in question (A Err).22 For example, on LOB, &amp;quot;All ties correct&amp;quot; pr