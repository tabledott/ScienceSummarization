ies.
  
  
    A number of projects in the past have looked into ways of extending and improving the Bleu metric.
    Doddington (2002) suggested changing Bleu&#8217;s weighted geometric average of n-gram matches to an arithmetic average, and calculating the brevity penalty in a slightly different manner.
    Hovy and Ravichandra (2003) suggested increasing Bleu&#8217;s sensitivity to inappropriate phrase movement by matching part-of-speech tag sequences against reference translations in addition to Bleu&#8217;s n-gram matches.
    Babych and Hartley (2004) extend Bleu by adding frequency weighting to lexical items through TF/IDF as a way of placing greater emphasis on content-bearing words and phrases.
    Two alternative automatic translation evaluation metrics do a much better job at incorporating recall than Bleu does.
    Melamed et al. (2003) formulate a metric which measures translation accuracy in terms of precision and recall directly rather than precision and a brevity penalty.
    Banerjee and Lavi