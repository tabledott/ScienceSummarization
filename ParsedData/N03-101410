cussed above.
    In addition the SSN has a hidden layer, which computes a finite vector of real valued features from a sequence of inputs specifying the derivation history .
    This hidden layer vector is the history representation .
    It is analogous to the hidden state of a Hidden Markov Model (HMM), in that it represents the state of the underlying generative process and in that it is not explicitly specified in the output of the generative process.
    The mapping from the derivation history to the history representation is computed with the recursive application of a function .
    As will be discussed in the next section, maps previous history representations plus pre-defined features of the derivation history to a real-valued vector .
    Because the function is nonlinear, the induction of this history representation allows the training process to explore a much more general set of estimators than would be possible with a log-linear model alone (i.e.
    ).1 This generality makes this estimation me