 both syntactic and semantic aspects of English is quite surprising given that they were constructed from nothing more than counts of bigrams.
    The class {that tha theat} is interesting because although tha and theat are not English words, the computer has discovered that in our data each of them is most often a mistyped that.
    Table 4 shows the number of class 1-, 2-, and 3-grams occurring in the text with various frequencies.
    We can expect from these data that maximum likelihood estimates will assign a probability of 0 to about 3.8 percent of the class 3-grams and to about .02 percent of the class 2-grams in a new sample of English text.
    This is a substantial improvement over the corresponding numbers for a 3-gram language model, which are 14.7 percent for word 3-grams and 2.2 percent for word 2-grams, but we have achieved this at the expense of precision in the model.
    With a class model, we distinguish between two different words of the same class only according to their relative frequenc