mproves lexicon recall, as a function of p#, with &#945;0 = 20 and (b) as a function of &#945;0, with p# = .5. but begins to degrade precision after a point.
    Due to the negative correlation between token accuracy and lexicon accuracy, there is no single best value for either p# or &#945;0; further discussion refers to the solution for p# = .5, &#945;0 = 20 (though others are qualitatively similar).
    In Table 1(a), we compare the results of our system to those of MBDP and NGS.4 Although our system has higher lexicon accuracy than the others, its token accuracy is much worse.
    This result occurs because our system often mis-analyzes frequently occurring words.
    In particular, many of these words occur in common collocations such as what&#8217;s that and do you, which the system interprets as a single words.
    It turns out that a full 31% of the proposed lexicon and nearly 30% of tokens consist of these kinds of errors.
    Upon reflection, it is not surprising that a unigram language model would 