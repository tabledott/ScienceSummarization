sists of chunk pairs aligned at sentence and sub-sentence level (Deng et al, 2004).
			In A-E, the training bitext consists of 3.8M English words, 3.2M Arabic words and 137K chunk pairs.
			In C-E, the training bitext consists of 11.7M English words, 8.9M Chinese words and 674K chunk pairs.Our Chinese text processing consists of word seg mentation (using the LDC segmenter) followed bygrouping of numbers.
			For Arabic our text pro cessing consisted of a modified Buckwalter analysis(LDC2002L49) followed by post processing to sep arate conjunctions, prepostions and pronouns, andAl-/w- deletion.
			The English text is processed us ing a simple tokenizer based on the text processing utility available in the the NIST MT-eval toolkit.
			The Language Model (LM) training data consistsof approximately 400M words of English text de rived from Xinhua and AFP (English Gigaword), the English side of FBIS, the UN and A-E News texts, and the online archives of The People?s Daily.
			Table 5 gives the performance of the MJ-