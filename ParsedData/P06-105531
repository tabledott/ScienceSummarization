to extremely accurate (an Fl of 90.2% for our largest grammar with only 1043 symbols).
    Splitting provides a tight fit to the training data, while merging improves generalization and controls grammar size.
    In order to overcome data fragmentation and overfitting, we smooth our parameters.
    Smoothing allows us to add a larger number of annotations, each specializing in only a fraction of the data, without overfitting our training set.
    As one can see in Table 4, the resulting parser ranks among the best lexicalized parsers, beating those of Collins (1999) and Charniak and Johnson (2005).8 Its Fl performance is a 27% reduction in error over Matsuzaki et al. (2005) and Klein and Manning (2003).
    Not only is our parser more accurate, but the learned grammar is also significantly smaller than that of previous work.
    While this all is accomplished with only automatic learning, the resulting grammar is human-interpretable.
    It shows most of the manually introduced annotations discussed by Klein 