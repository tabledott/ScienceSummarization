f y* contains at least one occurrence of w, and &#952;n is the weight indicating the relative importance of an n-gram match.
    If the hypergraph is already annotated with n-gram (n &gt; 4) language model states, this loss function is additively def decomposable.
    Using re = Le where Le is the loss for a hyperedge e, we compute the expected loss, With second-order expectation semirings, we can compute from a hypergraph the expectation and variance of hypothesis length; the feature expectation vector and covariance matrix; the Hessian (matrix of second derivatives) of Z; and the gradients of entropy and expected loss.
    The computations should be clear from earlier discussion.
    Below we compute gradient of entropy or Bayes risk.
    Gradient of Entropy or Risk It is easy to see that the gradient of entropy (5) is We may compute (Z, r, VZ, Vr) as explained in Case 3 of Section 5 by using defdef ke = (pe,pere,Vpe, (Vpe)re + peVre) = (pe,pe log pe, Vpe, (1 + log pe)Vpe), where Vpe depends on the particul