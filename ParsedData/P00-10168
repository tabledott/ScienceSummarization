 someone who could annotate with perfect consistency to the gold standard corpus annotation conventions.
    Because our goal is to investigate the relative costs of rule writing versus annotation, it is essential that we use a realistic model of annotation.
    Therefore, we decided to do a fully-fledged active learning annotation experiment, with real time human supervision, rather than assume the simulated feedback of actual Treebank annotators.
    We developed an annotation tool that is modeled on MITRE's Alembic Workbench software (Day et al., 1997), but written in Java for platform-independence.
    To enable data storage and the active learning sample selection to take place on the more powerful machines in our lab rather than the user's home machine, the tool was designed with network support so that it could communicate with our servers over the internet.
    Our real-time active learning experiment subjects were seven graduate students in computer science.
    Five of them are native English speake