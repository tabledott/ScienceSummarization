mum likelihood estimate to hopefully produce more accurate probabilities.
    As an example, one simple smoothing technique is to pretend each bigram occurs once more than it actually did (Lidstone, 1920; Johnson, 1932; Jeffreys, 1948), yielding where V is the vocabulary, the set of all words being considered.
    This has the desirable quality of 'To make the term P(wilw:=41) meaningful for i &lt; n, one can pad the beginning of the string with a distinguished token.
    In this work, we assume there are n &#8212;1 such distinguished tokens preceding each sentence. preventing zero bigram probabilities.
    However, this scheme has the flaw of assigning the same probability to say, burnish the and burnish thou (assuming neither occurred in the training data), even though intuitively the former seems more likely because the word the is much more common than thou.
    To address this, another smoothing technique is to interpolate the bigram model with a unigram model PmL(wi) = c(w)/N5, a model that reflects how