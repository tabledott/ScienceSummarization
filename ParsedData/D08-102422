IST 2008 evaluation (152+175 million words of Arabic+English), aligned by IBM Model 4 using GIZA++ (union of both directions).
    Hierarchical rules were extracted from the most in-domain corpora (4.2+5.4 million words) and phrases were extracted from the remainder.
    We trained the coarse-grained distortion model on 10,000 sentences of the training data.
    Two language models were trained, one on data similar to the English side of the parallel text and one on 2 billion words of English.
    Both were 5gram models with modified Kneser-Ney smoothing, lossily compressed using a perfect-hashing scheme similar to that of Talbot and Brants (2008) but using minimal perfect hashing (Botelho et al., 2005).
    We partitioned the documents of the NIST 2004 (newswire) and 2005 Arabic-English evaluation data into a tuning set (1178 sentences) and a development set (1298 sentences).
    The test data was the NIST 2006 Arabic-English evaluation data (NIST part, newswire and newsgroups, 1529 sentences).
    To obtain