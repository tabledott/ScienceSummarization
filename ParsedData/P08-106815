ween baseline or cluster-based feature sets, first-order (Eisner, 2000) or second-order (Carreras, 2007) factorizations, and labeled or unlabeled parsing.
    Table 2 compiles our final test results and also includes two results from previous work by McDonald et al. (2005a) and McDonald and Pereira (2006), for the purposes of comparison.
    We note a few small differences between our parsers and the 12Due to the sparsity of the perceptron updates, however, only a small fraction of the possible features were active in our trained models.
    13A punctuation token is any token whose gold-standard part of speech tag is one of {&#8216;&#8216; &#8217;&#8217; .1. parsers evaluated in this previous work.
    First, the MD1 and MD2 parsers were trained via the MIRA algorithm (Crammer and Singer, 2003; Crammer et al., 2004), while we use the averaged perceptron.
    In addition, the MD2 model uses only sibling interactions, whereas the dep2/dep2c parsers include both sibling and grandparent interactions.
    There ar