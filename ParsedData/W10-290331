not correspond directly to a grounded world state.
    Our learning framework closely follows recent work on learning from indirect supervision.
    The direct approach resembles learning a binary classifier over a latent structure (Chang et al., 2010a); while the aggressive approach has similarities with work that uses labeled structures and a binary signal indicating the existence of good structures to improve structured prediction (Chang et al., 2010b).
  
  
    In this paper we tackle one of the key bottlenecks in semantic parsing &#8212; providing sufficient supervision to train a semantic parser.
    Our solution is two fold, first we present a new training paradigm for semantic parsing that relies on natural, human level supervision.
    Second, we suggest a new model for semantic interpretation that does not rely on NL syntactic parsing rules, but rather uses the syntactic information to bias the interpretation process.
    This approach allows the model to generalize better and reduce the required a