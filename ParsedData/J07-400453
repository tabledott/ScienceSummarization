i; however, grouping the features into classes and using a different &#963; for each class is worth investigating and may improve the results.
    Optimization of the objective function, whether using iterative scaling or more general numerical optimization methods, requires calculation of the gradient of the objective function at each iteration.
    The components of the gradient vector are as follows: The first two terms are expectations of feature fi: the second expectation is over all derivations for each sentence in the training data, and the first is over only the derivations leading to the gold-standard dependency structure for each sentence.
    The estimation process attempts to make the expectations in Equation (17) equal (ignoring the Gaussian prior term).
    Another way to think of the estimation process is that it attempts to put as much mass as possible on the derivations leading to the goldstandard structures (Riezler et al. 2002).
    The Gaussian prior term prevents overfitting by penalizing