ceed more monotonically than the other methods.
    We quantified PRO&#8217;s stability as compared to MERT by repeating the Urdu-English baseline PBMT experiment five times with each configuration.
    The tune and test BLEU at each iteration is depicted in Figure 6.
    The standard deviation of the final test BLEU of MERT was 0.13 across the five experiment instances, while PRO had a standard deviation of just 0.05.
  
  
    Several works (Shen et al., 2004; Cowan et al., 2006; Watanabe et al., 2006) have used discriminative techniques to re-rank k-best lists for MT.
    Tillmann and Zhang (2005) used a customized form of multi-class stochastic gradient descent to learn feature weights for an MT model.
    Och and Ney (2002) used maximum entropy to tune feature weights but did not compare pairs of derivations.
    Ittycheriah and Roukos (2005) used a maximum entropy classifier to train an alignment model using hand-labeled data.
    Xiong et al. (2006) also used a maximum entropy classifier, in this case 