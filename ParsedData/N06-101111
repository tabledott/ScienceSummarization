far that are active in the input.
    As the iterative algorithm observes more data, it discovers and makes use of more features.
    This model is called the infinite attribute model (Blum, 1992) and it follows the perceptron version in SNoW (Roth, 1998).
    Positive examples used for iterative training are pairs of NEs and their best temporally aligned (thresholded) transliteration candidates.
    Negative examples are English non-NEs paired with random Russian words.
  
  
    We ran experiments using a bilingual comparable English-Russian news corpus we built by crawling a Russian news web site (www.lenta.ru).
    The site provides loose translations of (and pointers to) the original English texts.
    We collected pairs of articles spanning from 1/1/2001 through 12/24/2004.
    The corpus consists of 2,022 documents with 0-8 documents per day.
    The corpus is available on our web page at http://L2R.cs.uiuc.edu/ cogcomp/.
    The English side was tagged with a publicly available NER system based on the