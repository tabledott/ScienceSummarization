 that is widely studied in the statistics and probability theory communities (Pitman and Yor, 1997; Ishwaran and James, 2001; Pitman, 2002).
    Our model is a direct generalization of the hierarchical Dirichlet language model of (MacKay and Peto, 1994).
    Inference in our model is however not as straightforward and we propose an efficient Markov chain Monte Carlo sampling scheme.
    Pitman-Yor processes produce power-law distributions that more closely resemble those seen in natural languages, and it has been argued that as a result they are more suited to applications in natural language processing (Goldwater et al., 2006).
    We show experimentally that our hierarchical Pitman-Yor language model does indeed produce results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney, two of the currently best performing smoothing methods (Chen and Goodman, 1998).
    In fact we show a stronger result&#8212;that interpolated Kneser-Ney can be interpreted as a particular approximate inferenc