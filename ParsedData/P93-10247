for two words to measure how likely they are to be instances of the same cluster centroid.
    'A more formal discussion will appear in our paper Distributional Clustering, in preparation.
    From an information theoretic perspective D(p q) measures how inefficient on average it would be to use a code based on q to encode a variable distributed according to p. With respect to our problem, D(p n II Pc) thus gives us the information loss in using cluster centroid pc instead of the actual distribution pn for word n when modeling the distributional properties of n. Finally, relative entropy is a natural measure of similarity between distributions for clustering because its minimization leads to cluster centroids that are a simple weighted average of member distributions.
    One technical difficulty is that D(p II p') is not defined when p' (x) = 0 but p(x) &gt; 0.
    We could sidestep this problem (as we did initially) by smoothing zero frequencies appropriately (Church and Gale, 1991).
    However, this is no