tself, and the resulting compression is one of the matching children, such as, here, NP(2).
    There are several hundred rules of this type, and it is very simple to incorporate into our model.
    There are other structures that may be common enough to merit adding, but we limit this experiment to the original rules and our new &#8220;special rules.&#8221;
  
  
    One of the biggest problems with this model of sentence compression is the lack of appropriate training data.
    Typically, abstracts do not seem to contain short sentences matching long ones elsewhere in a paper, and we would prefer a much larger corpus.
    Despite this lack of training data, very good results were obtained both by the K&amp;M model and by our variant.
    We create a way to compress sentences without parallel training data, while sticking as closely to the K&amp;M model as possible.
    The source model stays the same, and we still pay a probability cost in the channel model for every subtree deleted.
    However, the way we