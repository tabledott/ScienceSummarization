ch the input vectors can be mapped nonlin early to a new set of vectors (Scho?lkopf et al, 1998).WSD can be performed by a Nearest Neighbor Clas sifier in the high-dimensional KPCA feature space.
			All these classifiers have the ability to handle large numbers of sparse features, many of which may be irrelevant.
			Moreover, the maximum entropy and boosting models are known to be well suited to handling features that are highly interdependent.
			4.5 Integrating WSD predictions in.
			phrase-based SMT architectures It is non-trivial to incorporate WSD into an existing phrase-based architecture such as Pharaoh (Koehn,2004), since the decoder is not set up to easily accept multiple translation probabilities that are dy namically computed in context-sensitive fashion.
			For every phrase in a given SMT input sentence,the WSD probabilities can be used as additional feature in a loglinear translation model, in combination with typical context-independent SMT bilexi con probabilities.
			We overcome this obstacle 