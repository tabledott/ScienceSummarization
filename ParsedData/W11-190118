into one of the categories shown in Figure 1.
    Figure 2 shows the distribution of these different types that were found in that sample.
    It can be seen that genuine ambiguity and annotator error are the biggest contributors &#8211; the latter of which is usually captured during adjudication, thus showing the increased agreement between the adjudicated version and the individual annotator version.
  
  
    This section describes the CoNLL-2011 Coreference task, including its closed and open track versions, and characterizes the data used for the task and how it was prepared.
    Despite close to a two-decade history of evaluations on coreference tasks, variation in the evaluation criteria and in the training data used have made it difficult for researchers to be clear about the state of the art or to determine which particular areas require further attention.
    There are many different parameters involved in defining a coreference task.
    Looking at various numbers reported in literature can greatly