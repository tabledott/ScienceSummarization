ion.
    The Kullback-Leibler divergence D(pllq) is 0.03.
    The aim of feature selection is to choose a feature that reduces this divergence as much as possible.
    The astute reader will note that there is a problem with the null field if L(G) is infinite.
    Namely, it is not possible to have a uniform probability mass distribution over an infinite set.
    If each dag in an infinite set of dags is assigned a constant nonzero probability E, then the total probability is infinite, no matter how small E is.
    There are a couple of ways of dealing with the problem.
    The approach that DD&amp;L adopt is to assume a consistent prior distribution p(k) over graph sizes k, and a family of random fields qk representing the conditional probability q(x I k); the probability of a tree is then p(k)q(x I k).
    All the random fields have the same features and weights, differing only in their normalizing constants.
    I will take a somewhat different approach here.
    As sketched at the beginning of section 3, 