distributions.
    We use for this purpose the relative entropy or Kullback-Leibler (KL) distance between two distributions This is a natural choice for a variety of reasons, which we will just sketch here.'
    First of all, D(p II q) is zero just when p = q, and it increases as the probability decreases that p is the relative frequency distribution of a random sample drawn according to q.
    More formally, the probability mass given by q to the set of all samples of length n with relative frequency distribution p is bounded by exp &#8212;nD(p II q) (Cover and Thomas, 1991).
    Therefore, if we are trying to distinguish among hypotheses qi when p is the relative frequency distribution of observations, D(p qi) gives the relative weight of evidence in favor of qi.
    Furthermore, a similar relation holds between D(p p') for two empirical distributions p and p' and the probability that p and p' are drawn from the same distribution q.
    We can thus use the relative entropy between the context distributions 