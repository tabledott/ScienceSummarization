erb.
  For role identification, we expect agree- ment between annotators to be much higher than chance, because while any node in the parse tree can be annotated, the vast majority of arguments are chosen from the small number of nodes near the verb.
  In order to isolate the role classification decisions from this effect and avoid artifically inflating the kappa score, we split role identification (role vs. nonrole) from role classification (Arg0 vs. Arg1 vs. .
  and calculate kappa for each decision separately.
  Thus, for the role identification kappa, the interannotator agreement probability P?A?
  is the number of node observation agreements divided by the total number of nodes considered, which is the number of nodes in each parse tree multiplied by the number of predicates annotated in the sentence.
  All the PropBank data were annotated by two people, and in calculating kappa we compare these two annotations, ignoring the specific identities of the annotators for the predicate (in practice, agreement 