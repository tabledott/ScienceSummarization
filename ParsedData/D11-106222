d, we observe lower rejection rates, corresponding to higher inter-annotator agreement, for grammaticality HITs (5.55% on average) than for more complex entailment-related tasks (12.02% on average).
    Looking at costs and execution time, it is hard to draw definite conclusions due to several factors that influence the progress of the crowdsourced jobs (e.g. the fluctuations of Turkers&#8217; performances, the time of the day at which jobs are posted, the difficulty to set the optimal cost for a given HIT14).
    On the one hand, as expected, the more creative &#8220;Add Info&#8221; task proved to be more demanding than the &#8220;Remove Info&#8221;: even though it was paid more, 13Moreover, it is worthwhile noticing that around 20% of the collected items were automatically rejected (and not paid) due to failures on the gold standard controls created both for generation and annotation tasks.
    14The payment for each HIT was set on the basis of a previous feasibility study aimed at determining the best trad