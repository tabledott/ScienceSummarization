en at face value.
    However, the four measures of reliability bear no relationship to each other.
    Worse yet, since none of them take into account the level of agreement one would expect coders to reach by chance, none of them are interpretable even on their own.
    We first explain what effect chance expected agreement has on each of these measures, and then argue that we should adopt the kappa statistic (Siegel and Castellan 1988) as a uniform measure of reliability.
  
  
    Measure (2) seems a natural choice when there are two coders, and there are several possible extensions when there are more coders, including citing separate agreement figures for each important pairing (as KID do by designating an expert), counting a unit as agreed only if all coders agree on it, or measuring one agreement over all possible pairs of coders thrown in together.
    Taking just the two-coder case, the amount of agreement we would expect coders to reach by chance depends on the number and relative proportions of th