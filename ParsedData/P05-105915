on part being X(e/f).
  
  
    We trained both the unlexicalized and the lexicalized ITGs on a parallel corpus of Chinese-English newswire text.
    The Chinese data were automatically segmented into tokens, and English capitalization was retained.
    We replaced words occurring only once with an unknown word token, resulting in a Chinese vocabulary of 23,783 words and an English vocabulary of 27,075 words.
    In the first experiment, we restricted ourselves to sentences of no more than 15 words in either language, resulting in a training corpus of 6,984 sentence pairs with a total of 66,681 Chinese words and 74,651 English words.
    In this experiment, we didn&#8217;t apply the pruning techniques for the lexicalized ITG.
    In the second experiment, we enabled the pruning techniques for the LITG with the beam ratio for the tic-tac-toe pruning as 10&#8722;5 and the number k for the top-k pruning as 25.
    We ran the experiments on sentences up to 25 words long in both languages.
    The resulting traini