g insights.
    Many models of interest can be constructed in our paradigm, without having to write new code.
    Bringing diverse models into the same declarative framework also allows one to apply new optimization methods, objective functions, and finite-state algorithms to all of them.
    To avoid local maxima, one might try deterministic annealing (Rao and Rose, 2001), or randomized methods, or place a prior on &#952;.
    Another extension is to adjust the machine topology, say by model merging (Stolcke and Omohundro, 1994).
    Such techniques build on our parameter estimation method.
    The key algorithmic ideas of this paper extend from forward-backward-style to inside-outside-style methods.
    For example, it should be possible to do end-to-end training of a weighted relation defined by an interestingly parameterized synchronous CFG composed with tree transducers and then FSTs.
  

