., roots are the same, and &#945;, is a sub-sequence of &#945;f.
    This leads to a quite restricted number of different productions on our base training set (ZD-0): 823 different productions were extracted, 593 of which appear only once.
    This first approach has serious limitations; the assumption that sentence compression appropriately models human abstractive data is particularly problematic.
    This considerably limits the amount of training data that can be exploited in Ziff-Davis (which contains overall more than 4,000 documentsabstract pairs), and this makes it very difficult to train lexicalized models.
    An approach to slightly loosen this assumption is to consider document-abstract sentence pairs in which the condensed version contains one or more substitutions or insertions.
    Consider for example the tree pair in Figure 2: the two sentences are syntactically very close, but the substitution of &#8220;computer&#8221; with &#8220;unit&#8221; makes this sentence pair unusable in the framewor