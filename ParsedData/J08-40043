CL.
    This issue of annotator bias was further debated in Di Eugenio and Glass (2004) and Craggs and McGee Wood (2005).
    Di Eugenio and Glass pointed out that the choice of calculating chance agreement by using individual coder marginals (x) or pooled distributions (K) can lead to reliability values falling on different sides of the accepted 0.67 threshold, and recommended reporting both values.
    Craggs and McGee Wood argued, following Krippendorff (2004a,b), that measures like Cohen&#8217;s x are inappropriate for measuring agreement.
    Finally, Passonneau has been advocating the use of Krippendorff&#8217;s &#945; (Krippendorff 1980, 2004a) for coding tasks in CL which do not involve nominal and disjoint categories, including anaphoric annotation, wordsense tagging, and summarization (Passonneau 2004, 2006; Nenkova and Passonneau 2004; Passonneau, Habash, and Rambow 2006).
    Now that more than ten years have passed since Carletta&#8217;s original presentation at the workshop on Empirical Methods 