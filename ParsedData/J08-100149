serve that linguistically impoverished models consistently perform worse than their linguistically elaborate counterparts.
    We assess whether differences in accuracy are statistically significant using a Fisher Sign Test.
    Specifically, we compare the full model against each of the less expressive models (see Table 5).
    Let us first discuss in more detail how the contribution of different knowledge sources varies across domains.
    On the Earthquakes corpus every model that does not use coreference information (Coreference&#8722;Syntax[+/&#8722;]Salience[+/&#8722;]) performs significantly worse than models augmented with coreference (Coreference+ Syntax[+/&#8722;]Salience[+/&#8722;]).
    This effect is less pronounced on the Accidents corpus, especially for model Coreference&#8722;Syntax+Salience+ whose accuracy drops only by 0.5% (the difference between Coreference&#8722;Syntax+Salience+ and Coreference+ Syntax+Salience+ is not statistically significant).
    The same model&#8217;s performance dec