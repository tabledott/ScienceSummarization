gest test set contained 27 input articles) to rate the validity of a summary.
    Consequently, the evaluation that we performed to date is limited.
    We performed a quantitative evaluation of our content-selection component.
    In order to prevent noisy input from the theme construction component from skewing the evaluation, we manually constructed 26 themes, each containing 4 sentences on average.
    Far more training data is needed to tune the generation portion.
    While we have tuned the system to perform with minor errors on the manual set of themes we have created (the missing article in the fourth sentence of the summary in Figure 1 is an example), we need more robust input data from the theme construction component, which is still under development, to train the generator before beginning large scale testing.
    One problem in improving output is determining how to recover from errors in tools used in early stages of the process, such as the tagger and the parser.
    The evaluation task for th