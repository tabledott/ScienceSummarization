ssification tasks.
    Our feature based model that uses only 100 features achieves similar accuracy as the unigram model that uses over 10,000 features.
    Our tree kernel based model outperforms both these models by a significant margin.
    We also experiment with a combination of models: combining unigrams with our features and combining our features with the tree kernel.
    Both these combinations outperform the unigram baseline by over 4% for both classification tasks.
    In this paper, we present extensive feature analysis of the 100 features we propose.
    Our experiments show that features that have to do with Twitter-specific features (emoticons, hashtags etc.) add value to the classifier but only marginally.
    Features that combine prior polarity of words with their parts-of-speech tags are most important for both the classification tasks.
    Thus, we see that standard natural language processing tools are useful even in a genre which is quite different from the genre on which they were trai