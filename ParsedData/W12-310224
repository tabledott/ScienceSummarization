 or for many language pairs the ranking is identical.
    The same is true for the two methods based on pairwise rankings (Lopez, Most Probable).
    However, the two types of ranking lead to significantly different outcomes.
    For instance, the win-based methods are pretty sure that ONLINE-B and RBMT-3 are the two top performers.
    Bootstrap resampling of rankings according to Expected Wins ranking draws a clear line between them and the rest.
    However, Lopez&#8217;s method ranks RBMT-4 first.
    Why?
    In direct comparison of the three systems, RBMT-4 beats statistically insignificantly ONLINE-B 45% wins against 42% wins and essentially ties with RBMT-3 41% wins against 41% wins (ONLINE-B beats RBMT-3 49%&#8211;35%, p G 0.01).
    We use Bojar&#8217;s method as our official method for ranking in Table 4 and as the human judgments that we used when calculating how well automatic evaluation metrics correlate with human judgments.
    In general, there are not enough judgments to rank systems unambig