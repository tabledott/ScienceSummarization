between jobs requiring data annotation, and those involving content generation.
    In the former case, Turkers are presented with the task of labelling input data referring to a fixed set of possible values (e.g. making a choice between multiple alternatives, assigning numerical scores to rank the given data).
    In the latter case, Turkers are faced with creative tasks consisting in the production of textual material (e.g. writing a correct translation, or a summary of a given text).
    The ease of controlling the quality of the acquired data depends on the nature of the job.
    For annotation jobs, quality control mechanisms can be easily set up by calculating Turkers&#8217; agreement, by applying voting schemes, or by adding hidden gold units to the data to be annotated8.
    In contrast, the quality of the results of content generation jobs is harder to assess, due to the fact that multiple valid results are acceptable (e.g. the same content can be expressed, translated, or summarized in different way