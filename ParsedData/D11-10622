l evaluation campaigns, including the RTE Challenge1, the Answer Validation Exercise at CLEF2, and the Textual Entailment task at EVALITA3.
    Despite the differences in the design of the tasks, all the released datasets were collected through similar procedures, always involving expensive manual work done by expert annotators.
    Moreover, in the data creation process, large amounts of hand-crafted T-H pairs often have to be discarded in order to retain only those featuring full agreement, in terms of the assigned entailment judgements, among multiple annotators.
    The amount of discarded pairs is usually high, contributing to increase the costs of creating textual entailment datasets4.
    The issues related to the shortage of datasets and the high costs for their creation are more evident in the CLTE scenario, where: i) the only dataset currently available is an English-Spanish corpus obtained by translating the RTE-3 corpus (Negri and Mehdad, 2010), and ii) the application of the standard methods adop