
  Findings of the 2009 Workshop on Statistical Machine Translation
  
    j schroeder ed ac uk Abstract This paper presents the results of the WMT09 shared tasks, which included a translation task, a system combination task, and an evaluation task.
    We conducted a large-scale manual evaluation of 87 machine translation systems and 22 system combination entries.
    We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality, for more than 20 metrics.
    We present a new evaluation technique whereby system output is edited and judged for correctness.
  
  
    This paper presents the results of the shared tasks of the 2009 EACL Workshop on Statistical Machine Translation, which builds on three previous workshops (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008).
    There were three shared tasks this year: a translation task between English and five other European languages, a task to combine the output 