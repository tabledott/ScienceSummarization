ation scores on average, when translating into English.
    When translating from English into French, German, and Spanish, Bleu and posbleu resulted in the highest correlations with human judgments.
    Finally, we investigated inter- and intra-annotator agreement of human judgments using Kappa coefficients.
    We noticed that ranking whole sentences results in relatively low Kappa coefficients, meaning that there is only fair agreement between the assessors.
    Constituent ranking and acceptability judgments on the other hand show moderate and substantial inter-annotator agreement, respectively.
    Intraannotator agreement was substantial to almost perfect, except for the sentence ranking assessment where agreement was only moderate.
    Although it is difficult to draw exact conclusions from this, one might wonder whether the sentence ranking task is simply too complex, involving too many aspects according to which translations can be ranked.
    The huge wealth of the data generated by this workshop, i