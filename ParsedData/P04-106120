e context distributions are then clustered in some way, often using standard i&#8722;1si &#8212; jsj+1 (see figure 5).
    The model generates all constituent-context pairs, span by span.
    The first stage is to choose a bracketing B for the sentence, which is a maximal non-crossing subset of the spans (equivalent to a binary tree).
    In the basic model, P(B) is uniform over binary trees.
    Then, for each (i, j), the subspan and context pair (isj, i&#8722;1si &#8212; jsj+1) is generated via a classconditional independence model: data clustering methods.
    In the most common case, the items are words, and one uses distributions over adjacent words to induce word classes.
    Previous work has shown that even this quite simple representation allows the induction of quite high quality word classes, largely corresponding to traditional parts of speech (Finch, 1993; Sch&#168;utze, 1995; Clark, 2000).
    A typical pattern would be that stocks and treasuries both frequently occur before the words fell and r