 the property of the loss function, and not on the size of .
    This property is critical as it shows that as long as Eq.
    6 can be computed efficiently, then the Approximate Relevant Set algorithm is efficient.
    Moreover, it gives a bound on the size of an approximate relevant set with a certain accuracy.5 The approximate solution of Eq.
    5 in (**) can be implemented using stochastic gradient descent (SGD), where we may simply update as: The parameter is a fixed constant often referred to as learning rate.
    Again, convergence results can be proved for this procedure.
    Due to the space limitation, we skip the formal statement as well as the corresponding analysis.
    Up to this point, we have not assumed any specific form of the decoder scoring function in our algorithm.
    Now consider Eq.
    1 used in our model.
    We may express it as: where .
    Using this feature representation and the loss function in Eq.
    4, we obtain the following costMargin SGD update rule for each training da