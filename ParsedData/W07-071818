cs disprefer SYSTRAN, which was strongly favored in the human evaluation.
  
  
    In addition to evaluating the translation quality of the shared task entries, we also performed a &#8220;metaevaluation&#8221; of our evaluation methodologies.
    We measured pairwise agreement among annotators using the kappa coefficient (K) which is widely used in computational linguistics for measuring agreement in category judgments (Carletta, 1996).
    It is defined as where P(A) is the proportion of times that the annotators agree, and P(E) is the proportion of time that they would agree by chance.
    We define chance agreement for fluency and adequacy as 5, since they are based on five point scales, and for ranking as s since there are three possible out comes when ranking the output of a pair of systems: A &gt; B, A = B, A &lt; B.
    For inter-annotator agreement we calculated P(A) for fluency and adequacy by examining all items that were annotated by two or more annotators, and calculating the proportion of time t