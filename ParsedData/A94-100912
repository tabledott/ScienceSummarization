tion probabilities, calculated from f (i, j)/ f (i).
    Ti All transition probabilities have the same value.
    We could expect to achieve D1 from, say, a printed dictionary listing parts of speech in order of frequency.
    Perfect training is represented by case DO+TO.
    The Xerox experiments (Cutting et a/., 1992) correspond to something between D1 and D2, and between TO and Ti, in that there is some initial biasing of the probabilities.
    For the test, four corpora were constructed from the LOB corpus: LOB-B from part B, LOB-L from part L, LOB-B-G from parts B to G inclusive and LOB-l3-3 from parts B to J inclusive.
    Corpus LOBB-J was used to train the model, and LOB-B, LOBL and LOB-B-G were passed through thirty iterations of the BW algorithm as untagged data.
    In each case, the best accuracy (on ambiguous words, as usual) from the FB algorithm was noted.
    As an additional test, we tried assigning the most probable tag from the DO lexicon, completely ignoring tag-tag transitions.
    The r