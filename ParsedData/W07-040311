e stories that create a sentence and its translation simultaneously.
    The similarities become more apparent when we consider the canonical-form binary-bracketing ITG (Wu, 1997) shown here: (3) is employed in place of (2) to reduce redundant alignments and clean up EM expectations.1 More importantly for our purposes, it introduces a preterminal C, which generates all phrase pairs or cepts.
    When (3) is parameterized as a stochastic ITG, the conditional distribution p(6/ 1|C) is equivalent to the JPTM&#8217;s p(e, 1); both are joint distributions over all possible phrase pairs.
    The distributions conditioned on the remaining three non-terminals assign probability to concept movement by tracking inversions.
    Like the JPTM&#8217;s distortion model, these parameters grade each movement decision independently.
    With terminal productions producing cepts, and inversions measuring distortion, our phrasal ITG is essentially a variation on the JPTM with an alternate distortion model.
    Our phrasal ITG h