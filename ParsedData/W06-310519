inistic translation for caract&#180;erise is not a common translation for the word.
    Not only does the statistical learning process yield low-entropy translation distributions, but occasionally the translation with undesirably high conditional probability does not have a strong surface correlation with the source phrase.
    This example is not unique; during different initializations of the EM algorithm, we noticed such patterns even for common French phrases such as de and ne.
    The third source of errors is closely related: common phrases that translate in many ways depending on the context can introduce errors if they have a spuriously peaked distribution.
    For instance, consider the lone apostrophe, which is treated as a single token in our data set (figure 5).
    The shape of the heuristic translation distribution for the phrase is intuitively appealing, showing a relatively flat distribution among many possible translations.
    Such a distribution has very high entropy.
    On the other hand,