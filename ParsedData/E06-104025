LG, so BLEU scores against 4 reference translations from reputable sources (as in MT &#8217;05) are a feasible evaluation regime.
    It seems likely that for automatic evaluation in NLG, a larger number of reference texts than four are needed.
    In our experiments, we have found NIST a more reliable evaluation metric than BLEU and in particular ROUGE which did not seem to offer any advantage over simple string-edit distance.
    We also found individual experts&#8217; judgments are not likely to correlate highly with average expert opinion, in fact less likely than NIST scores.
    This seems to imply that if expert evaluation can only be done with one or two experts, but a high-quality reference corpus is available, then a NIST-based evaluation may produce more accurate results than an expert-based evaluation.
    It seems clear that for automatic corpus-based evaluation to work well, we need high-quality reference texts written by many different authors and large enough to give reasonable coverage of phe