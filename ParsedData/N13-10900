
  Linguistic Regularities in Continuous Space Word Representations
  
    Continuous space language models have recently demonstrated outstanding results across a variety of tasks.
    In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights.
    We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset.
    This allows vector-oriented reasoning based on the offsets between words.
    For example, the male/female relationship is automatically learned, and with the induced vector representations, &#8220;King - Man + Woman&#8221; results in a vector very close to &#8220;Queen.&#8221; We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.
    We demonstrate that the word