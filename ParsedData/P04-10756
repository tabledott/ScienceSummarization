sification on which most SVM active learning works are conducted (Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003).
    In the next part, we will introduce informativeness, representativeness and diversity measures for the SVM-based NER.
    The basic idea of informativeness criterion is similar to certainty-based sample selection methods, which have been used in many previous works.
    In our task, we use a distance-based measure to evaluate the informativeness of a word and extend it to the measure of an entity using three scoring functions.
    We prefer the examples with high informative degree for which the current model are most uncertain.
    In the simplest linear form, training SVM is to find a hyperplane that can separate the positive and negative examples in training set with maximum margin.
    The margin is defined by the distance of the hyperplane to the nearest of the positive and negative examples.
    The training examples which are closest to the hyperplane are called support vecto