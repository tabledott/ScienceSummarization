labeling.
    Our analysis suggests that corrected co-training and similar moderately supervised methods may help cotraining scale to large natural language learning tasks.
  
  
    Co-Training (Blum and Mitchell, 1998) is a weakly supervised paradigm for learning a classification task from a small set of labeled data and a large set of unlabeled data, using separate, but redundant, views of the data.
    While previous research (summarized in Section 2) has investigated the theoretical basis of co-training, this study is motivated by practical concerns.
    We seek to apply the co-training paradigm to problems in natural language learning, with the goal of reducing the amount of humanannotated data required for developing natural language processing components.
    In particular, many natural language learning tasks contrast sharply with the classification tasks previously studied in conjunction with co-training in that they require hundreds of thousands, rather than hundreds, of training examples.
    Cons