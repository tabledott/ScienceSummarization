orma- tion.
  This repeats until all the words in the vocabulary have been exhausted.
  We then take our C classes, and use the same algorithm to merge classes that minimize the loss of mutual information, until one class remains.
  If we trace the order in which words and classes are merged, we can form a binary tree whose leaves consists of words and whose root is the class which spans the entire vocabulary.
  Consequently, we uniquely identify each word by its path from the root, which 1 See Table 7 for examples of  features 251 can be represented by a string of binary digits.
  If a path lengt of a word is less than the maximum depth, we pad the bottor of the path with Os (dummy left branches), so that all word are represented by an equally long bitstring.
  "Class" feature query the value of bits, and hence examine the path of th word in the mutual information tree.
  Special Features In addition to the types of features de scribed above, we employ two special features in the MI model, the: Complement an