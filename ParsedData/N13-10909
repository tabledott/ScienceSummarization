r (xa, xd).
  
  
    To evaluate the vector offset method, we used vectors generated by the RNN toolkit of Mikolov (2012).
    Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600.
    The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary.
    Table 2 shows results for both RNNLM and LSA vectors on the syntactic task.
    LSA was trained on the same data as the RNN.
    We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly.
    2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we computed the intersection of the vocabular