ive studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)).
    In many ensemble approaches the member classifiers are learned with different algorithms that are trained with the same data.
    For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data.
    This paper takes a different approach, where the learning algorithm is the same for all classifiers but the training data is different.
    This is motivated by the belief that there is more to be gained by varying the representation of context than there is from using many different learning algorithms on the same data.
    This is especially true in this domain since the Naive Bayesian classifier has a history of success and since there is no generally agreed upon set of features th