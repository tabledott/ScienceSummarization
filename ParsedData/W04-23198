reference so it cannot be matched again (this prevents felicitous matches due to identical repeated words).
    (4) Repeat this process for each word in each reference-labeler pair, and rotate to the next labeler as the reference.
    Note that this metric requires perfect matching of the full utterance a word is in for that word to be matched.
    For example in the following case, labelers agree on 3 segmentation locations, but the agreement on our metric is only 0.14, since only 1 of 7 words is matched: Overall segmentation results on this metric are provided by labeler pair in Table 1.
    We examined agreement on DA labels using the Kappa statistic [3], which adjusts for chance agreement.
    Because of the large number of unique full label combinations, we report Kappa values in Table 2 using various class mappings distributed with the corpus.
    Values are shown by labeler pair.
    The overall value of Kappa for our basic, six-way classmap (Map1) is 0.80, representing good agreement for this type of 