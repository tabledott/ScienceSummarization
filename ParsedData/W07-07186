nslation systems, they are an imperfect substitute for human assessment of translation quality.
    Manual evaluation is time consuming and expensive to perform, so comprehensive comparisons of multiple systems are rare.
    For our manual evaluation we distributed the workload across a number of people, including participants in the shared task, interested volunteers, and a small number of paid annotators.
    More than 100 people participated in the manual evaluation, with 75 of those people putting in at least an hour&#8217;s worth of effort.
    A total of 330 hours of labor was invested, nearly doubling last year&#8217;s all-volunteer effort which yielded 180 hours of effort.
    Beyond simply ranking the shared task submissions, we had a number of scientific goals for the manual evaluation.
    Firstly, we wanted to collect data which could be used to assess how well automatic metrics correlate with human judgments.
    Secondly, we wanted to examine different types of manual evaluation and assess which