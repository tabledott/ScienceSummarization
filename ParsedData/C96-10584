of sensible or perverse ways.
  lh(~ choice of l;he right model is not a priori (A)vious.
  One way to huild a l)robabilistie g rammar  is to specify what sequences of moves (such as shift an(/ reduce) a parser is likely to make.
  It is reasonable to expect a given move to be correct about as often on test data.
  as on training data.
  This is tire philosophy behind stochastic CF(I  (aelinek et a1.1992), "history-based" phrase-structure parsing (I-~lack et al., 1992), +m(I others.
  IIowever, i)rol)ability models derived from parsers sotnetimes focus on i,lci(lental prope.rties of the data.
  This utW be the case for (l,alli.rty et M., 1992)s model for link grammar,  l[ we were to adapt their top-(h)wn stochastic parsing str~tegy to the rather similar case of depen(lency gram- mar, we would find their elementary probabil it ies tabulat ing only non-intuitive aspects of the parse structure: Pr(word j is the r ightmost pre-k chihl of word i ] i is a right-sl)ine st, rid, descendant of one of the left children