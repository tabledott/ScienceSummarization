t al., 2009), and one previous MetricsMATR meeting (Przybocki et al., 2008).
    There were three shared tasks this year: a translation task between English and four other European languages, a task to combine the output of multiple machine translation systems, and a task to predict human judgments of translation quality using automatic evaluation metrics.
    The performance on each of these shared task was determined after a comprehensive human evaluation.
    There were a number of differences between this year&#8217;s workshop and last year&#8217;s workshop: crease the statistical significance of our findings.
    We discuss the feasibility of using nonexperts evaluators, by analyzing the cost, volume and quality of non-expert annotations.
    &#8226; Clearer results for system combination &#8211; This year we excluded Google translations from the systems used in system combination.
    In last year&#8217;s evaluation, the large margin between Google and many of the other systems meant that it was hard to