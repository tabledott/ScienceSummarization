he grief will always be zero for the default rank predictions).
    Now consider the case where the training data is not linearly separable with regard to agreement classification.
    Define the margin of the worst case error to be &#946; = maxt{|(a&#183;xt) |: (a&#183;xt)at &lt; 0}.
    If &#946; &lt; &#947;, then again Good Grief decoding will always produce the default results (since the grief of the agreement model will be at most &#946; in cases of error, whereas the grief of the ranking models for any deviation from their default predictions will be at least &#947;).
    On the other hand, if &#946; &#57745; &#947;, then the agreement model errors could potentially disrupt the perfect ranking.
    However, we need only rescale w&#8727; := w&#8727;(&#65533; &#65533; + 0 and b&#8727; := b&#8727;(&#65533; + E) to ensure that the grief of the ranking models will always exceed the grief of the agreement model in cases where the latter is in error.
    Thus whenever independent ranking models can perfectly r