ning corpus by probabilistically sampling sentences from the different sources weighted by the size of each source.
    In Figure 1, we show learning curves for each learner, up to one billion words of training data.
    Each point in the graph is the average performance over ten confusion sets for that size training corpus.
    Note that the curves appear to be log-linear even out to one billion words.
    Of course for many problems, additional training data has a non-zero cost.
    However, these results suggest that we may want to reconsider the trade-off between spending time and money on algorithm development versus spending it on corpus development.
    At least for the problem of confusable disambiguation, none of the learners tested is close to asymptoting in performance at the training corpus size commonly employed by the field.
    Such gains in accuracy, however, do not come for free.
    Figure 2 shows the size of learned representations as a function of training data size.
    For some applicati