0 word limit in Brown et al.
  And even then, there would be 81 million pairs to consider.
  If the training corpus is not too large (e.g., 50,000 regions), then it is possible to consider all pairs of words that actually co-occur in at least one region (i.e., a ~ 0).
  Unfortunately, with a training corpus of N = 890,000 regions, we have found that there are too many such pairs and it becomes necessary to be more sdective (heuristic).
  We have had fakly good success with a progressive deepening strategy.
  That is, select a small set of regions (e.g., 10,000) and use all of the training material to compute #2 for all pairs of words that appear in any of these 10,000 regions.
  Select the best pairs.
  That is, take a pair (x, y) if it has a ~2 significantly better than any other pair of the form (x, z) or (w, y).
  This procedure would take house/chambre but not house/communes.
  Repeat his operation, using larger and larger samples of the training corpus to suggest possibly interesting pairs.
  On each ite