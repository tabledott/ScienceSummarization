than conditional estimates, ie: cI(s, t) + w,\(s, t)co(, s, t), suitably normalized.5 Such an approach could be simulated by a MAP-style combination in which separate 0(t) values were maintained for each t. This would make the model more powerful, but at the cost of having to learn to downweight OUT separately for each t, which we suspect would require more training data for reliable performance.
    We have not explored this strategy.
    We used 22 features for the logistic weighting model, divided into two groups: one intended to reflect the degree to which a phrase pair belongs to general language, and one intended to capture similarity to the IN domain.
    The 14 general-language features embody straightforward cues: frequency, &#8220;centrality&#8221; as reflected in model scores, and lack of burstiness.
    They are: 5We are grateful to an anonymous reviewer for pointing this out.
    6One of our experimental settings lacks document boundaries, and we used this approximation in both settings for consi