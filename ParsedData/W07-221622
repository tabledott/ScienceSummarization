
    This distribution is usually smoothed and is often conditioned on more information including the orientation of x relative to y (i.e., to the left/right) and distance between the two words.
    In the supervised setting this model can be trained with maximum likelihood estimation, which amounts to simple counts over the data.
    Learning in the unsupervised setting requires EM and is discussed in Section 4.4.2.
    Another generative dependency model of interest is that given by Klein and Manning (2004).
    In this model the sentence and tree are generated jointly, which allows one to drop the assumption that p(T |n) is uniform.
    This requires the addition to the model of parameters px,STOP for each x E E, with the normalization condition px,STOP + Py,k pkx,y = 1.
    It is possible to extend the model of Klein and Manning (2004) to the non-projective case.
    However, the resulting distribution will be over multisets of words from the alphabet instead of strings.
    The discussion in this section