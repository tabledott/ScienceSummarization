n therefore be built by dynamic program- ming, where we build and retain the highest- scoring span of each signature.
  Taking the Markov process to gen- erate (tag,word) pairs from right to left, we let (6) define the score of a span from word k to word (?.
  The first product encodes the Markovian proba- bility that the (tag,word) pairs k through g -  1 are as claimed by the span, conditional on the appear- ance of specific (tag,word) pairs at g, ~+1.
  ~ Again, scores can be easily updated when spans combine, and the probability of a complete parse P, divided by the total probability of all parses that succeed in satisfying lexical preferences, is just P s  score.
  Finally, model A is scored the same as model B, except for the second factor in (6), SThe third factor depends on, e.g., kid(i,c- 1), which we recover flom the span signature.
  Also, mat- ters are complicated slightly by the probabilities asso- ciated with the generation of STOP.
  6Different k-g spans have scores conditioned on dif- ferent hy