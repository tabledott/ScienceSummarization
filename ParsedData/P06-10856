ood solution is not competitive, since it contains no utterance-internal word boundaries.
    To see why not, consider the solution in which p$ = 1 and each utterance is a single &#8216;word&#8217;, with probability equal to the empirical probability of that utterance.
    Any other solution will match the empirical distribution of the data less well.
    In particular, a solution with additional word boundaries must have 1 &#8722; p$ &gt; 0, which means it wastes probability mass modeling unseen data (which can now be generated by concatenating observed utterances together).
    Intuitively, the NGS model considers the unsegmented solution to be optimal because it ranks all hypotheses equally probable a priori.
    We know, however, that hypotheses that memorize the input data are unlikely to generalize to unseen data, and are therefore poor solutions.
    To prevent memorization, we could restrict our hypothesis space to models with fewer parameters than the number of utterances in the data.
    A more gene