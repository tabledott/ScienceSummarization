valuation in this domain, and we adopt it here.
  
  
    In our work we made use of the Word Space model of (semantic) similiarty (Sch&#168;utze, 1998) and extended it slightly to MWEs.
    In this framework, &#8220;meaning&#8221; is modeled as an n-dimensional vector, derived via singular value decomposition (Deerwester et al., 1990) from word co-occurrence counts for the expression in question, a technique frequently referred to as Latent Semantic Analysis (LSA).
    This kind of dimensionality reduction has been shown to improve performance in a number of text-based domains (Berry et al., 1999).
    For our experiments we used a local German newspaper corpus.2 We built our LSA model with the Infomap Software package.3, using the 1000 most frequent words not on the 102-word hand-generated stop list as the content-bearing dimension words (the columns of the matrix).
    The 20,000 most frequent content words were assigned row values by counting occurrences within a 30word window.
    SVD was used to reduce 