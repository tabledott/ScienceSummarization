ed in isolation, the two methods seem particularly suited to each other &#8212; in particular, the method of GPB can greatly facilitate the production of FLMs with better performance.
  
  
    In a factored language model, a word is viewed as a vector of k factors, so that wt &#8801; {f1t , f2t , ... , fKt }.
    Factors can be anything, including morphological classes, stems, roots, and other such features in highly inflected languages (e.g., Arabic, German, Finnish, etc.
    ), or data-driven word classes or semantic features useful for sparsely inflected languages (e.g., English).
    Clearly, a two-factor FLM generalizes standard class-based language models, where one factor is the word class and the other is words themselves.
    An FLM is a model over factors, i.e., p(ft:K|ft1 F'1K that can be factored as a product of probabilities of the form p(f|f1, f2, ... , fN).
    Our task is twofold: 1) find an appropriate set of factors, and 2) induce an appropriate statistical model over those factors (i.e., t