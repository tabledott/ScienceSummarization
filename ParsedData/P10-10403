
    This leads to a one-hot representation over a smaller vocabulary size.
    Neural language models (Bengio et al., 2001; Schwenk &amp; Gauvain, 2002; Mnih &amp; Hinton, 2007; Collobert &amp; Weston, 2008), on the other hand, induce dense real-valued low-dimensional word embeddings using unsupervised approaches.
    (See Bengio (2008) for a more complete list of references on neural language models.)
    Unsupervised word representations have been used in previous NLP work, and have demonstrated improvements in generalization accuracy on a variety of tasks.
    But different word representations have never been systematically compared in a controlled way.
    In this work, we compare different techniques for inducing word representations, evaluating them on the tasks of named entity recognition (NER) and chunking.
    We retract former negative results published in Turian et al. (2009) about Collobert and Weston (2008) embeddings, given training improvements that we describe in Section 7.1.
  
  
    Distr