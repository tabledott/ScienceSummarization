84.
    For reference, the perplexity after 5 iterations of Model 1 was 24.01.
    Perplexity values roughly indicate the predictive power of the model.
    Generally, lower perplexity means a better model, but it might cause over-fitting to a training data.
    Since the IBM Model usually requires millions of training sentences, the lower perplexity value for the IBM Model is likely due to over-fitting.
  
  
    We have presented a syntax-based translation model that statistically models the translation process from an English parse tree into a foreignlanguage sentence.
    The model can make use of syntactic information and performs better for language pairs with different word orders and case marking schema.
    We conducted a small-scale experiment to compare the performance with IBM Model 5, and got better alignment results.
  
  
    This appendix describes an efficient implementation of the EM algorithm for our translation model.
    This implementation uses a graph structure for a pair .
    A graph 