in a finite, though impractically large, number of steps because each translation has only a finite number of alignments.
    In practice, we are never sure that we have found the Viterbi alignment.
    If we reinterpret the Viterbi alignment to mean the most probable alignment that we can find rather than the most probable alignment that exists, then a similarly reinterpreted Viterbi training algorithm still converges.
    We have already used this algorithm successfully as a part of a system to assign senses to English and French words on the basis of the context in which they appear (Brown et al. 1991a, 1991b).
    We expect to use it in models that we develop beyond Model 5.
    In Models 1-5, we restrict our attention to alignments with cepts containing no more than one word each.
    Except in Models 4 and 5, cepts play little role in our development.
    Even in these models, cepts are determined implicitly by the fertilities of the words in the alignment: words for which the fertility is greater than 