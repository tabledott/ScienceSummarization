ies with the smoothing parameter &amp;.
    Figure 6 shows learning curves for various values of &amp;.
    It can be seen that values other than &amp; = 0.0025 can lead to undertraining or overtraining of the model.
    Results on section 23 of the WSJ Treebank.
    &#8220;LR&#8221;is labeled recall; &#8220;LP&#8221;is labeled precision; &#8220;CBs&#8221;is the average number of crossing brackets per sentence; &#8220;0 CBs&#8221;is the percentage of sentences with 0 crossing brackets; &#8220;2 CBs&#8221;is the percentage of sentences with two or more crossing brackets.
    All the results in this table are for models trained and tested on the same data, using the same evaluation metric.
    Note that the ExpLoss results are very slightly different from the original results published in Collins (2000).
    We recently reimplemented the boosting code and reran the experiments, and minor differences in the code and a values tested on development data led to minor improvements in the results.
    Learning curve 