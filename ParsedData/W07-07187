 was the best.
    A number of criteria could be adopted for choosing among different types of manual evaluation: the ease with which people are able to perform the task, their agreement with other annotators, their reliability when asked to repeat judgments, or the number of judgments which can be collected in a fixed time period.
    There are a range of possibilities for how human evaluation of machine translation can be done.
    For instance, it can be evaluated with reading comprehension tests (Jones et al., 2005), or by assigning subjective scores to the translations of individual sentences (LDC, 2005).
    We examined three different ways of manually evaluating machine translation quality: The most widely used methodology when manually evaluating MT is to assign values from two five point scales representing fluency and adequacy.
    These scales were developed for the annual NIST Machine Translation Evaluation Workshop by the Linguistics Data Consortium (LDC, 2005).
    The five point scale for adequ