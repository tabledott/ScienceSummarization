ic language pair.
    ITG has the advantage of being entirely data-driven &#8211; the trees are derived from an expectation maximization procedure given only the original strings as input.
    In this paper, we extend ITG to condition the grammar production probabilities on lexical information throughout the tree.
    This model is reminiscent of lexicalization as used in modern statistical parsers, in that a unique head word is chosen for each constituent in the tree.
    It differs in that the head words are chosen through EM rather than deterministic rules.
    This approach is designed to retain the purely data-driven character of ITG, while giving the model more information to work with.
    By conditioning on lexical information, we expect the model to be able capture the same systematic differences in languages&#8217; grammars that motive the tree-to-string model, for example, SVO vs. SOV word order or prepositions vs. postpositions, but to be able to do so in a more fine-grained manner.
    The intera