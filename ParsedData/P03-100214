omain NE recognizer, having 96% F-measure on the MUC61 data.
    We reserved section 23 of PropBank/TreeBank for testing, and we trained on the rest.
    Due to memory limitations on our hardware, for the argument finding task we trained on the first 150 KB of TreeBank (about 11% of TreeBank), and for the role assignment task on the first 75 KB of argument constituents (about 60% of PropBank annotations).
    Table 1 shows the results obtained by our inductive learning approach.
    The first column describes the feature sets used in each of the 7 experiments performed.
    The following three columns indicate the precision (P), recall (R), and F-measure ( )2 obtained for the task of identifying argument constituents.
    The last column shows the accuracy (A) for the role assignment task using known argument constituents.
    The first row in Table 1 lists the results obtained when using only the FS1 features.
    The next five lines list the individual contributions of each of the newly added features when 