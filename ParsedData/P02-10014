he forward-backward algorithm (Baum, 1972) trains only Hidden Markov Models, while (Ristad and Yianilos, 1996) trains only stochastic edit distance.
    In short, current finite-state toolkits include no training algorithms, because none exist for the large space of statistical models that the toolkits can in principle describe and run.
    'Given output, find input to maximize P(input, output).
    This paper aims to provide a remedy through a new paradigm, which we call parameterized finitestate machines.
    It lays out a fully general approach for training the weights of weighted rational relations.
    First &#167;2 considers how to parameterize such models, so that weights are defined in terms of underlying parameters to be learned.
    &#167;3 asks what it means to learn these parameters from training data (what is to be optimized?
    ), and notes the apparently formidable bookkeeping involved.
    &#167;4 cuts through the difficulty with a surprisingly simple trick.
    Finally, &#167;5 removes ineff