m large training sets and on bigram models in general.
    These results question the generality of the previous reference result concerning Katz smoothing: Katz (1987) reported that his method slightly outperforms an unspecified version ofJelinek-Mercer smoothing on a single training set of 750,000 words.
    Furthermore, we show that Church-Gale smoothing, which previously had not been compared with common smoothing techniques, outperforms all existing methods on bigram models produced from large training sets.
    Finally, we find that our novel methods average-count and one-count are superior to existing methods for trigram models and perform well on bigram models; method one-count yields marginally worse performance but is extremely easy to implement.
    In this study, we measure performance solely through the cross-entropy of test data; it would be interesting to see how these cross-entropy differences correlate with performance in end applications such as speech recognition.
    In addition, it would 