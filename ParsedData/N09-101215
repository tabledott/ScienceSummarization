221; (Figure 5).
    We would expect that distribution over the nearest left argument for &#8220;dog&#8221; to be different than farther left arguments.
    The figure shows that EVG allows these two distributions to be different (nonterminals L2dog and L1dog) whereas DMV forces them to be equivalent (both use L1dog as the nonterminal).
    All of the probabilistic models discussed thus far have incorporated only part-of-speech information (see Footnote 2).
    In supervised parsing of both dependencies and constituency, lexical information is critical (Collins, 1999).
    We incorporate lexical information into EVG (henceforth L-EVG) by extending the distributions over argument parts-of-speech A to condition on the head word h in addition to the head part-of-speech H, direction d and argument position v. The argument word a distribution is merely conditioned on part-of-speech A; we leave refining this model to future work.
    In order to incorporate lexicalization, we extend the EVG CFG to allow the nonterm