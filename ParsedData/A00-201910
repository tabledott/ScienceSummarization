he mutual information measures provide candidate errors, but this approach overgenerates &#8212; it finds rare, but still quite grammatical, sequences.
    To reduce the number of false positives, no candidate found by the MI measures is considered an error if it appears in the word-specific corpus at least two times.
    This increases ALEK's precision at the price of reduced recall.
    For example, a knowledge will not be treated as an error because it appears in the training corpus as part of the longer a knowledge of sequence (as in a knowledge of mathematics).
    ALEK also uses another statistical technique for finding rare and possibly ungrammatical tag and function word bigrams by computing the x2 (chi square) statistic for the difference between the bigram proportions found in the word-specific and in the general corpus: = The x2 measure faces the same problem of overgenerating errors.
    Due to the large sample sizes, extreme values can be obtained even though effect size may be minuscule.
    To 