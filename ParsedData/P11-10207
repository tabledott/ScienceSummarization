adequacy evaluator, as well as sample human judgments to train the overall metric.
    We designed our data collection framework for use on crowdsourcing platforms such as Amazon&#8217;s Mechanical Turk.
    Crowdsourcing can allow inexpensive and rapid data collection for various NLP tasks (Ambati and Vogel, 2010; Bloodgood and Callison-Burch, 2010a; Bloodgood and CallisonBurch, 2010b; Irvine and Klementiev, 2010), including human evaluations of NLP systems (CallisonBurch, 2009; Denkowski and Lavie, 2010; Zaidan and Callison-Burch, 2009).
    Of particular relevance are the paraphrasing work by Buzek et al. (2010) and Denkowski et al.
    (2010).
    Buzek et al. automatically identified problem regions in a translation task and had workers attempt to paraphrase them, while Denkowski et al. asked workers to assess the validity of automatically extracted paraphrases.
    Our work is distinct from these earlier efforts both in terms of the task &#8211; attempting to collect linguistic descriptions using a visu