as ? ?
			1, alpha skew is identically KL divergence.
			4.1 Zero-KL Divergence.
			In this section we introduce a novel measure of distribu tional divergence based on a reinterpretation of the skew divergence.
			Skew divergence avoids zeros in q by mixingin some of p, but its performance on many natural language tasks improves as it better approximates KL diver gence.
			We propose an alternative approximation to KL divergence called Zero-KL divergence, or ZKL.
			Whenqi is non-zero, we use exactly the term from KL diver gence.
			When qi = 0, we have a problem?in the limit as ? ?
			1, the corresponding term approaches infinity.
			We let ZKL use the Skew divergence value for these terms: pi log pi ?qi+(1??)pi . Because qi = 0 this simplifies to.
			pi log pi (1??)pi = pi log 11??
			Lee showed skew divergence?s best performance was for ? near to 1, so we formalize this intuition by choosing ? exponentially near to 1, i.e. we can choose our ? as 1?2??
			for some ? ?
			R+.
			Zero terms in the sum can now