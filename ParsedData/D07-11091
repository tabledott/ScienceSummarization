random variable that is inferred from data.
			There are two central advantages to this approach.First, with LDAWN we automatically learn the con text in which a word is disambiguated.
			Rather than disambiguating at the sentence-level or the document-level, our model uses the other words that share the same hidden topic across many documents.
			Second, LDAWN is a fully-fledged generative model.
			Generative models are modular and can beeasily combined and composed to form more complicated models.
			(As a canonical example, the ubiq uitous hidden Markov model is a series of mixturemodels chained together.)
			Thus, developing a gen erative model for WSD gives other generative NLP algorithms a natural way to take advantage of the hidden senses of words.
			In general, topic models are statistical models of text that posit a hidden space of topics in which the corpus is embedded (Blei et al, 2003).
			Given a corpus, posterior inference in topic models amounts to automatically discovering the underlying the