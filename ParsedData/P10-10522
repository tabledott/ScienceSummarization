ests on the observations (see discussions in, eg., (Punyakanok et al., 2005; Liang et al., 2008)).
    Limitating the feature set or the number of output labels is however frustrating for many NLP tasks, where the type and number of potentially relevant features are very large.
    A number of studies have tried to alleviate this problem.
    Pal et al. (2006) propose to use a &#8220;sparse&#8221; version of the forward-backward algorithm during training, where sparsity is enforced through beam pruning.
    Related ideas are discussed by Dietterich et al. (2004); by Cohn (2006), who considers &#8220;generalized&#8221; feature functions; and by Jeong et al.
    (2009), who use approximations to simplify the forward-backward recursions.
    In this paper, we show that the sparsity that is induced by il-penalized estimation of CRFs can be used to reduce the total training time, while yielding extremely compact models.
    The benefits of sparsity are even greater during inference: less features need to be extrac