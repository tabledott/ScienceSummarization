alization error by 35%-45% relative over a 1-gram capitalization model.
    We have also presented a general technique for adapting MaxEnt probability models.
    It was shown to be very effective in adapting a background MEMM capitalization model, improving the accuracy by 20-25% relative.
    An overall 50-60% reduction in capitalization error over the standard 1-gram baseline is achieved.
    A surprising result is that the adaptation performance gain is not due to adding more, domain-specific features but rather making better use of the background features for modeling the in-domain data.
    As expected, adding more background training data improves performance but a very small amount of domain specific data also helps significantly if one can make use of it in an effective way.
    The &#8220;There&#8217;s no data like more data&#8221; rule-of-thumb could be amended by &#8220;..., especially if it&#8217;s the right data!&#8221;.
    As future work we plan to investigate the best way to blend increasing 