d the three component models of the alignment templates.
    The first row shows the results using only the four baseline features with &#955;1 = &#183; &#183; &#183; = &#955;4 = 1.
    The second row shows the result if we train the model scaling factors.
    We see a systematic improvement on all error rates.
    The following three rows show the results if we add the word penalty, an additional class-based five-gram GIS algorithm for maximum entropy training of alignment templates. language model and the conventional dictionary features.
    We observe improved error rates for using the word penalty and the class-based language model as additional features.
    Figure 3 show how the sentence error rate (SER) on the test corpus improves during the iterations of the GIS algorithm.
    We see that the sentence error rates converges after about 4000 iterations.
    We do not observe significant overfitting.
    Table 3 shows the resulting normalized model scaling factors.
    Multiplying each model scaling fac