poorly in unsupervised induction of linguistic structure (Carroll and Charniak, 1992;Merialdo, 1994; Klein, 2005; Smith, 2006).
			In ret rospect one can certainly find reasons to explain this failure: after all, likelihood does not appear in thewide variety of linguistic tests proposed for identi fying linguistic structure (Fromkin, 2001).This paper focuses on unsupervised part-ofspeech (POS) tagging, because it is perhaps the sim plest linguistic induction task.
			We suggest that onereason for the apparent failure of EM for POS tagging is that it tends to assign relatively equal numbers of tokens to each hidden state, while the em pirical distribution of POS tags is highly skewed, like many linguistic (and non-linguistic) phenomena(Mitzenmacher, 2003).
			We focus on first-order Hid den Markov Models (HMMs) in which the hidden state is interpreted as a POS tag, also known as bitag models.
			In this setting we show that EM performs poorlywhen evaluated using a ?1-to-1 accuracy?
			evalua tion, where each P