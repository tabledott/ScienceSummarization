 rule F` and classifier G satisfy precision independence just in case Precision independence is stated here so that it looks like a conditional independence assumption, to emphasize the similarity to the analysis of co-training.
    In fact, it is only &#8220;half&#8221; an independence assumption&#8212;for precision independence, it is not necessary that P(Y`J &#175;F`, G&#8727;) = P(Y`J The second assumption is that classifiers make balanced errors.
    That is: P(Y`, G&#175;`JF`) = P(Y&#175;`, G`JF`) Let us first consider a concrete (but hypothetical) example.
    Suppose the initial classifier correctly labels 100 out of 1000 instances, and makes no mistakes.
    Then the initial precision is 1(Yarowsky, 1995), citing (Yarowsky, 1994), actually uses a superficially different score that is, however, a monotone transform of precision, hence equivalent to precision, since it is used only for sorting.
    1 and recall is 0.1.
    Suppose further that we add an atomic rule that correctly labels 19 new instance