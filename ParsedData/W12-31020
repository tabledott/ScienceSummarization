
  Findings of the 2012 Workshop on Statistical Machine Translation
  
    This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality.
    We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams.
    We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics.
    We introduced a new quality estimation task this year, and evaluated submissions from 11 teams.
  
  
    This paper presents the results of the shared tasks of the Workshop on statistical Machine Translation (WMT), which was held at NAACL 2012.
    This workshop builds on six previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010; CallisonBurch et al., 