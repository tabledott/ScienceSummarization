ommon with maximum entropy models than conventionally smoothed n-gram models.
  
  
    We conducted a range of experiments to explore the effectiveness and the error-space trade-off of Bloom filters for language modelling in SMT.
    The spaceefficiency of these models also allows us to investigate the impact of using much larger corpora and higher-order n-grams on translation quality.
    While our main experiments use the Bloom filter models in conjunction with a conventional smoothed trigram model, we also present experiments with these models in isolation to highlight the impact of different order n-grams on the translation process.
    Finally, we present some empirical analysis of both the logfrequency Bloom filter and the sub-sequence filtering technique which may be of independent interest.
    All of our experiments use publically available resources.
    We use the French-English section of the Europarl (EP) corpus for parallel data and language modelling (Koehn, 2003) and the English Gigaword Corp