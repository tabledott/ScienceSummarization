To introduce generalization capabilities, some of the features will depend on word classes or partof-speech information.
    Let F1J denote the word class sequence that corresponds to the source language sentence fJ1 and let EI1 denote the target word class sequence that corresponds to the target language sentence eI1.
    Then, the feature functions are of the form hn(fJ1 , eI1, F1J , EI1, i, j, j&#8242;).
    We consider the following binary features: Here, &#948;(', ') denotes the Kronecker-function.
    In the experiments, we will use d E {&#8722;1, 0, 11.
    Many other feature functions are imaginable, e.g. combinations of the described feature functions, n-gram or multi-word features, joint source and target language feature functions.
    As training criterion, we use the maximum class posterior probability.
    This corresponds to maximizing the likelihood of the maximum entropy model.
    Since the optimization criterion is convex, there is only a single optimum and no convergence problems occur.
  