curacy P-CFG 59.8% HBG 74.6% Error Reduction 36.8% Figure 4: Parsing accuracy: P-CFG vs. HBG In developing HBG, we experimented with similar models of varying complexity.
    One discovery made during this experimentation is that models which incorporated more context than HBG performed slightly worse than HBG.
    This suggests that the current training corpus may not contain enough sentences to estimate richer models.
    Based on the results of these experiments, it appears likely that significantly increasing the size of the training corpus should result in a corresponding improvement in the accuracy of HBG and richer HBG-like models.
    To check the value of the above detailed history, we tried the simpler model: 1.
    2.
    3. p(Syn p(Sem ISyn, p(R ISyn, Sem, This model corresponds to a P-CFG with NTs that are the crude syntax and semantic categories with the lexical heads.
    The in this case was 66%, a small improvement over the P-CFG model indicating the value of using more context from the deriv