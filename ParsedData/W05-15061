the distribution of parse trees for a given sentence under a PCFG can be represented as a packed forest from which the highest-probability tree can be easily extracted.However, when the objective function f has no com patible packed representation, exact inference would beintractable.
			To alleviate this problem, one common approach from machine learning is loopy belief propaga tion (Pearl, 1988).
			Another solution (which is popular in NLP) is to split the computation into two phases: in the first phase, use some compatible objective function f ? to produce a k-best list (the top k candidates under f ?), which serves as an approximation to the full set.Then, in the second phase, optimize f over all the analyses in the k-best list.
			A typical example is discrimina tive reranking on k-best lists from a generative module, such as (Collins, 2000) for parsing and (Shen et al, 2004) for translation, where the reranking model has nonlocal features that cannot be computed during parsing proper.Another example is