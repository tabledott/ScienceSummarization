hinese, which is also anSVO language.
			While the t-table for Chinese com posed rules clearly gives good estimates for the?correct?
			x0 x1 ordering (p = .9), i.e. subject before verb, the t-table for minimal rules unreason ably gives preference to verb-subject ordering (x1x0, p = .37), because the most probable transfor mation (x0 x1) does not correspond to a minimal rule.
			We obtain different results with Arabic, an VSO language, and our model effectively learns to move the subject after the verb (p = .59).
			lhs1 in Table 4 shows that our model is able to learn large-scale constituent re-orderings, such as re-ordering NPs in a NP-of-NP construction, and put the modifier first as it is more commonlythe case in Chinese (p = .54).
			If more syntac tic context is available as in lhs2, our modelprovides much sharper estimates, and appropri ately reverses the order of three constituents with high probability (p = .68), inserting modifiers first (possessive markers?
			are needed here for better syntactic d