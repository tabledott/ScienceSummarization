e rules), but also be able to combine them in order to generate new text, while attempting to produce a shorter resulting string.
			Quirk et al (2004) present an end-to-end paraphrasing system inspired byphrase-based machine translation that can both ac quire paraphrases and use them to generate new strings.
			However, their model is limited to lexical substitution ? no reordering takes place ? and is 137 lacking the compression objective.Once we move away from extractive compres sion we are faced with two problems.
			First, wemust find an appropriate training set for our abstractive task.
			Compression corpora are not natu rally available and existing paraphrase corpora do not normally contain compressions.
			Our second problem concerns the modeling task itself.
			Ideally, our learning framework should handle structural mismatches and complex rewriting operations.In what follows, we first present a new cor pus for abstractive compression which we created by having annotators compress sentences while re