main, it is still small compared with other linguistic annotated corpora such as the Penn Treebank.
    Thus, the data sparseness problem is severe, and must be treated carefully.
    Usually, the data sparseness is prevented by using more general features that apply to a broader set of instances (e.g., disjunctions).
    While polynomial kernels in the SVM learning can effectively generate feature conjunctions, kernel functions that can effectively generate feature disjunctions are not known.
    Thus, we should explicitly add dimensions for such general features.
    The word cache feature is defined as the disjunction of several word features as: We intend that the word cache feature captures the similarities of the patterns with a common key word such as follows.
    We use a left word cache defined as lwck,i &#65533; wc{_k,&#183;&#183;&#183;,0},i, and a right word cache defined as rwck,i - wc{1,&#183;&#183;&#183;,k},i for patterns like (a) and (b) in the above example respectively.
    Kazama et al. (200