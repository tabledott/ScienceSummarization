le sense for other components to be exceedingly selective about which data they use in an attempt to avoid as many errors as possible.
    Rather, it would seem more desirable to extract as much information as possible out of the text (even if it is noisy), and then to use appropriate statistical techniques to handle the noise.
    There is a more fundamental reason to think that this is the right approach.
    Brent and Berwick's original program learned just five subcategorization frames (TV, THAT, NPTHAT, INF and NPINF).
    While at the time they suggested that &amp;quot;we foresee no impediment to detecting many more,&amp;quot; this has apparently not proved to be the case (in Brent (1992) only six are learned: the above plus DTV).
    It seems that the reason for this is that their approach has depended upon finding cues that are very accurate predictors for a certain subcategorization (that is, there are very few false triggers), such as pronouns for NP objects and to plus a finite verb for infinitives