e the per-word cross-entropy, according to a language model trained on I, of a text segment s drawn from N. Let HN(s) be the per-word cross-entropy of s according to a language model trained on a random sample of N. We partition N into text segments (e.g., sentences), and score the segments according to HI(s) &#8722; HN(s), selecting all text segments whose score is less than a threshold T. This method can be justified by reasoning simliar to that used to derive methods for training binary text classifiers without labeled negative examples (Denis et al., 2002; Elkin and Noto, 2008).
    Let us imagine that our non-domainspecific corpus N contains an in-domain subcorpus NI, drawn from the same distribution as our in-domain corpus I.
    Since NI is statistically just like our in-domain data I, it would seem to be a good candidate for the data that we want to extract from N. By a simple variant of Bayes rule, the probability P(NI|s, N) of a text segment s, drawn randomly from N, being in NI is given by Since NI