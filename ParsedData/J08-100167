nd test sets, we augmented the materials used in the elicitation study with additional DUC summaries generated by humans for the same input sets.
    We assumed that these summaries were maximally coherent.
    As mentioned previously, our participants tend to rate human-authored summaries higher than machine-generated ones.
    To ensure that we do not tune a model to a particular system, we used the output summaries of distinct systems for training and testing.
    Our set of training materials contained 6 x 16 summaries (average length 4.8), yielding (2) x 16 = 240 pairwise rankings.
    Because human summaries often have identical (high) scores, we eliminated pairs of such summaries from the training set.
    Consequently, the resulting training corpus consisted of 144 summaries.
    In a similar fashion, we obtained 80 pairwise rankings for the test set.
    Six documents from the training data were used as a development set.
    Features, Parameter Settings, and Training Requirements.
    We examine the