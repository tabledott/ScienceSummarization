the perfect ranker by about 5% for BNEWS and 3% for both NPAPER and NWIRE in terms of F-measure, suggesting that the supervised ranker still has room for improvement.
    Moreover, by comparing rows 1-2 and 7 of Table 4, we can see that the perfect ranker outperforms the baselines by less than 5%.
    This is essentially an upper limit on how much our approach can improve upon the baselines given the current set of candidate partitions.
    In other words, the performance of our approach is limited in part by the quality of the candidate partitions, more so with B-CUBED than with the MUC scorer.
  
  
    Two questions naturally arise after examining the above results.
    First, which of the 54 coreference systems generally yield superior results?
    Second, why is the same set of candidate partitions scored so differently by the two scoring programs?
    To address the first question, we take the 54 coreference systems that were trained on half of the available training texts (see Section 4) and apply them