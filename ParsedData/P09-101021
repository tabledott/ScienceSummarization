 Evaluation Metrics For evaluation, we compare the results to manually constructed sequences of actions.
    We measure the number of correct actions, sentences, and documents.
    An action is correct if it matches the annotations in terms of command and parameters.
    A sentence is correct if all of its actions are correctly identified, and analogously for documents.11 Statistical significance is measured with the sign test.
    Additionally, we compute a word alignment score to investigate the extent to which the input text is used to construct correct analyses.
    This score measures the percentage of words that are aligned to the corresponding annotated actions in correctly analyzed documents.
    Baselines We consider the following baselines to characterize the performance of our approach. lems like ours are typically addressed using supervised techniques.
    We measure how a standard supervised approach would perform on this task by using a reward signal based on manual annotations of output action 