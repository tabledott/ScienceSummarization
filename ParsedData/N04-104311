had previously been annotated with the MUC name classes.
    Sections 02-21 were used as training material, and Section 23 was used as test (note that the syntactic trees were not used in any way).
    Scoring was performed using the MUC scorer.
    For unsupervised clustering data, we used the Wall Street Journal subset of the Continuous Speech Recognition (CSR-III) collection (LDC catalog # LDC95T6).
    This portion of the collection contains approximately 100 million words.
    Active learning experiments were performed by permitting the system to choose examples from among the pool of annotated data, rather than presenting the examples in their natural chronological order.
    This approach, previously used in [Boschee et al, 2002], permits simulation of human-in-the-loop experiments that are inexpensive to run and repeatable because they don&#8217;t actually involve a human annotator.
    However, because the pool of pre-annotated examples is limited, the results are most meaningful for small training s