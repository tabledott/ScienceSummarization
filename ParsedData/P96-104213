ction (Lewis and Catlett, 1994; Lewis and Gale, 1994).
    We describe here general algorithms for both sequential and batch selection.
    Sequential selection examines unlabeled examples as they are supplied, one by one, and measures the disagreement in their classification by the committee.
    Those examples determined to be sufficiently informative are selected for training.
    Most simply, we can use a committee of size two and select an example when the two models disagree on its classification.
    This gives the following, parameter-free, two member sequential selection algorithm, executed for each unlabeled input example e: 'The normal approximation, while easy to implement, can be avoided.
    The posterior probability P(a, = adS) for the multinomial is given exactly by the Dirichlet distribution (Johnson, 1972) (which reduces to the Beta distribution in the binomial case).
    In this work we assumed a uniform prior distribution for each model parameter; we have not addressed the question of how 