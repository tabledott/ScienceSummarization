
  Simple Semi-supervised Dependency Parsing
  
    We present a simple and effective semisupervised method for training dependency parsers.
    We focus on the problem of lexical representation, introducing features that incorporate word clusters derived from a large unannotated corpus.
    We demonstrate the effectiveness of the approach in a series of dependency parsing experiments on the Penn Treebank and Prague Dependency Treebank, and we show that the cluster-based features yield substantial gains in performance across a wide range of conditions.
    For example, in the case of English unlabeled second-order parsing, we improve from a baseline accuof in the case of Czech unlabeled second-order parsing, we from a baseline accuracy of addition, we demonstrate that our method also improves performance when small amounts of training data are available, and can roughly halve the amount of supervised data required to reach a desired level of performance.
  
  
    In natural language parsing, lexical informat