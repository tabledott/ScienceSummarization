associated A.
    The function Z(H), called the partition function, is a normalizing constant (for fixed H), so the probabilities over all a sum to one.
    Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a, H) for 0 &lt; i &lt; j: Here go(a, H) = 11Z (H) and gi(a, H) = eAi(a,H) fi(&#176;,11).
    The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely, one if the feature has no effect, and smaller than one if it makes the probability less likely.
    Maximum-entropy models have two benefits for a parser builder.
    First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various &amp;quot;features&amp;quot; suggests that the probability model should be easily changeable &#8212; just change the set of features used.
    This point is emphasized by Ratnaparkhi in discussing his parser [17).
    Second, and this is a point we have