mbols with many &#8216;features&#8217; (Goodman, 1997; Johnson, 1998).
    Examples of such features are head words of constituents, labels of ancestor and sibling nodes, and subcategorization frames of lexical heads.
    Effective features and their good combinations are normally explored using trial-and-error.
    This paper defines a generative model of parse trees that we call PCFG with latent annotations (PCFG-LA).
    This model is an extension of PCFG models in which non-terminal symbols are annotated with latent variables.
    The latent variables work just like the features attached to non-terminal symbols.
    A fine-grained PCFG is automatically induced from parsed corpora by training a PCFG-LA model using an EM-algorithm, which replaces the manual feature selection used in previous research.
    The main focus of this paper is to examine the effectiveness of the automatically trained models in parsing.
    Because exact inference with a PCFG-LA, i.e., selection of the most probable parse, is NP-ha