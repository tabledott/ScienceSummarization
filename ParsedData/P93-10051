 Since the grammar produces parses which may be more detailed than the Treebank, the decision tree was built using a training set constructed in the following manner.
    Using the grammar with the P-CFG model we determined the most likely parse that is consistent with the Treebank and considered the resulting sentence-tree pair as an event.
    Note that the grammar parse will also provide the lexical head structure of the parse.
    Then, we extracted using leftmost derivation order tuples of a history (truncated to the definition of a history in the HBG model) and the corresponding rule used in expanding a node.
    Using the resulting data set we built a decision tree by classifying histories to locally minimize the entropy of the rule template.
    With a training set of about 9000 sentencetree pairs, we had about 240,000 tuples and we grew a tree with about 40,000 nodes.
    This required 18 hours on a 25 MIPS RISC-based machine and the resulting decision tree was nearly 100 megabytes.
    Immediate vs.