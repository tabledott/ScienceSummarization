where the decision tree / power divergence approach is less accurate than the SENSEVAL average; promise-n, scrap-n, shirt-n, amazev, bitter-p, and sanction-p.
    The most dramatic difference occurred with amaze-v, where the SENSEVAL average was 92.4% and the decision tree accuracy was 58.6%.
    However, this was an unusual task where every instance in the test data belonged to a single sense that was a minority sense in the training data.
  
  
    The characteristics of the decision trees and decision stumps learned for each word are shown in Table 2.
    Column 1 shows the word and part of speech.
    Columns 2, 3, and 4 are based on the feature set selected by the power divergence statistic while columns 5, 6, and 7 are based on the Dice Coefficient.
    Columns 2 and 5 show the node selected to serve as the decision stump.
    Columns 3 and 6 show the number of leaf nodes in the learned decision tree relative to the number of total nodes.
    Columns 4 and 7 show the number of bigram features selected t