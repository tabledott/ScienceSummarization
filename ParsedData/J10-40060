
  Distributional Memory: A General Framework for Corpus-Based Semantics
  
    Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus.
    As an alternative to this &#8220;one task, one model&#8221; approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.
    Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems.
    In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relatio