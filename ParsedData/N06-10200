
  Effective Self-Training For Parsing
  
    We present a simple, but surprisingly effective, method of self-training a twophase parser-reranker system using readily available unlabeled data.
    We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. improved model achieves an of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing.
    Finally, we provide some analysis to better understand the phenomenon.
  
  
    In parsing, we attempt to uncover the syntactic structure from a string of words.
    Much of the challenge of this lies in extracting the appropriate parsing decisions from textual examples.
    Given sufficient labelled data, there are several &#8220;supervised&#8221; techniques of training high-performance parsers (Charniak and Johnson, 2005; Collins, 2000; Henderson, 2004).
    Other methods are &#8220;semi-supervised&#8221; where they use some l